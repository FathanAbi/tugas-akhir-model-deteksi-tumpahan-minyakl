{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab9c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "from HSI_class import HSI\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import zeroPadding\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f79fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "datasets = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i>0:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        datasets.append(hsi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d40bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithDataset(n): \n",
    "    hsi_test = datasets[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    test_gt = hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {test_gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(test_gt == 0)\n",
    "    indices1 = np.argwhere(test_gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    num_samples = 10\n",
    "\n",
    "    random_indices0 = indices0[np.random.choice(len(indices0), num_samples, replace=False)]\n",
    "    random_indices1 = indices1[np.random.choice(len(indices1), num_samples, replace=False)]\n",
    "\n",
    "    test_indices = np.vstack((random_indices0, random_indices1))\n",
    "\n",
    "    print(test_indices.shape)\n",
    "\n",
    "    return test_indices, test_gt, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e908e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, feature_extractor, batch_input, device):\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        test_features = feature_extractor(batch_input)  # shape: (10, 128)\n",
    "        test_features_np = test_features.numpy()     # convert to NumPy\n",
    "\n",
    "    predicted_classes = model.predict(test_features_np)\n",
    "    probs = model.predict_proba(test_features_np)\n",
    "    positive_probs = probs[:, 1]  # probability of class 1\n",
    "\n",
    "\n",
    "    return predicted_classes, positive_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d26ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model svm_model.pkl...\n",
      "Model loaded and moved to device\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64  # You can change this depending on your GPU capacity\n",
    "\n",
    "model_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\svm\\svm_model.pkl\"\n",
    "model_name = model_path.split('\\\\')[-1]\n",
    "\n",
    "print(f\"Creating model {model_name}...\")\n",
    "svm_loaded = joblib.load(model_path)\n",
    "\n",
    "print(\"Model loaded and moved to device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e55134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvTo1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvTo1D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=224, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # (batch_size, 128, 1, 1)\n",
    "        self.flatten = nn.Flatten()               # (batch_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "feature_extractor = ConvTo1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763880d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: (1243, 684, 224)\n",
      "img shape after padding (1251, 692, 224)\n",
      "number of pixel 850212\n",
      "ground truth shape: (1243, 684)\n",
      "indices = 0 shape: (820876, 2)\n",
      "indices = 1 shape: (29336, 2)\n",
      "(20, 2)\n",
      "(1243, 684)\n"
     ]
    }
   ],
   "source": [
    "test_indices, test_gt, matrix = testWithDataset(0)\n",
    "print(test_gt.shape)\n",
    "# for indice in test_indices:\n",
    "#     print(test_gt[indice[0]][indice[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b6836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 128)\n",
      "Sample 0: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 1: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 2: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 3: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 4: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 5: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 6: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 7: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 8: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 9: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 10: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 11: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 12: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 13: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 14: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 15: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 16: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 17: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 18: Predicted = 0, Prob(class 1) = 0.0000\n",
      "Sample 19: Predicted = 0, Prob(class 1) = 0.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays use different devices: cpu, cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Predicted = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Prob(class 1) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# y_pred = svm.predict(X_val)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:224\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    156\u001b[0m     {\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maccuracy_score\u001b[39m(y_true, y_pred, \u001b[38;5;241m*\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    In multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    0.5\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     xp, _, device \u001b[38;5;241m=\u001b[39m \u001b[43mget_namespace_and_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:614\u001b[0m, in \u001b[0;36mget_namespace_and_device\u001b[1;34m(remove_none, remove_types, *array_list)\u001b[0m\n\u001b[0;32m    611\u001b[0m skip_remove_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(remove_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, remove_types\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    613\u001b[0m xp, is_array_api \u001b[38;5;241m=\u001b[39m get_namespace(\u001b[38;5;241m*\u001b[39marray_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mskip_remove_kwargs)\n\u001b[1;32m--> 614\u001b[0m arrays_device \u001b[38;5;241m=\u001b[39m \u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mskip_remove_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_array_api:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp, is_array_api, arrays_device\n",
      "File \u001b[1;32mc:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:178\u001b[0m, in \u001b[0;36mdevice\u001b[1;34m(remove_none, remove_types, *array_list)\u001b[0m\n\u001b[0;32m    176\u001b[0m     device_other \u001b[38;5;241m=\u001b[39m _single_array_device(array)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_ \u001b[38;5;241m!=\u001b[39m device_other:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput arrays use different devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device_)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device_other)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m         )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays use different devices: cpu, cpu"
     ]
    }
   ],
   "source": [
    "patch = 9\n",
    "half_patch = patch // 2\n",
    "input_patches = []\n",
    "true_labels = []\n",
    "\n",
    "# Prepare all patches\n",
    "for x_pos, y_pos in test_indices:\n",
    "    true_label = test_gt[x_pos][y_pos]\n",
    "\n",
    "    selected_rows = matrix[x_pos:x_pos + 2*half_patch + 1, :]\n",
    "    testing_patch = selected_rows[:, y_pos:y_pos + 2*half_patch + 1]\n",
    "\n",
    "    patch_tensor = torch.tensor(testing_patch, dtype=torch.float32)\n",
    "    patch_tensor = patch_tensor.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "    input_patches.append(patch_tensor)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "input_patches = torch.cat(input_patches, dim=0)  # Shape: (N, C, H, W)\n",
    "true_labels = torch.tensor(true_labels)\n",
    "\n",
    "with torch.no_grad():  # No gradient needed for feature extraction\n",
    "    features = feature_extractor(input_patches)  # (100, 128)\n",
    "    features_np = features.numpy() \n",
    "\n",
    "print(features_np.shape)\n",
    "\n",
    "X = features_np\n",
    "y = true_labels\n",
    "\n",
    "\n",
    "predicted_classes = svm_loaded.predict(X)\n",
    "probs = svm_loaded.predict_proba(X)\n",
    "positive_probs = probs[:, 1]  # probability of class 1\n",
    "\n",
    "# Combine both into a display\n",
    "for i, (pred, prob) in enumerate(zip(predicted_classes, positive_probs)):\n",
    "    print(f\"Sample {i}: Predicted = {pred}, Prob(class 1) = {prob:.4f}\")\n",
    "\n",
    "# y_pred = svm.predict(X_val)\n",
    "acc = accuracy_score(y, predicted_classes)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "patch_size = 9\n",
    "half_patch = patch_size // 2\n",
    "\n",
    "scores = []\n",
    "\n",
    "groundtruth = []\n",
    "prediction = []\n",
    "y_probs = []\n",
    "\n",
    "\n",
    "for dataset in range(len(datasets)):\n",
    "    print(f\"tes: {dataset}\")\n",
    "    test_indices, test_gt, matrix = testWithDataset(dataset)\n",
    "\n",
    "    total = len(test_indices)\n",
    "    correct0 = 0\n",
    "    correct1 = 0\n",
    "\n",
    "    input_patches = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Prepare all patches\n",
    "    for x_pos, y_pos in test_indices:\n",
    "        true_label = test_gt[x_pos][y_pos]\n",
    "\n",
    "        selected_rows = matrix[x_pos:x_pos + 2*half_patch + 1, :]\n",
    "        testing_patch = selected_rows[:, y_pos:y_pos + 2*half_patch + 1]\n",
    "\n",
    "        patch_tensor = torch.tensor(testing_patch, dtype=torch.float32)\n",
    "        patch_tensor = patch_tensor.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "        input_patches.append(patch_tensor)\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "    input_patches = torch.cat(input_patches, dim=0)  # Shape: (N, C, H, W)\n",
    "    true_labels = torch.tensor(true_labels)\n",
    "\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, total, batch_size), desc=\"Predicting\"):\n",
    "        batch = input_patches[i:i+batch_size]\n",
    "        labels = true_labels[i:i+batch_size]\n",
    "\n",
    "        groundtruth.append(labels)\n",
    "\n",
    "        preds, postive_class_probs = predict_batch(svm_loaded, feature_extractor, batch, device)\n",
    "\n",
    "        prediction.append(preds)\n",
    "        y_probs.append(postive_class_probs)\n",
    "\n",
    "        for j in range(len(preds)):\n",
    "            index = i + j\n",
    "            print(f\"{index+1}: prediction = {preds[j]}, expected: {labels[j].item()}\")\n",
    "            if preds[j] == labels[j].item():\n",
    "                if labels[j].item() == 0:\n",
    "                    correct0 += 1\n",
    "                elif labels[j] == 1:\n",
    "                    correct1 += 1\n",
    "\n",
    "    correct = correct0 + correct1\n",
    "    print(f\"Score: {correct}/{total}\")\n",
    "    \n",
    "    score = {\n",
    "        'dataset': dataset,\n",
    "        'correct_0': correct0,\n",
    "        'correct_1': correct1,\n",
    "        'correct_total': correct,\n",
    "        'total': total\n",
    "    }\n",
    "    scores.append(score)\n",
    "    # scores.append((f\"dataset{dataset}\", f'{correct0}/{total/2}', f'{correct1}/{total/2}', f'{correct}/{total}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correct = 0\n",
    "all_total = 0\n",
    "all_correct0 = 0\n",
    "all_correct1 = 0\n",
    "\n",
    "for score in scores:\n",
    "    dataset = score['dataset']\n",
    "    correct0 = score['correct_0']\n",
    "    correct1 = score['correct_1']\n",
    "    correct = score['correct_total']\n",
    "    total = score['total']\n",
    "    print(f\"dataset: {dataset}\\t\", f'{correct0}/{total/2}\\t', f'{correct1}/{total/2}\\t', f'{correct}/{total}\\t')\n",
    "\n",
    "    all_correct += correct\n",
    "    all_total += total\n",
    "    all_correct0 += correct0\n",
    "    all_correct1 += correct1\n",
    "\n",
    "\n",
    "print(f\"total: \\t\\t {all_correct0}/{all_total/2} \\t {all_correct1}/{all_total/2} \\t {all_correct}/{all_total}\")\n",
    "\n",
    "print(f\"acc: {all_correct/all_total}\")\n",
    "\n",
    "all_total_score = {\n",
    "    'dataset': 'Total Dataset',\n",
    "    'correct_0': all_correct0,\n",
    "    'correct_1': all_correct1,\n",
    "    'correct_total': all_correct,\n",
    "    'total': all_total\n",
    "}\n",
    "\n",
    "scores.append(all_total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee05689",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruths = groundtruth\n",
    "groundtruth = []\n",
    "\n",
    "for x in groundtruths:\n",
    "    for y in x:\n",
    "        groundtruth.append(y)\n",
    "\n",
    "predictions = prediction\n",
    "prediction = []\n",
    "\n",
    "for x in predictions:\n",
    "    for y in x:\n",
    "        prediction.append(y)\n",
    "\n",
    "\n",
    "y_prob = []\n",
    "\n",
    "for x in y_probs:\n",
    "    for y in x:\n",
    "        y_prob.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groundtruth))\n",
    "print(len(prediction))\n",
    "print(len(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e42082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = groundtruth\n",
    "y_pred = prediction\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for x, y in zip(y_test, y_pred):\n",
    "    total += 1\n",
    "    if x == y:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e990078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{correct}/{total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = np.array([label.item() for label in y_test])\n",
    "# Ensure labels are binary (0 and 1)\n",
    "print(\"Unique values in y_test:\", pd.Series(y_test_np).unique())\n",
    "\n",
    "# Check if y_pred is probability (float) or hard prediction (int)\n",
    "print(\"Sample y_pred values:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e48bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    {'True': y_test_np, 'Model': y_prob})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_df['True'], test_df['Model'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'Model (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Two Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_true = np.array([int(label) for label in y_test_np])  # true labels\n",
    "y_pred = prediction                          # predicted class labels (e.g., from predict_batch)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # Use 'binary' if binary task\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Overall Accuracy (OA)\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Average Accuracy (AA) — mean of per-class accuracies\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "aa = per_class_acc.mean()\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"OA:        {oa:.4f}\")\n",
    "print(f\"AA:        {aa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63022330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance = {\n",
    "    'AUC': float(roc_auc),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'F1 Score': float(f1),\n",
    "    'OA': float(oa),\n",
    "    'AA': float(aa),\n",
    "}\n",
    "result_json = {\n",
    "    'prediction' : scores,\n",
    "    'performance' : performance,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711abcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(result_json)\n",
    "\n",
    "with open(f\"performance/{model_name}_results.json\", \"w\") as f:\n",
    "    json.dump(result_json, f, indent=2)\n",
    "\n",
    "print(\"JSON saved to results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Run time: {end_time - start_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
