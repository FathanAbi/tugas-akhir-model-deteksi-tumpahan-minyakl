{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a5a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "epochs = 200\n",
    "\n",
    "batch_size = 20\n",
    "test_size = 0.5\n",
    "\n",
    "random_indice = 1\n",
    "\n",
    "seeded_run = True\n",
    "seed = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25da8a0f-8f90-4f9e-9a04-991692e9ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed has been set\n",
      "seet used: 10\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # PyTorch determinism\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if seeded_run:\n",
    "    set_seed(seed)\n",
    "    print(\"seed has been set\")\n",
    "    print(f\"seet used: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578786fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: D:\\FathanAbi\\tugas-akhir-model-deteksi-tumpahan-minyakl\\Hyperspectral oil spill detection datasets\\GM01.mat\n",
      "random: 1\n",
      "generating random sample\n",
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "creating 5 Randomly chosen 0 indices:\n",
      "creating 5 Randomly chosen 1 indices:\n",
      "indices 0 used: [(np.int64(910), np.int64(192)), (np.int64(51), np.int64(255)), (np.int64(689), np.int64(202)), (np.int64(772), np.int64(547)), (np.int64(920), np.int64(471))]\n",
      "indices 1 used: [(np.int64(22), np.int64(455)), (np.int64(170), np.int64(145)), (np.int64(410), np.int64(233)), (np.int64(1055), np.int64(123)), (np.int64(469), np.int64(582))]\n",
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"D:\\FathanAbi\\tugas-akhir-model-deteksi-tumpahan-minyakl\\Hyperspectral oil spill detection datasets\"\n",
    "# dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 0:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1\n",
    "\n",
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random_indice}\")\n",
    "\n",
    "if random_indice:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(188), np.int64(124)), (np.int64(523), np.int64(150)), (np.int64(1003), np.int64(474)), (np.int64(616), np.int64(508)), (np.int64(905), np.int64(552))]\n",
    "    indices_1 = [(np.int64(106), np.int64(606)), (np.int64(297), np.int64(468)), (np.int64(926), np.int64(35)), (np.int64(536), np.int64(519)), (np.int64(508), np.int64(442))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21c1c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-219 -372  361  395  442  586  641  667  632  594  559  517  484  453\n",
      "  432  402  375  347  324  297  275  250  230  202  176  163  153  142\n",
      "  137  125  124  116  116  111   99   86   73   65   54   19   39   47\n",
      "   59   20   40   41   37   35   33   17   19   23   38   34   35   27\n",
      "   28   11    1   -6   -5  -13 -184  -77    5    4   19   25   26   29\n",
      "   31   25   27   27   28   23   18   15    4  -26 -217 -248 -115 -122\n",
      "  -43   -5    2    0    4    0    6   12   17   17   22   22   23   22\n",
      "   36   17   18    7    4   15   -5  -31    0    0    0    0    0    0\n",
      "    0    0  -41 -185  -52  -35  -86  -44   -6    6   12   12   22   22\n",
      "   20   20   22   25   27   26   24   21   20   24   27   16   23   28\n",
      "   25   29   22   18   12   17   14    5    7   -1    5  120    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -66  -65    0    0    1   -9    2  -15   13   14   17   18   15   12\n",
      "   18   14   17   22   21   18   17   20   27   10   11   13   12   16\n",
      "    8   17   11    9   11   18   14   12   21    8    6    8    0   13\n",
      "    1    4   -3   -3   -1  -43   -7  -12  -17  -11  -16    9    5  -23]\n",
      "[-219. -372.  361.  395.  442.  586.  641.  667.  632.  594.  559.  517.\n",
      "  484.  453.  432.  402.  375.  347.  324.  297.  275.  250.  230.  202.\n",
      "  176.  163.  153.  142.  137.  125.  124.  116.  116.  111.   99.   86.\n",
      "   73.   65.   54.   19.   39.   47.   59.   20.   40.   41.   37.   35.\n",
      "   33.   17.   19.   23.   38.   34.   35.   27.   28.   11.    1.   -6.\n",
      "   -5.  -13. -184.  -77.    5.    4.   19.   25.   26.   29.   31.   25.\n",
      "   27.   27.   28.   23.   18.   15.    4.  -26. -217. -248. -115. -122.\n",
      "  -43.   -5.    2.    0.    4.    0.    6.   12.   17.   17.   22.   22.\n",
      "   23.   22.   36.   17.   18.    7.    4.   15.   -5.  -31.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.  -41. -185.  -52.  -35.  -86.  -44.\n",
      "   -6.    6.   12.   12.   22.   22.   20.   20.   22.   25.   27.   26.\n",
      "   24.   21.   20.   24.   27.   16.   23.   28.   25.   29.   22.   18.\n",
      "   12.   17.   14.    5.    7.   -1.    5.  120.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -66.  -65.    0.    0.    1.   -9.    2.  -15.   13.   14.   17.   18.\n",
      "   15.   12.   18.   14.   17.   22.   21.   18.   17.   20.   27.   10.\n",
      "   11.   13.   12.   16.    8.   17.   11.    9.   11.   18.   14.   12.\n",
      "   21.    8.    6.    8.    0.   13.    1.    4.   -3.   -3.   -1.  -43.\n",
      "   -7.  -12.  -17.  -11.  -16.    9.    5.  -23.]\n",
      "[-162 -310  271  351  382  549  626  635  623  599  561  519  479  449\n",
      "  423  403  388  364  343  324  310  290  275  254  238  228  218  213\n",
      "  203  200  193  187  189  182  176  165  146  140  123   84   93   97\n",
      "  103   73   93  101   99   94   94   75   73   72   84   83   81   72\n",
      "   67   49   30   17   18  -21 -158  -55    9   23   34   45   50   53\n",
      "   57   51   61   54   61   50   42   35   18   -8 -185 -181 -103 -109\n",
      "  -29    7   12   15   23   14   26   31   34   37   34   44   36   33\n",
      "   22   30   18   14   14   62  -29  -32    0    0    0    0    0    0\n",
      "    0    0 -105 -118  -63  -44  -61  -42   -7   12   12   24   22   40\n",
      "   32   33   31   33   32   34   29   35   28   29   27   27   29   28\n",
      "   25   39   22   21   28   21   18   13    7   -1  -22   -6    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " -108 -153  -18    0    2    6   23   10    8   17   22   24   32   24\n",
      "   22   28   20   26   28   28   24   24   27   27   29   22   29   28\n",
      "   20   26   20   23   27   33   31   17   31   20   17   14   22   19\n",
      "   15    4  -23   -3    8  -40    0    3   -3  -14  -34 -139  -38  -36]\n",
      "[-162. -310.  271.  351.  382.  549.  626.  635.  623.  599.  561.  519.\n",
      "  479.  449.  423.  403.  388.  364.  343.  324.  310.  290.  275.  254.\n",
      "  238.  228.  218.  213.  203.  200.  193.  187.  189.  182.  176.  165.\n",
      "  146.  140.  123.   84.   93.   97.  103.   73.   93.  101.   99.   94.\n",
      "   94.   75.   73.   72.   84.   83.   81.   72.   67.   49.   30.   17.\n",
      "   18.  -21. -158.  -55.    9.   23.   34.   45.   50.   53.   57.   51.\n",
      "   61.   54.   61.   50.   42.   35.   18.   -8. -185. -181. -103. -109.\n",
      "  -29.    7.   12.   15.   23.   14.   26.   31.   34.   37.   34.   44.\n",
      "   36.   33.   22.   30.   18.   14.   14.   62.  -29.  -32.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -105. -118.  -63.  -44.  -61.  -42.\n",
      "   -7.   12.   12.   24.   22.   40.   32.   33.   31.   33.   32.   34.\n",
      "   29.   35.   28.   29.   27.   27.   29.   28.   25.   39.   22.   21.\n",
      "   28.   21.   18.   13.    7.   -1.  -22.   -6.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " -108. -153.  -18.    0.    2.    6.   23.   10.    8.   17.   22.   24.\n",
      "   32.   24.   22.   28.   20.   26.   28.   28.   24.   24.   27.   27.\n",
      "   29.   22.   29.   28.   20.   26.   20.   23.   27.   33.   31.   17.\n",
      "   31.   20.   17.   14.   22.   19.   15.    4.  -23.   -3.    8.  -40.\n",
      "    0.    3.   -3.  -14.  -34. -139.  -38.  -36.]\n",
      "[-225 -433  322  434  449  605  656  672  632  595  564  513  458  433\n",
      "  401  367  339  311  285  259  237  215  190  168  149  137  124  109\n",
      "  106   97   94   87   82   79   67   58   51   41   29    6   20   33\n",
      "   38    3   24   28   24   23   22    2    8   18   30   25   23   26\n",
      "   24    7   -6   -7    0  -11 -118  -73    4   11   17   24   27   27\n",
      "   27   25   31   25   26   19   16   15    4  -24 -190 -181 -117 -124\n",
      "  -36    0    5    0    4    4   10   16   20   19   25   29   26   18\n",
      "   18   21   12    8    6   14  -13  -48    0    0    0    0    0    0\n",
      "    0    0    0 -213 -143  -98 -163  -40   -9   -4    7   10   15    5\n",
      "   18   18   14   14   19   17   15   21   17   15   12   10   17   19\n",
      "   19   18    6   13    8   -4    0    1    8   -7   10  -40    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  -11  -10   -9   -4  -17    2  -14    0    7    9    6   15   12\n",
      "   11   10   10   15   15   18   13   12   19   19    7    9    0   12\n",
      "   12   14   15    4   21   12    9   24    0    8    6    3    0    6\n",
      "    8  -11  -22   11  -17  -31  -19   -9    0   -1  -11   -4    0    0]\n",
      "[-225. -433.  322.  434.  449.  605.  656.  672.  632.  595.  564.  513.\n",
      "  458.  433.  401.  367.  339.  311.  285.  259.  237.  215.  190.  168.\n",
      "  149.  137.  124.  109.  106.   97.   94.   87.   82.   79.   67.   58.\n",
      "   51.   41.   29.    6.   20.   33.   38.    3.   24.   28.   24.   23.\n",
      "   22.    2.    8.   18.   30.   25.   23.   26.   24.    7.   -6.   -7.\n",
      "    0.  -11. -118.  -73.    4.   11.   17.   24.   27.   27.   27.   25.\n",
      "   31.   25.   26.   19.   16.   15.    4.  -24. -190. -181. -117. -124.\n",
      "  -36.    0.    5.    0.    4.    4.   10.   16.   20.   19.   25.   29.\n",
      "   26.   18.   18.   21.   12.    8.    6.   14.  -13.  -48.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0. -213. -143.  -98. -163.  -40.\n",
      "   -9.   -4.    7.   10.   15.    5.   18.   18.   14.   14.   19.   17.\n",
      "   15.   21.   17.   15.   12.   10.   17.   19.   19.   18.    6.   13.\n",
      "    8.   -4.    0.    1.    8.   -7.   10.  -40.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.  -11.  -10.   -9.   -4.  -17.    2.  -14.    0.    7.    9.    6.\n",
      "   15.   12.   11.   10.   10.   15.   15.   18.   13.   12.   19.   19.\n",
      "    7.    9.    0.   12.   12.   14.   15.    4.   21.   12.    9.   24.\n",
      "    0.    8.    6.    3.    0.    6.    8.  -11.  -22.   11.  -17.  -31.\n",
      "  -19.   -9.    0.   -1.  -11.   -4.    0.    0.]\n",
      "[-246 -212  408  416  449  564  637  632  598  556  513  460  433  389\n",
      "  368  348  332  317  305  289  277  268  262  276  289  289  293  303\n",
      "  323  347  363  377  358  362  372  377  392  408  436  507  474  476\n",
      "  492  589  583  590  609  630  678  745  765  779  797  823  837  856\n",
      "  864  849  804  771  770  937  990 1048 1011 1044 1101 1206 1313 1375\n",
      " 1426 1471 1473 1483 1475 1429 1290 1141 1048  959 1105  959  999  992\n",
      "  811  837  955 1055 1189 1304 1379 1437 1471 1536 1620 1738 1740 1713\n",
      " 1644 1542 1444 1341 1384 1106 1413 1448    0    0    0    0    0    0\n",
      "    0    0  117  119  242  300  356  511  589  621  687  765  839  863\n",
      "  995 1056 1094 1130 1167 1174 1185 1205 1187 1185 1122 1071 1035 1017\n",
      "  940  914  841  746  689  618  577  522  529  538  558  408    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -60    6   56   82  105  148  200  214  299  374  414  445  504  590\n",
      "  633  655  692  707  730  724  730  741  754  757  758  755  744  722\n",
      "  700  653  604  529  440  383  304  273  184  195  195  178  147  150\n",
      "  158  137  124  123  114   96   75   25   64   40   38  -11    4  -21]\n",
      "[-246. -212.  408.  416.  449.  564.  637.  632.  598.  556.  513.  460.\n",
      "  433.  389.  368.  348.  332.  317.  305.  289.  277.  268.  262.  276.\n",
      "  289.  289.  293.  303.  323.  347.  363.  377.  358.  362.  372.  377.\n",
      "  392.  408.  436.  507.  474.  476.  492.  589.  583.  590.  609.  630.\n",
      "  678.  745.  765.  779.  797.  823.  837.  856.  864.  849.  804.  771.\n",
      "  770.  937.  990. 1048. 1011. 1044. 1101. 1206. 1313. 1375. 1426. 1471.\n",
      " 1473. 1483. 1475. 1429. 1290. 1141. 1048.  959. 1105.  959.  999.  992.\n",
      "  811.  837.  955. 1055. 1189. 1304. 1379. 1437. 1471. 1536. 1620. 1738.\n",
      " 1740. 1713. 1644. 1542. 1444. 1341. 1384. 1106. 1413. 1448.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.  117.  119.  242.  300.  356.  511.\n",
      "  589.  621.  687.  765.  839.  863.  995. 1056. 1094. 1130. 1167. 1174.\n",
      " 1185. 1205. 1187. 1185. 1122. 1071. 1035. 1017.  940.  914.  841.  746.\n",
      "  689.  618.  577.  522.  529.  538.  558.  408.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -60.    6.   56.   82.  105.  148.  200.  214.  299.  374.  414.  445.\n",
      "  504.  590.  633.  655.  692.  707.  730.  724.  730.  741.  754.  757.\n",
      "  758.  755.  744.  722.  700.  653.  604.  529.  440.  383.  304.  273.\n",
      "  184.  195.  195.  178.  147.  150.  158.  137.  124.  123.  114.   96.\n",
      "   75.   25.   64.   40.   38.  -11.    4.  -21.]\n"
     ]
    }
   ],
   "source": [
    "i =1\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])\n",
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828871fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi 1 shape: (20, 9, 9, 224)\n",
      "label augmentai 1 shape: (20,)\n",
      "hasil augmentasi 2 shape: (20, 9, 9, 224)\n",
      "label augmentasi 2 shape: (20,)\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "hasil augmentasi gabungan untuk training: (40, 9, 9, 224)\n",
      "label augmentasi gabungan: (40,)\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# # Count occurrences of each unique element\n",
    "# counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# # Print results\n",
    "# for i, count in enumerate(counts1):\n",
    "#     print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "# counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# # Print results\n",
    "# for i, count in enumerate(counts2):\n",
    "#     print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "# print(label_augment1[3])\n",
    "\n",
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "392df997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a SimSiam model.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
    "        \"\"\"\n",
    "        dim: feature dimension (default: 2048)\n",
    "        pred_dim: hidden dimension of the predictor (default: 512)\n",
    "        \"\"\"\n",
    "        super(SimSiam, self).__init__()\n",
    "        # Custom Convolutional Layer: Process 9x9x224 input\n",
    "        self.pre_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=224, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Reduce to (256, 1, 1)\n",
    "        )\n",
    "        # Fully Connected Layer to reshape to (64, 56, 56)\n",
    "        self.fc = nn.Linear(256 * 1 * 1, 64 * 56 * 56)\n",
    "\n",
    "        # create the encoder\n",
    "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
    "        # self.encoder = base_encoder(num_classes=dim, zero_init_residual=True)\n",
    "        self.encoder = base_encoder(pretrained=True)\n",
    "\n",
    "        self.encoder.features = nn.Sequential(*list(self.encoder.features.children())[1:])\n",
    "\n",
    "        # Modify the classifier to match the desired output dimensions\n",
    "        self.encoder.classifier[6] = nn.Linear(4096, dim)\n",
    "\n",
    "        # Fix: Get the correct input dimension from VGG16 classifier\n",
    "        prev_dim = self.encoder.classifier[6].out_features\n",
    "\n",
    "        # Fix: Assign modified layers to classifier instead of non-existing 'fc'\n",
    "        self.projector = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(prev_dim),\n",
    "                                        nn.ReLU(inplace=True), # first layer\n",
    "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(prev_dim),\n",
    "                                        nn.ReLU(inplace=True), # second layer\n",
    "                                        # self.projector,\n",
    "                                        nn.BatchNorm1d(dim, affine=False)) # output layer\n",
    "                                        \n",
    "\n",
    "        # self.projector[6].bias.requires_grad = False\n",
    "\n",
    "        # build a 3-layer projector\n",
    "        # prev_dim = self.encoder.fc.weight.shape[1]\n",
    "        # self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "        #                                 nn.BatchNorm1d(prev_dim),\n",
    "        #                                 nn.ReLU(inplace=True), # first layer\n",
    "        #                                 nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "        #                                 nn.BatchNorm1d(prev_dim),\n",
    "        #                                 nn.ReLU(inplace=True), # second layer\n",
    "        #                                 self.encoder.fc,\n",
    "        #                                 nn.BatchNorm1d(dim, affine=False)) # output layer\n",
    "        # self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
    "\n",
    "        # build a 2-layer predictor\n",
    "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(pred_dim),\n",
    "                                        nn.ReLU(inplace=True), # hidden layer\n",
    "                                        nn.Linear(pred_dim, dim)) # output layer\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x1: first views of images\n",
    "            x2: second views of images\n",
    "        Output:\n",
    "            p1, p2, z1, z2: predictors and targets of the network\n",
    "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
    "        \"\"\"\n",
    "        print(f\"x1 before preconv {x1.shape}\")\n",
    "        x1 = self.pre_conv(x1)\n",
    "        x2 = self.pre_conv(x2)\n",
    "        print(f\"x1 after preconv {x1.shape}\")\n",
    "\n",
    "        print(f\"x1 before flatten {x1.shape}\")\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        print(f\"x1 after flatten {x1.shape}\")\n",
    "\n",
    "        print(f\"x1 before FC {x1.shape}\")\n",
    "        x1 = self.fc(x1)\n",
    "        x2 = self.fc(x2)\n",
    "        print(f\"x1 after FC {x1.shape}\")\n",
    "\n",
    "        print(f\"x1 after reshape {x1.shape}\")\n",
    "        x1 = x1.view(x1.size(0), 64, 56, 56)\n",
    "        x2 = x2.view(x2.size(0), 64, 56, 56)\n",
    "        print(f\"x1 after reshape {x1.shape}\")\n",
    "        # compute features for one view\n",
    "\n",
    "        # print(x1.shape)\n",
    "        print(f\"x1 before vgg.features {x1.shape}\")\n",
    "        z1 = self.encoder.features(x1) # NxC\n",
    "        z2 = self.encoder.features(x2) # NxC\n",
    "        print(f\"z1 after vgg.features {z1.shape}\")\n",
    "\n",
    "        print(f\"z1 before vgg.avgpool {z1.shape}\")\n",
    "        z1 = self.encoder.avgpool(z1)\n",
    "        z2 = self.encoder.avgpool(z2)\n",
    "        print(f\"z1 after vgg.avgpool {z1.shape}\")\n",
    "\n",
    "\n",
    "        z1 = torch.flatten(z1, 1)\n",
    "        z2 = torch.flatten(z2, 1)\n",
    "        print(f\"z1 after vgg.flatten {z1.shape}\")\n",
    "\n",
    "        print(f\"z1 before vgg.classifier {z1.shape}\")\n",
    "        z1 = self.encoder.classifier(z1)\n",
    "        z2 = self.encoder.classifier(z2)\n",
    "        print(f\"z1 after vgg.classifier {z1.shape}\")\n",
    "        # print(z1.shape)\n",
    "        \n",
    "        print(f\"z1 before projector {z1.shape}\")\n",
    "        z1 = self.projector(z1)\n",
    "        z2 = self.projector(z2)\n",
    "        print(f\"z1 after projector {z1.shape}\")\n",
    "\n",
    "        print(f\"z1 before predictor {z1.shape}\")\n",
    "        p1 = self.predictor(z1) # NxC\n",
    "        p2 = self.predictor(z2) # NxC\n",
    "        print(f\"p1 after predictor {z1.shape}\")\n",
    "\n",
    "        return p1, p2, z1.detach(), z2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9f52de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception_v3', 'list_models', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n",
      "=> creating model 'vgg16'\n",
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)\n",
    "# create model\n",
    "arch = 'vgg16' \n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = SimSiam(\n",
    "    models.__dict__[arch])\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "init_lr = lr * batch_size / 256\n",
    "gpu = 0\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ffc071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "x1 before preconv torch.Size([1, 224, 9, 9])\n",
      "x1 after preconv torch.Size([1, 256, 1, 1])\n",
      "x1 before flatten torch.Size([1, 256, 1, 1])\n",
      "x1 after flatten torch.Size([1, 256])\n",
      "x1 before FC torch.Size([1, 256])\n",
      "x1 after FC torch.Size([1, 200704])\n",
      "x1 after reshape torch.Size([1, 200704])\n",
      "x1 after reshape torch.Size([1, 64, 56, 56])\n",
      "x1 before vgg.features torch.Size([1, 64, 56, 56])\n",
      "z1 after vgg.features torch.Size([1, 512, 1, 1])\n",
      "z1 before vgg.avgpool torch.Size([1, 512, 1, 1])\n",
      "z1 after vgg.avgpool torch.Size([1, 512, 7, 7])\n",
      "z1 after vgg.flatten torch.Size([1, 25088])\n",
      "z1 before vgg.classifier torch.Size([1, 25088])\n",
      "z1 after vgg.classifier torch.Size([1, 2048])\n",
      "z1 before projector torch.Size([1, 2048])\n",
      "z1 after projector torch.Size([1, 2048])\n",
      "z1 before predictor torch.Size([1, 2048])\n",
      "p1 after predictor torch.Size([1, 2048])\n",
      "tensor([[ 0.0002,  0.0215,  0.0078,  ..., -0.0418, -0.0045, -0.0472]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0002,  0.0215,  0.0078,  ..., -0.0418, -0.0045, -0.0472]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0681, 0.0000, 0.0268,  ..., 0.0000, 0.0000, 0.0254]])\n",
      "tensor([[0.0682, 0.0000, 0.0264,  ..., 0.0000, 0.0000, 0.0257]])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0224d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f8b97ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Parameter 205281472\n"
     ]
    }
   ],
   "source": [
    "pretrain_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Pretrain Parameter {pretrain_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32ba8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "class VGG16_HSI(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16_HSI, self).__init__()\n",
    "\n",
    "         # Custom Convolutional Layer: Process 9x9x224 input\n",
    "        self.pre_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=224, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Reduce to (256, 1, 1)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer to reshape to (64, 56, 56)\n",
    "        self.fc = nn.Linear(256 * 1 * 1, 64 * 56 * 56)\n",
    "\n",
    "        # Load VGG-16 Model\n",
    "        self.encoder = vgg16(pretrained=False)\n",
    "\n",
    "        # Remove first VGG-16 conv layer\n",
    "        self.encoder.features = nn.Sequential(*list(self.encoder.features.children())[1:])\n",
    "\n",
    "        # Modify classifier to output 2 classes\n",
    "        self.encoder.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'before {x.shape}')\n",
    "        x = self.pre_conv(x)  # Process hyperspectral input\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # print(f'after preconv {x.shape}')\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        # print(f'after fc {x.shape}')\n",
    "        # Reshape to (batch_size, 64, 56, 56) before passing to VGG\n",
    "        x = x.view(x.size(0), 64, 56, 56)\n",
    "        # print(f'after reshape, before vgg second layer {x.shape}')\n",
    "\n",
    "        x = self.encoder.features(x)  # Pass to VGG-16\n",
    "        x = self.encoder.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.encoder.classifier(x)  # Final classification layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65498039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "=> creating model\n",
      "pre_conv.0.weight: requires_grad=False\n",
      "pre_conv.0.bias: requires_grad=False\n",
      "pre_conv.2.weight: requires_grad=False\n",
      "pre_conv.2.bias: requires_grad=False\n",
      "pre_conv.3.weight: requires_grad=False\n",
      "pre_conv.3.bias: requires_grad=False\n",
      "pre_conv.5.weight: requires_grad=False\n",
      "pre_conv.5.bias: requires_grad=False\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "encoder.features.1.weight: requires_grad=False\n",
      "encoder.features.1.bias: requires_grad=False\n",
      "encoder.features.4.weight: requires_grad=False\n",
      "encoder.features.4.bias: requires_grad=False\n",
      "encoder.features.6.weight: requires_grad=False\n",
      "encoder.features.6.bias: requires_grad=False\n",
      "encoder.features.9.weight: requires_grad=False\n",
      "encoder.features.9.bias: requires_grad=False\n",
      "encoder.features.11.weight: requires_grad=False\n",
      "encoder.features.11.bias: requires_grad=False\n",
      "encoder.features.13.weight: requires_grad=False\n",
      "encoder.features.13.bias: requires_grad=False\n",
      "encoder.features.16.weight: requires_grad=False\n",
      "encoder.features.16.bias: requires_grad=False\n",
      "encoder.features.18.weight: requires_grad=False\n",
      "encoder.features.18.bias: requires_grad=False\n",
      "encoder.features.20.weight: requires_grad=False\n",
      "encoder.features.20.bias: requires_grad=False\n",
      "encoder.features.23.weight: requires_grad=False\n",
      "encoder.features.23.bias: requires_grad=False\n",
      "encoder.features.25.weight: requires_grad=False\n",
      "encoder.features.25.bias: requires_grad=False\n",
      "encoder.features.27.weight: requires_grad=False\n",
      "encoder.features.27.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=True\n",
      "encoder.classifier.6.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "\n",
    "print(\"Use GPU: {} for training\".format(gpu))\n",
    "\n",
    "print(\"=> creating model\")\n",
    "\n",
    "model_finetune = VGG16_HSI()\n",
    "\n",
    "# Freeze all layers except the last fully connected layer\n",
    "for param in model_finetune.pre_conv.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "for param in model_finetune.fc.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "for param in model_finetune.encoder.features.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "\n",
    "for param in model_finetune.encoder.classifier[:-1].parameters():\n",
    "    param.requires_grad = False  # Freeze all but the last FC layer\n",
    "# Initialize the last FC layer\n",
    "# Initialize the last FC layer\n",
    "torch.nn.init.normal_(model_finetune.encoder.classifier[6].weight, mean=0.0, std=0.01)\n",
    "torch.nn.init.zeros_(model_finetune.encoder.classifier[6].bias)\n",
    "\n",
    "# Check which layers are trainable\n",
    "for name, param in model_finetune.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28417fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16_HSI(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5f67fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[1.8294, 0.6276]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model_finetune.eval()\n",
    "output = model_finetune(input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c149368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune Parameter 186401986\n",
      "Pretrain Parameter 205281472\n"
     ]
    }
   ],
   "source": [
    "finetune_parameters = sum(p.numel() for p in model_finetune.parameters())\n",
    "print(f\"finetune Parameter {finetune_parameters}\")\n",
    "print(f\"Pretrain Parameter {pretrain_parameters}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fathanvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
