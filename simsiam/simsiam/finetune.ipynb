{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "best_acc1 = 0\n",
    "\n",
    "\n",
    "# pretrained = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\20250504_124255_checkpoint_0005.pth.tar\"\n",
    "pretrained = r'C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\pretrain\\20250513_160103_best_model.pth.tar'\n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 20\n",
    "num_per_category_augment_2 = 20\n",
    "epochs = 200\n",
    "batch_size = 20\n",
    "test_size = 0.5\n",
    "\n",
    "random = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "class VGG16_HSI(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16_HSI, self).__init__()\n",
    "\n",
    "         # Custom Convolutional Layer: Process 9x9x224 input\n",
    "        self.pre_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=224, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Reduce to (256, 1, 1)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer to reshape to (64, 56, 56)\n",
    "        self.fc = nn.Linear(256 * 1 * 1, 64 * 56 * 56)\n",
    "\n",
    "        # Load VGG-16 Model\n",
    "        self.encoder = vgg16(pretrained=False)\n",
    "\n",
    "        # Remove first VGG-16 conv layer\n",
    "        self.encoder.features = nn.Sequential(*list(self.encoder.features.children())[1:])\n",
    "\n",
    "        # Modify classifier to output 2 classes\n",
    "        self.encoder.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'before {x.shape}')\n",
    "        x = self.pre_conv(x)  # Process hyperspectral input\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # print(f'after preconv {x.shape}')\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        # print(f'after fc {x.shape}')\n",
    "        # Reshape to (batch_size, 64, 56, 56) before passing to VGG\n",
    "        x = x.view(x.size(0), 64, 56, 56)\n",
    "        # print(f'after reshape, before vgg second layer {x.shape}')\n",
    "\n",
    "        x = self.encoder.features(x)  # Pass to VGG-16\n",
    "        x = self.encoder.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.encoder.classifier(x)  # Final classification layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "=> creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "\n",
    "print(\"Use GPU: {} for training\".format(gpu))\n",
    "\n",
    "print(\"=> creating model\")\n",
    "\n",
    "model = VGG16_HSI()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_conv.0.weight: requires_grad=False\n",
      "pre_conv.0.bias: requires_grad=False\n",
      "pre_conv.2.weight: requires_grad=False\n",
      "pre_conv.2.bias: requires_grad=False\n",
      "pre_conv.3.weight: requires_grad=False\n",
      "pre_conv.3.bias: requires_grad=False\n",
      "pre_conv.5.weight: requires_grad=False\n",
      "pre_conv.5.bias: requires_grad=False\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "encoder.features.1.weight: requires_grad=False\n",
      "encoder.features.1.bias: requires_grad=False\n",
      "encoder.features.4.weight: requires_grad=False\n",
      "encoder.features.4.bias: requires_grad=False\n",
      "encoder.features.6.weight: requires_grad=False\n",
      "encoder.features.6.bias: requires_grad=False\n",
      "encoder.features.9.weight: requires_grad=False\n",
      "encoder.features.9.bias: requires_grad=False\n",
      "encoder.features.11.weight: requires_grad=False\n",
      "encoder.features.11.bias: requires_grad=False\n",
      "encoder.features.13.weight: requires_grad=False\n",
      "encoder.features.13.bias: requires_grad=False\n",
      "encoder.features.16.weight: requires_grad=False\n",
      "encoder.features.16.bias: requires_grad=False\n",
      "encoder.features.18.weight: requires_grad=False\n",
      "encoder.features.18.bias: requires_grad=False\n",
      "encoder.features.20.weight: requires_grad=False\n",
      "encoder.features.20.bias: requires_grad=False\n",
      "encoder.features.23.weight: requires_grad=False\n",
      "encoder.features.23.bias: requires_grad=False\n",
      "encoder.features.25.weight: requires_grad=False\n",
      "encoder.features.25.bias: requires_grad=False\n",
      "encoder.features.27.weight: requires_grad=False\n",
      "encoder.features.27.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=True\n",
      "encoder.classifier.6.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers except the last fully connected layer\n",
    "for param in model.pre_conv.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "for param in model.encoder.features.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "\n",
    "for param in model.encoder.classifier[:-1].parameters():\n",
    "    param.requires_grad = False  # Freeze all but the last FC layer\n",
    "\n",
    "# Initialize the last FC layer\n",
    "# Initialize the last FC layer\n",
    "torch.nn.init.normal_(model.encoder.classifier[6].weight, mean=0.0, std=0.01)\n",
    "torch.nn.init.zeros_(model.encoder.classifier[6].bias)\n",
    "\n",
    "# Check which layers are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\pretrain\\20250513_160103_best_model.pth.tar'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus TUF\\AppData\\Local\\Temp\\ipykernel_9680\\4147497479.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict keys: odict_keys(['pre_conv.0.weight', 'pre_conv.0.bias', 'pre_conv.2.weight', 'pre_conv.2.bias', 'pre_conv.2.running_mean', 'pre_conv.2.running_var', 'pre_conv.2.num_batches_tracked', 'pre_conv.3.weight', 'pre_conv.3.bias', 'pre_conv.5.weight', 'pre_conv.5.bias', 'pre_conv.5.running_mean', 'pre_conv.5.running_var', 'pre_conv.5.num_batches_tracked', 'fc.weight', 'fc.bias', 'encoder.features.1.weight', 'encoder.features.1.bias', 'encoder.features.4.weight', 'encoder.features.4.bias', 'encoder.features.6.weight', 'encoder.features.6.bias', 'encoder.features.9.weight', 'encoder.features.9.bias', 'encoder.features.11.weight', 'encoder.features.11.bias', 'encoder.features.13.weight', 'encoder.features.13.bias', 'encoder.features.16.weight', 'encoder.features.16.bias', 'encoder.features.18.weight', 'encoder.features.18.bias', 'encoder.features.20.weight', 'encoder.features.20.bias', 'encoder.features.23.weight', 'encoder.features.23.bias', 'encoder.features.25.weight', 'encoder.features.25.bias', 'encoder.features.27.weight', 'encoder.features.27.bias', 'encoder.classifier.0.weight', 'encoder.classifier.0.bias', 'encoder.classifier.3.weight', 'encoder.classifier.3.bias', 'encoder.classifier.6.weight', 'encoder.classifier.6.bias'])\n",
      "Checkpoint state_dict keys: odict_keys(['pre_conv.0.weight', 'pre_conv.0.bias', 'pre_conv.2.weight', 'pre_conv.2.bias', 'pre_conv.2.running_mean', 'pre_conv.2.running_var', 'pre_conv.2.num_batches_tracked', 'pre_conv.3.weight', 'pre_conv.3.bias', 'pre_conv.5.weight', 'pre_conv.5.bias', 'pre_conv.5.running_mean', 'pre_conv.5.running_var', 'pre_conv.5.num_batches_tracked', 'fc.weight', 'fc.bias', 'encoder.features.1.weight', 'encoder.features.1.bias', 'encoder.features.4.weight', 'encoder.features.4.bias', 'encoder.features.6.weight', 'encoder.features.6.bias', 'encoder.features.9.weight', 'encoder.features.9.bias', 'encoder.features.11.weight', 'encoder.features.11.bias', 'encoder.features.13.weight', 'encoder.features.13.bias', 'encoder.features.16.weight', 'encoder.features.16.bias', 'encoder.features.18.weight', 'encoder.features.18.bias', 'encoder.features.20.weight', 'encoder.features.20.bias', 'encoder.features.23.weight', 'encoder.features.23.bias', 'encoder.features.25.weight', 'encoder.features.25.bias', 'encoder.features.27.weight', 'encoder.features.27.bias', 'encoder.classifier.0.weight', 'encoder.classifier.0.bias', 'encoder.classifier.3.weight', 'encoder.classifier.3.bias', 'encoder.classifier.6.weight', 'encoder.classifier.6.bias', 'projector.0.weight', 'projector.1.weight', 'projector.1.bias', 'projector.1.running_mean', 'projector.1.running_var', 'projector.1.num_batches_tracked', 'projector.3.weight', 'projector.4.weight', 'projector.4.bias', 'projector.4.running_mean', 'projector.4.running_var', 'projector.4.num_batches_tracked', 'projector.6.running_mean', 'projector.6.running_var', 'projector.6.num_batches_tracked', 'predictor.0.weight', 'predictor.1.weight', 'predictor.1.bias', 'predictor.1.running_mean', 'predictor.1.running_var', 'predictor.1.num_batches_tracked', 'predictor.3.weight', 'predictor.3.bias'])\n",
      "Missing keys: ['encoder.classifier.6.weight', 'encoder.classifier.6.bias']\n",
      "=> loaded pre-trained model 'C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\pretrain\\20250513_160103_best_model.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if pretrained:\n",
    "    if os.path.isfile(pretrained):\n",
    "        print(\"=> loading checkpoint '{}'\".format(pretrained))\n",
    "        checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "\n",
    "        # rename moco pre-trained keys\n",
    "        state_dict = checkpoint['state_dict']\n",
    "\n",
    "        print(\"Model state_dict keys:\", model.state_dict().keys())  # Debugging\n",
    "        print(\"Checkpoint state_dict keys:\", state_dict.keys())  \n",
    "        # for k in list(state_dict.keys()):\n",
    "        #     print(f\"Processing key: {k}\")  # Debugging\n",
    "        #     if k.startswith('module.encoder') and not k.startswith('module.encoder.fc'):\n",
    "        #         state_dict[k[len(\"module.encoder.\"):]] = state_dict[k]\n",
    "        #     del state_dict[k]\n",
    "\n",
    "        # Remove the final classification layer from state_dict\n",
    "        state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder.classifier.6\")}\n",
    "\n",
    "        # Load the modified state_dict (ignoring the missing classification layer)\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        # Check missing keys\n",
    "        print(\"Missing keys:\", msg.missing_keys)\n",
    "\n",
    "        start_epoch = 0\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "  \n",
    "        assert set(msg.missing_keys) == {\"encoder.classifier.6.weight\", \"encoder.classifier.6.bias\"}\n",
    "\n",
    "        print(\"=> loaded pre-trained model '{}'\".format(pretrained))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(pretrained))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16_HSI(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM02.mat\n",
      "random: 0\n",
      "using generated indices\n",
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "indices 0 used: [(np.int64(830), np.int64(208)), (np.int64(596), np.int64(629)), (np.int64(878), np.int64(437)), (np.int64(393), np.int64(431)), (np.int64(707), np.int64(564))]\n",
      "indices 1 used: [(np.int64(581), np.int64(245)), (np.int64(292), np.int64(678)), (np.int64(529), np.int64(412)), (np.int64(206), np.int64(124)), (np.int64(516), np.int64(515))]\n",
      "[-171 -233  361  388  427  582  645  652  639  602  570  524  483  439\n",
      "  418  378  362  333  307  277  256  229  210  184  168  152  138  133\n",
      "  120  114  109  104   97   95   86   69   54   51   45    3   27   37\n",
      "   43    9   28   34   27   22   20    1    8   15   30   25   26   20\n",
      "   20    0  -13  -19  -18 -132 -118 -126  -14   -5   11   14   19   20\n",
      "   25   13   19   17   17   10    5    2  -11  -37 -122 -115 -143 -134\n",
      "  -53  -14   -9  -12  -12  -16   -5   -3    6    5    4    4    4   28\n",
      "   17   13   10    8    4   -3  -38  -87 -124    0    0    0    0    0\n",
      "    0  -88  -57  -98  -52  -52  -74  -71  -12    8    7   12   13   24\n",
      "   26   20   17   19   21   20   27   26   20   21   21   28   26   25\n",
      "   28   20   16   18   17   18   11    0   -8    0  -39  -40  -32    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   -9   -9   -3   -6   -1  -11    0   -3    3    9    4   17    8   15\n",
      "   10   17   13   14   14   14   20   15   13   14   15   17   16   15\n",
      "   16   18   11   22    6   21   13   16    9   13    1   13    0    5\n",
      "    0    2   -5    7  -19  -21   -6    0  -11   -3  -16  -12  -14  -21]\n",
      "[-171. -233.  361.  388.  427.  582.  645.  652.  639.  602.  570.  524.\n",
      "  483.  439.  418.  378.  362.  333.  307.  277.  256.  229.  210.  184.\n",
      "  168.  152.  138.  133.  120.  114.  109.  104.   97.   95.   86.   69.\n",
      "   54.   51.   45.    3.   27.   37.   43.    9.   28.   34.   27.   22.\n",
      "   20.    1.    8.   15.   30.   25.   26.   20.   20.    0.  -13.  -19.\n",
      "  -18. -132. -118. -126.  -14.   -5.   11.   14.   19.   20.   25.   13.\n",
      "   19.   17.   17.   10.    5.    2.  -11.  -37. -122. -115. -143. -134.\n",
      "  -53.  -14.   -9.  -12.  -12.  -16.   -5.   -3.    6.    5.    4.    4.\n",
      "    4.   28.   17.   13.   10.    8.    4.   -3.  -38.  -87. -124.    0.\n",
      "    0.    0.    0.    0.    0.  -88.  -57.  -98.  -52.  -52.  -74.  -71.\n",
      "  -12.    8.    7.   12.   13.   24.   26.   20.   17.   19.   21.   20.\n",
      "   27.   26.   20.   21.   21.   28.   26.   25.   28.   20.   16.   18.\n",
      "   17.   18.   11.    0.   -8.    0.  -39.  -40.  -32.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   -9.   -9.   -3.   -6.   -1.  -11.    0.   -3.    3.    9.    4.   17.\n",
      "    8.   15.   10.   17.   13.   14.   14.   14.   20.   15.   13.   14.\n",
      "   15.   17.   16.   15.   16.   18.   11.   22.    6.   21.   13.   16.\n",
      "    9.   13.    1.   13.    0.    5.    0.    2.   -5.    7.  -19.  -21.\n",
      "   -6.    0.  -11.   -3.  -16.  -12.  -14.  -21.]\n",
      "[-178 -147  389  430  490  655  737  755  747  729  700  651  610  575\n",
      "  556  536  507  490  470  446  425  401  396  382  369  353  340  332\n",
      "  327  331  318  319  304  296  292  276  273  266  268  297  260  240\n",
      "  231  262  261  246  240  234  242  262  255  249  240  237  231  228\n",
      "  215  191  159  136  126   30  -28   70  140  157  187  221  247  261\n",
      "  273  274  268  265  252  229  189  151  119   79  -90  -73  -58  -57\n",
      "   46   87  111  137  171  196  204  214  222  228  239  234  252  219\n",
      "  233  213  189  171  129   84  114   48 -124    0    0    0    0    0\n",
      "    0  -56  -37  -73  -52   -5  -25   39   76   96  128  162  181  222\n",
      "  215  226  239  237  240  238  243  239  239  246  235  236  229  219\n",
      "  220  216  205  208  185  159  137  115   77   79   69  -20  -10    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   -5   -2   27   21   25   47   60   83   93  103  111  117  132  142\n",
      "  152  149  156  162  155  157  162  158  168  157  159  167  163  175\n",
      "  167  171  168  177  163  170  166  159  147  154  130  119  119  107\n",
      "   96   80   50   66   52   22   36   30   17   21   -3  -12    0  -11]\n",
      "[-178. -147.  389.  430.  490.  655.  737.  755.  747.  729.  700.  651.\n",
      "  610.  575.  556.  536.  507.  490.  470.  446.  425.  401.  396.  382.\n",
      "  369.  353.  340.  332.  327.  331.  318.  319.  304.  296.  292.  276.\n",
      "  273.  266.  268.  297.  260.  240.  231.  262.  261.  246.  240.  234.\n",
      "  242.  262.  255.  249.  240.  237.  231.  228.  215.  191.  159.  136.\n",
      "  126.   30.  -28.   70.  140.  157.  187.  221.  247.  261.  273.  274.\n",
      "  268.  265.  252.  229.  189.  151.  119.   79.  -90.  -73.  -58.  -57.\n",
      "   46.   87.  111.  137.  171.  196.  204.  214.  222.  228.  239.  234.\n",
      "  252.  219.  233.  213.  189.  171.  129.   84.  114.   48. -124.    0.\n",
      "    0.    0.    0.    0.    0.  -56.  -37.  -73.  -52.   -5.  -25.   39.\n",
      "   76.   96.  128.  162.  181.  222.  215.  226.  239.  237.  240.  238.\n",
      "  243.  239.  239.  246.  235.  236.  229.  219.  220.  216.  205.  208.\n",
      "  185.  159.  137.  115.   77.   79.   69.  -20.  -10.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   -5.   -2.   27.   21.   25.   47.   60.   83.   93.  103.  111.  117.\n",
      "  132.  142.  152.  149.  156.  162.  155.  157.  162.  158.  168.  157.\n",
      "  159.  167.  163.  175.  167.  171.  168.  177.  163.  170.  166.  159.\n",
      "  147.  154.  130.  119.  119.  107.   96.   80.   50.   66.   52.   22.\n",
      "   36.   30.   17.   21.   -3.  -12.    0.  -11.]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 1:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1\n",
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random}\")\n",
    "\n",
    "if random:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(830), np.int64(208)), (np.int64(596), np.int64(629)), (np.int64(878), np.int64(437)), (np.int64(393), np.int64(431)), (np.int64(707), np.int64(564))]\n",
    "    indices_1 = [(np.int64(581), np.int64(245)), (np.int64(292), np.int64(678)), (np.int64(529), np.int64(412)), (np.int64(206), np.int64(124)), (np.int64(516), np.int64(515))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-219 -275  358  394  428  558  618  630  605  567  529  478  443  406\n",
      "  376  347  324  300  280  244  223  203  176  138  123  108   95   86\n",
      "   78   71   69   60   57   56   47   38   22   17   10  -27   -4   11\n",
      "   19  -18    0   10    4    4    0  -24  -13   -7    9    5    7    0\n",
      "    2  -12  -22  -24  -22  -83 -194  -99  -20  -12   -2    0   -1    2\n",
      "    2    0    1    0    3   -1   -5   -7  -14  -41 -242 -279 -133 -143\n",
      "  -52  -18  -12  -21  -12  -22  -14  -12   -4    0    1    0    0   -3\n",
      "    1    4   -6   -3   -9  -78  -30  -76    0    0    0    0    0    0\n",
      "    0    0 -146 -182  -71  -60 -131  -46  -26  -11   -5   -3   -1    8\n",
      "    4   -3   -4    3    5   -2   -4    2    0   -4    1    1    3   -3\n",
      "    0    0    0   -2   -8  -12   -8  -10   -8  -25  -35   -7    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -54  -81  -26  -11   -7  -43    2  -16   -4   -4  -16    0   -1   -4\n",
      "   -2   -3   -7   -6   -2   -2   -8   -7   -1   -6   -5   -3   -7    0\n",
      "   -2   -1   -5    0   -9   -3   -7   -6    0  -17   -3   -8  -15  -19\n",
      "  -11  -12   -3  -17  -20  -23  -27  -11  -28  -12  -31  -28  -64  -21]\n",
      "[-219. -275.  358.  394.  428.  558.  618.  630.  605.  567.  529.  478.\n",
      "  443.  406.  376.  347.  324.  300.  280.  244.  223.  203.  176.  138.\n",
      "  123.  108.   95.   86.   78.   71.   69.   60.   57.   56.   47.   38.\n",
      "   22.   17.   10.  -27.   -4.   11.   19.  -18.    0.   10.    4.    4.\n",
      "    0.  -24.  -13.   -7.    9.    5.    7.    0.    2.  -12.  -22.  -24.\n",
      "  -22.  -83. -194.  -99.  -20.  -12.   -2.    0.   -1.    2.    2.    0.\n",
      "    1.    0.    3.   -1.   -5.   -7.  -14.  -41. -242. -279. -133. -143.\n",
      "  -52.  -18.  -12.  -21.  -12.  -22.  -14.  -12.   -4.    0.    1.    0.\n",
      "    0.   -3.    1.    4.   -6.   -3.   -9.  -78.  -30.  -76.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -146. -182.  -71.  -60. -131.  -46.\n",
      "  -26.  -11.   -5.   -3.   -1.    8.    4.   -3.   -4.    3.    5.   -2.\n",
      "   -4.    2.    0.   -4.    1.    1.    3.   -3.    0.    0.    0.   -2.\n",
      "   -8.  -12.   -8.  -10.   -8.  -25.  -35.   -7.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -54.  -81.  -26.  -11.   -7.  -43.    2.  -16.   -4.   -4.  -16.    0.\n",
      "   -1.   -4.   -2.   -3.   -7.   -6.   -2.   -2.   -8.   -7.   -1.   -6.\n",
      "   -5.   -3.   -7.    0.   -2.   -1.   -5.    0.   -9.   -3.   -7.   -6.\n",
      "    0.  -17.   -3.   -8.  -15.  -19.  -11.  -12.   -3.  -17.  -20.  -23.\n",
      "  -27.  -11.  -28.  -12.  -31.  -28.  -64.  -21.]\n",
      "[-210 -486  361  402  421  546  620  623  602  560  527  469  435  404\n",
      "  381  358  349  325  303  286  267  250  235  223  210  193  187  185\n",
      "  182  180  174  173  168  165  162  155  146  142  139  122  133  140\n",
      "  145  126  145  153  146  141  146  136  136  144  157  154  156  150\n",
      "  147  118  100   86   77   30  -42   35   99  107  128  144  159  163\n",
      "  166  163  166  164  161  145  129  114   88   58 -148 -104  -28  -42\n",
      "   30   64   84   95  111  116  135  138  140  144  146  155  155  158\n",
      "  152  132  117  116  116  117   77  139    0    0    0    0    0    0\n",
      "    0    0 -146  -22  -20  -13    6   15   59   67   76   91  101  108\n",
      "  111  114  117  121  123  119  126  126  124  121  117  114  117  109\n",
      "  112  118  120  108   90   74   77   69   70   69   74   -7    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -32   35   24   10   20   24   79   54   55   63   68   88   91   97\n",
      "   93  102  103  107  101  104  100  108  112  106  111  118  114  109\n",
      "  111  102  101   99  109  104  103   89   83   72   63   65   61   52\n",
      "   56   21   47   39   38    6   12    2   49   15   -2  -28  -18  -21]\n",
      "[-210. -486.  361.  402.  421.  546.  620.  623.  602.  560.  527.  469.\n",
      "  435.  404.  381.  358.  349.  325.  303.  286.  267.  250.  235.  223.\n",
      "  210.  193.  187.  185.  182.  180.  174.  173.  168.  165.  162.  155.\n",
      "  146.  142.  139.  122.  133.  140.  145.  126.  145.  153.  146.  141.\n",
      "  146.  136.  136.  144.  157.  154.  156.  150.  147.  118.  100.   86.\n",
      "   77.   30.  -42.   35.   99.  107.  128.  144.  159.  163.  166.  163.\n",
      "  166.  164.  161.  145.  129.  114.   88.   58. -148. -104.  -28.  -42.\n",
      "   30.   64.   84.   95.  111.  116.  135.  138.  140.  144.  146.  155.\n",
      "  155.  158.  152.  132.  117.  116.  116.  117.   77.  139.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -146.  -22.  -20.  -13.    6.   15.\n",
      "   59.   67.   76.   91.  101.  108.  111.  114.  117.  121.  123.  119.\n",
      "  126.  126.  124.  121.  117.  114.  117.  109.  112.  118.  120.  108.\n",
      "   90.   74.   77.   69.   70.   69.   74.   -7.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -32.   35.   24.   10.   20.   24.   79.   54.   55.   63.   68.   88.\n",
      "   91.   97.   93.  102.  103.  107.  101.  104.  100.  108.  112.  106.\n",
      "  111.  118.  114.  109.  111.  102.  101.   99.  109.  104.  103.   89.\n",
      "   83.   72.   63.   65.   61.   52.   56.   21.   47.   39.   38.    6.\n",
      "   12.    2.   49.   15.   -2.  -28.  -18.  -21.]\n"
     ]
    }
   ],
   "source": [
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(830), np.int64(208)), (np.int64(596), np.int64(629)), (np.int64(878), np.int64(437)), (np.int64(393), np.int64(431)), (np.int64(707), np.int64(564))]\n",
      "[(np.int64(581), np.int64(245)), (np.int64(292), np.int64(678)), (np.int64(529), np.int64(412)), (np.int64(206), np.int64(124)), (np.int64(516), np.int64(515))]\n",
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "print(indices_0)\n",
    "print(indices_1)\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  40\n",
      "hasil augmentasi 1 shape: (40, 9, 9, 224)\n",
      "label augmentai 1 shape: (40,)\n",
      "hasil augmentasi 2 shape: (40, 9, 9, 224)\n",
      "label augmentasi 2 shape: (40,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts1):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts2):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "print(label_augment1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi gabungan untuk training: (80, 9, 9, 224)\n",
      "label augmentasi gabungan: (80,)\n",
      "Element 0 occurs 40 times.\n",
      "Element 1 occurs 40 times.\n"
     ]
    }
   ],
   "source": [
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[-0.1168, -0.0928]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "output = model(input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 0\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "torch.cuda.set_device(gpu)\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "\n",
    "# optimize only the linear classifier\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "assert len(parameters) == 2  # fc.weight, fc.bias\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, init_lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = transforms.Compose(augmentation)\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([80, 224, 9, 9])\n",
      "Train shape: torch.Size([40, 224, 9, 9]), Validation shape: torch.Size([40, 224, 9, 9])\n",
      "torch.Size([20])\n",
      "Train loader size: 2, Validation loader size: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "        \n",
    "            return img1, label  # Return both augmented versions\n",
    "        \n",
    "        return img, label  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "preloaded_images = data_augment  \n",
    "X = torch.tensor(preloaded_images)\n",
    "X= X.to(torch.float32)\n",
    "X = X.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {X.shape}\")\n",
    "\n",
    "y = torch.tensor(label_augment)\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose(augmentation)\n",
    "testSize = 0.5\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = testSize, random_state=42, stratify=y)\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = CustomDataset(X_val, y_val, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1 = next(iter(train_loader))\n",
    "\n",
    "print(batch1[1].size())\n",
    "print(f\"Train loader size: {len(train_loader)}, Validation loader size: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    \"\"\"\n",
    "    Switch to eval mode:\n",
    "    Under the protocol of linear classification on frozen features/models,\n",
    "    it is not legitimate to change any part of the pre-trained model.\n",
    "    BatchNorm in train mode may revise running mean/std (even if it receives\n",
    "    no gradient), which are part of the model parameters too.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        gpu = 0\n",
    "        images = images.cuda(gpu, non_blocking=True)\n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, = accuracy(output, target, topk=(1,))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_freq = 10\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "  \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "      \n",
    "            gpu = 0\n",
    "            images = images.cuda(gpu, non_blocking=True)\n",
    "            target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, = accuracy(output, target, topk=(1,))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            # top5.update(acc5[0], images.size(0))\n",
    "            print(f\"in validation finction {acc1}\")\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 10\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(timestamp, epoch, state, is_best, filename='models/checkpoint.pth.tar'):\n",
    "    filename='models/finetune/{}_checkpoint_best_model.pth.tar'.format(timestamp)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def sanity_check(state_dict, pretrained_weights):\n",
    "    \"\"\"\n",
    "    Linear classifier should not change any weights other than the linear layer.\n",
    "    This sanity check asserts nothing wrong happens (e.g., BN stats updated).\n",
    "    \"\"\"\n",
    "    print(\"=> loading '{}' for sanity check\".format(pretrained_weights))\n",
    "    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "    state_dict_pre = checkpoint['state_dict']\n",
    "\n",
    "    for k in list(state_dict.keys()):\n",
    "        # Ignore fc layer\n",
    "        if 'fc.weight' in k or 'fc.bias' in k:\n",
    "            continue\n",
    "\n",
    "        # Adjust key mapping to match checkpoint format\n",
    "        k_pre = k.replace('module.encoder.', '')  # Remove unnecessary prefix\n",
    "\n",
    "        # Skip missing keys\n",
    "        if k_pre not in state_dict_pre:\n",
    "            print(f\"Warning: {k_pre} not found in pretrained model. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Check if tensor shapes match before comparing values\n",
    "        if state_dict[k].shape != state_dict_pre[k_pre].shape:\n",
    "            print(f\"Warning: Shape mismatch for {k}: {state_dict[k].shape} vs {state_dict_pre[k_pre].shape}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Assert that the weights remain unchanged\n",
    "        assert ((state_dict[k].cpu() == state_dict_pre[k_pre]).all()), \\\n",
    "            '{} is changed in linear classifier training.'.format(k)\n",
    "\n",
    "    print(\"=> sanity check passed.\")\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = cur_lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_conv.0.weight: requires_grad=False\n",
      "pre_conv.0.bias: requires_grad=False\n",
      "pre_conv.2.weight: requires_grad=False\n",
      "pre_conv.2.bias: requires_grad=False\n",
      "pre_conv.3.weight: requires_grad=False\n",
      "pre_conv.3.bias: requires_grad=False\n",
      "pre_conv.5.weight: requires_grad=False\n",
      "pre_conv.5.bias: requires_grad=False\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "encoder.features.1.weight: requires_grad=False\n",
      "encoder.features.1.bias: requires_grad=False\n",
      "encoder.features.4.weight: requires_grad=False\n",
      "encoder.features.4.bias: requires_grad=False\n",
      "encoder.features.6.weight: requires_grad=False\n",
      "encoder.features.6.bias: requires_grad=False\n",
      "encoder.features.9.weight: requires_grad=False\n",
      "encoder.features.9.bias: requires_grad=False\n",
      "encoder.features.11.weight: requires_grad=False\n",
      "encoder.features.11.bias: requires_grad=False\n",
      "encoder.features.13.weight: requires_grad=False\n",
      "encoder.features.13.bias: requires_grad=False\n",
      "encoder.features.16.weight: requires_grad=False\n",
      "encoder.features.16.bias: requires_grad=False\n",
      "encoder.features.18.weight: requires_grad=False\n",
      "encoder.features.18.bias: requires_grad=False\n",
      "encoder.features.20.weight: requires_grad=False\n",
      "encoder.features.20.bias: requires_grad=False\n",
      "encoder.features.23.weight: requires_grad=False\n",
      "encoder.features.23.bias: requires_grad=False\n",
      "encoder.features.25.weight: requires_grad=False\n",
      "encoder.features.25.bias: requires_grad=False\n",
      "encoder.features.27.weight: requires_grad=False\n",
      "encoder.features.27.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=True\n",
      "encoder.classifier.6.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# # Freeze all layers except the last fully connected layer\n",
    "# for param in model.pre_conv.parameters():\n",
    "#     param.requires_grad = False  # Freeze convolutional layers\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = False  # Freeze convolutional layers\n",
    "# for param in model.encoder.features.parameters():\n",
    "#     param.requires_grad = False  # Freeze convolutional layers\n",
    "\n",
    "# for param in model.encoder.classifier[:-1].parameters():\n",
    "#     param.requires_grad = False  # Freeze all but the last FC layer\n",
    "\n",
    "# # Initialize the last FC layer\n",
    "# # Initialize the last FC layer\n",
    "# torch.nn.init.normal_(model.encoder.classifier[6].weight, mean=0.0, std=0.01)\n",
    "# torch.nn.init.zeros_(model.encoder.classifier[6].bias)\n",
    "\n",
    "# Check which layers are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/2]\tTime  1.229 ( 1.229)\tData  0.067 ( 0.067)\tLoss 2.0442e+00 (2.0442e+00)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.252 ( 0.252)\tLoss 4.9701e+01 (4.9701e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 50.000\n",
      "✅ Epoch 1: New best Acc@1: 50.00. Model saved.\n",
      "Epoch: [1][0/2]\tTime  0.076 ( 0.076)\tData  0.036 ( 0.036)\tLoss 1.0234e+02 (1.0234e+02)\tAcc@1  40.00 ( 40.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 1.0951e+02 (1.0951e+02)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 2: No improvement. Patience counter: 1/50\n",
      "Epoch: [2][0/2]\tTime  0.061 ( 0.061)\tData  0.030 ( 0.030)\tLoss 5.6906e+01 (5.6906e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.071 ( 0.071)\tLoss 2.0421e+02 (2.0421e+02)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 55.000\n",
      "✅ Epoch 3: New best Acc@1: 55.00. Model saved.\n",
      "Epoch: [3][0/2]\tTime  0.087 ( 0.087)\tData  0.047 ( 0.047)\tLoss 2.1843e+02 (2.1843e+02)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.087 ( 0.087)\tLoss 4.5852e+01 (4.5852e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 4: No improvement. Patience counter: 1/50\n",
      "Epoch: [4][0/2]\tTime  0.061 ( 0.061)\tData  0.029 ( 0.029)\tLoss 8.3689e+01 (8.3689e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.085 ( 0.085)\tLoss 1.9338e+02 (1.9338e+02)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 5: No improvement. Patience counter: 2/50\n",
      "Epoch: [5][0/2]\tTime  0.079 ( 0.079)\tData  0.046 ( 0.046)\tLoss 1.2154e+02 (1.2154e+02)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.082 ( 0.082)\tLoss 7.9656e+01 (7.9656e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 62.500\n",
      "✅ Epoch 6: New best Acc@1: 62.50. Model saved.\n",
      "Epoch: [6][0/2]\tTime  0.089 ( 0.089)\tData  0.048 ( 0.048)\tLoss 9.6199e+01 (9.6199e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.073 ( 0.073)\tLoss 9.1914e+01 (9.1914e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 7: No improvement. Patience counter: 1/50\n",
      "Epoch: [7][0/2]\tTime  0.074 ( 0.074)\tData  0.042 ( 0.042)\tLoss 4.7139e+01 (4.7139e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.070 ( 0.070)\tLoss 4.6146e+01 (4.6146e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([40.], device='cuda:0')\n",
      " * Acc@1 42.500\n",
      "❌ Epoch 8: No improvement. Patience counter: 2/50\n",
      "Epoch: [8][0/2]\tTime  0.064 ( 0.064)\tData  0.034 ( 0.034)\tLoss 4.3810e+01 (4.3810e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.088 ( 0.088)\tLoss 5.7896e+01 (5.7896e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      " * Acc@1 52.500\n",
      "❌ Epoch 9: No improvement. Patience counter: 3/50\n",
      "Epoch: [9][0/2]\tTime  0.067 ( 0.067)\tData  0.037 ( 0.037)\tLoss 3.5907e+01 (3.5907e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.083 ( 0.083)\tLoss 7.0269e+01 (7.0269e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      " * Acc@1 57.500\n",
      "❌ Epoch 10: No improvement. Patience counter: 4/50\n",
      "Epoch: [10][0/2]\tTime  0.067 ( 0.067)\tData  0.034 ( 0.034)\tLoss 8.3985e+01 (8.3985e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.106 ( 0.106)\tLoss 2.7448e+01 (2.7448e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 52.500\n",
      "❌ Epoch 11: No improvement. Patience counter: 5/50\n",
      "Epoch: [11][0/2]\tTime  0.120 ( 0.120)\tData  0.084 ( 0.084)\tLoss 4.2043e+01 (4.2043e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.070 ( 0.070)\tLoss 2.6817e+01 (2.6817e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 52.500\n",
      "❌ Epoch 12: No improvement. Patience counter: 6/50\n",
      "Epoch: [12][0/2]\tTime  0.192 ( 0.192)\tData  0.144 ( 0.144)\tLoss 2.4349e+01 (2.4349e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.058 ( 0.058)\tLoss 5.5378e+01 (5.5378e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([40.], device='cuda:0')\n",
      " * Acc@1 42.500\n",
      "❌ Epoch 13: No improvement. Patience counter: 7/50\n",
      "Epoch: [13][0/2]\tTime  0.065 ( 0.065)\tData  0.031 ( 0.031)\tLoss 3.6951e+01 (3.6951e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.075 ( 0.075)\tLoss 1.5358e+00 (1.5358e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "✅ Epoch 14: New best Acc@1: 67.50. Model saved.\n",
      "Epoch: [14][0/2]\tTime  0.093 ( 0.093)\tData  0.050 ( 0.050)\tLoss 1.0079e+01 (1.0079e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.091 ( 0.091)\tLoss 2.9826e+01 (2.9826e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 57.500\n",
      "❌ Epoch 15: No improvement. Patience counter: 1/50\n",
      "Epoch: [15][0/2]\tTime  0.070 ( 0.070)\tData  0.034 ( 0.034)\tLoss 7.2808e+01 (7.2808e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.078 ( 0.078)\tLoss 1.0666e+02 (1.0666e+02)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "✅ Epoch 16: New best Acc@1: 70.00. Model saved.\n",
      "Epoch: [16][0/2]\tTime  0.073 ( 0.073)\tData  0.016 ( 0.016)\tLoss 8.9002e+01 (8.9002e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.074 ( 0.074)\tLoss 2.7027e+01 (2.7027e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 17: No improvement. Patience counter: 1/50\n",
      "Epoch: [17][0/2]\tTime  0.048 ( 0.048)\tData  0.031 ( 0.031)\tLoss 1.0432e+02 (1.0432e+02)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.076 ( 0.076)\tLoss 4.8886e+01 (4.8886e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      " * Acc@1 55.000\n",
      "❌ Epoch 18: No improvement. Patience counter: 2/50\n",
      "Epoch: [18][0/2]\tTime  0.063 ( 0.063)\tData  0.034 ( 0.034)\tLoss 5.4778e+01 (5.4778e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.055 ( 0.055)\tLoss 4.4362e+01 (4.4362e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 19: No improvement. Patience counter: 3/50\n",
      "Epoch: [19][0/2]\tTime  0.060 ( 0.060)\tData  0.031 ( 0.031)\tLoss 3.4643e+01 (3.4643e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.058 ( 0.058)\tLoss 4.6912e+01 (4.6912e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 20: No improvement. Patience counter: 4/50\n",
      "Epoch: [20][0/2]\tTime  0.060 ( 0.060)\tData  0.026 ( 0.026)\tLoss 4.0513e+01 (4.0513e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.073 ( 0.073)\tLoss 2.4814e+01 (2.4814e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 21: No improvement. Patience counter: 5/50\n",
      "Epoch: [21][0/2]\tTime  0.063 ( 0.063)\tData  0.033 ( 0.033)\tLoss 1.6504e+01 (1.6504e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.048 ( 0.048)\tLoss 4.0273e+01 (4.0273e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "✅ Epoch 22: New best Acc@1: 77.50. Model saved.\n",
      "Epoch: [22][0/2]\tTime  0.080 ( 0.080)\tData  0.032 ( 0.032)\tLoss 3.4170e+01 (3.4170e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.079 ( 0.079)\tLoss 9.2701e+01 (9.2701e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 23: No improvement. Patience counter: 1/50\n",
      "Epoch: [23][0/2]\tTime  0.086 ( 0.086)\tData  0.049 ( 0.049)\tLoss 8.6191e+01 (8.6191e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.072 ( 0.072)\tLoss 4.9830e+00 (4.9830e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 24: No improvement. Patience counter: 2/50\n",
      "Epoch: [24][0/2]\tTime  0.063 ( 0.063)\tData  0.036 ( 0.036)\tLoss 8.0155e+00 (8.0155e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.063 ( 0.063)\tLoss 1.2371e+02 (1.2371e+02)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      " * Acc@1 57.500\n",
      "❌ Epoch 25: No improvement. Patience counter: 3/50\n",
      "Epoch: [25][0/2]\tTime  0.049 ( 0.049)\tData  0.026 ( 0.026)\tLoss 1.1081e+02 (1.1081e+02)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 4.2493e+00 (4.2493e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 26: No improvement. Patience counter: 4/50\n",
      "Epoch: [26][0/2]\tTime  0.061 ( 0.061)\tData  0.038 ( 0.038)\tLoss 2.7337e+01 (2.7337e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 4.6385e+01 (4.6385e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 27: No improvement. Patience counter: 5/50\n",
      "Epoch: [27][0/2]\tTime  0.070 ( 0.070)\tData  0.038 ( 0.038)\tLoss 7.3348e+01 (7.3348e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.071 ( 0.071)\tLoss 4.6446e+01 (4.6446e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 28: No improvement. Patience counter: 6/50\n",
      "Epoch: [28][0/2]\tTime  0.082 ( 0.082)\tData  0.045 ( 0.045)\tLoss 5.1059e+01 (5.1059e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.069 ( 0.069)\tLoss 2.4837e+01 (2.4837e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 29: No improvement. Patience counter: 7/50\n",
      "Epoch: [29][0/2]\tTime  0.070 ( 0.070)\tData  0.032 ( 0.032)\tLoss 1.9698e+01 (1.9698e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 1.7022e+01 (1.7022e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 30: No improvement. Patience counter: 8/50\n",
      "Epoch: [30][0/2]\tTime  0.067 ( 0.067)\tData  0.036 ( 0.036)\tLoss 5.6340e+01 (5.6340e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 2.9868e+01 (2.9868e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 31: No improvement. Patience counter: 9/50\n",
      "Epoch: [31][0/2]\tTime  0.063 ( 0.063)\tData  0.030 ( 0.030)\tLoss 4.5430e+01 (4.5430e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.086 ( 0.086)\tLoss 1.0497e+01 (1.0497e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 32: No improvement. Patience counter: 10/50\n",
      "Epoch: [32][0/2]\tTime  0.061 ( 0.061)\tData  0.029 ( 0.029)\tLoss 3.2774e+01 (3.2774e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.087 ( 0.087)\tLoss 1.1332e+01 (1.1332e+01)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "✅ Epoch 33: New best Acc@1: 80.00. Model saved.\n",
      "Epoch: [33][0/2]\tTime  0.089 ( 0.089)\tData  0.048 ( 0.048)\tLoss 1.2814e+01 (1.2814e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.074 ( 0.074)\tLoss 5.2583e+00 (5.2583e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "✅ Epoch 34: New best Acc@1: 82.50. Model saved.\n",
      "Epoch: [34][0/2]\tTime  0.123 ( 0.123)\tData  0.031 ( 0.031)\tLoss 3.2577e+00 (3.2577e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.077 ( 0.077)\tLoss 8.2636e+00 (8.2636e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 35: No improvement. Patience counter: 1/50\n",
      "Epoch: [35][0/2]\tTime  0.115 ( 0.115)\tData  0.085 ( 0.085)\tLoss 1.6667e+01 (1.6667e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.115 ( 0.115)\tLoss 5.3258e+01 (5.3258e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 36: No improvement. Patience counter: 2/50\n",
      "Epoch: [36][0/2]\tTime  0.053 ( 0.053)\tData  0.024 ( 0.024)\tLoss 8.6922e+00 (8.6922e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 5.4052e+01 (5.4052e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 37: No improvement. Patience counter: 3/50\n",
      "Epoch: [37][0/2]\tTime  0.054 ( 0.054)\tData  0.025 ( 0.025)\tLoss 6.2271e+01 (6.2271e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 1.2624e+02 (1.2624e+02)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 57.500\n",
      "❌ Epoch 38: No improvement. Patience counter: 4/50\n",
      "Epoch: [38][0/2]\tTime  0.057 ( 0.057)\tData  0.030 ( 0.030)\tLoss 1.1763e+02 (1.1763e+02)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.068 ( 0.068)\tLoss 3.5868e+00 (3.5868e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 85.000\n",
      "✅ Epoch 39: New best Acc@1: 85.00. Model saved.\n",
      "Epoch: [39][0/2]\tTime  0.074 ( 0.074)\tData  0.023 ( 0.023)\tLoss 7.7838e+00 (7.7838e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.061 ( 0.061)\tLoss 5.9938e+01 (5.9938e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 40: No improvement. Patience counter: 1/50\n",
      "Epoch: [40][0/2]\tTime  0.059 ( 0.059)\tData  0.026 ( 0.026)\tLoss 4.3804e+01 (4.3804e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 3.9551e+01 (3.9551e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 57.500\n",
      "❌ Epoch 41: No improvement. Patience counter: 2/50\n",
      "Epoch: [41][0/2]\tTime  0.058 ( 0.058)\tData  0.030 ( 0.030)\tLoss 5.7369e+01 (5.7369e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.055 ( 0.055)\tLoss 4.0299e+01 (4.0299e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 42: No improvement. Patience counter: 3/50\n",
      "Epoch: [42][0/2]\tTime  0.055 ( 0.055)\tData  0.024 ( 0.024)\tLoss 6.0543e+01 (6.0543e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.069 ( 0.069)\tLoss 6.7504e+01 (6.7504e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      " * Acc@1 57.500\n",
      "❌ Epoch 43: No improvement. Patience counter: 4/50\n",
      "Epoch: [43][0/2]\tTime  0.061 ( 0.061)\tData  0.026 ( 0.026)\tLoss 9.3561e+01 (9.3561e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.067 ( 0.067)\tLoss 4.3609e+01 (4.3609e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 62.500\n",
      "❌ Epoch 44: No improvement. Patience counter: 5/50\n",
      "Epoch: [44][0/2]\tTime  0.071 ( 0.071)\tData  0.039 ( 0.039)\tLoss 6.9854e+01 (6.9854e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.071 ( 0.071)\tLoss 1.1925e+02 (1.1925e+02)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 62.500\n",
      "❌ Epoch 45: No improvement. Patience counter: 6/50\n",
      "Epoch: [45][0/2]\tTime  0.055 ( 0.055)\tData  0.027 ( 0.027)\tLoss 4.6009e+01 (4.6009e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.068 ( 0.068)\tLoss 3.5859e+01 (3.5859e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 46: No improvement. Patience counter: 7/50\n",
      "Epoch: [46][0/2]\tTime  0.055 ( 0.055)\tData  0.024 ( 0.024)\tLoss 7.1255e+01 (7.1255e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 6.0583e+01 (6.0583e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 47: No improvement. Patience counter: 8/50\n",
      "Epoch: [47][0/2]\tTime  0.058 ( 0.058)\tData  0.030 ( 0.030)\tLoss 4.9467e+01 (4.9467e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.063 ( 0.063)\tLoss 1.1780e+01 (1.1780e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 48: No improvement. Patience counter: 9/50\n",
      "Epoch: [48][0/2]\tTime  0.053 ( 0.053)\tData  0.023 ( 0.023)\tLoss 2.7940e+00 (2.7940e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.056 ( 0.056)\tLoss 3.9488e+01 (3.9488e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 49: No improvement. Patience counter: 10/50\n",
      "Epoch: [49][0/2]\tTime  0.067 ( 0.067)\tData  0.040 ( 0.040)\tLoss 3.5401e+01 (3.5401e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 2.2998e+01 (2.2998e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 50: No improvement. Patience counter: 11/50\n",
      "Epoch: [50][0/2]\tTime  0.055 ( 0.055)\tData  0.027 ( 0.027)\tLoss 7.3022e+00 (7.3022e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 2.2932e+01 (2.2932e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 51: No improvement. Patience counter: 12/50\n",
      "Epoch: [51][0/2]\tTime  0.054 ( 0.054)\tData  0.026 ( 0.026)\tLoss 6.2200e+01 (6.2200e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 3.2511e+01 (3.2511e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 52: No improvement. Patience counter: 13/50\n",
      "Epoch: [52][0/2]\tTime  0.054 ( 0.054)\tData  0.028 ( 0.028)\tLoss 2.8580e+01 (2.8580e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.061 ( 0.061)\tLoss 3.4929e+00 (3.4929e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 53: No improvement. Patience counter: 14/50\n",
      "Epoch: [53][0/2]\tTime  0.047 ( 0.047)\tData  0.018 ( 0.018)\tLoss 3.4543e+01 (3.4543e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.091 ( 0.091)\tLoss 1.3071e+01 (1.3071e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 54: No improvement. Patience counter: 15/50\n",
      "Epoch: [54][0/2]\tTime  0.052 ( 0.052)\tData  0.024 ( 0.024)\tLoss 2.0163e+01 (2.0163e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([95.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.061 ( 0.061)\tLoss 9.2801e-01 (9.2801e-01)\tAcc@1  95.00 ( 95.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 55: No improvement. Patience counter: 16/50\n",
      "Epoch: [55][0/2]\tTime  0.052 ( 0.052)\tData  0.023 ( 0.023)\tLoss 1.1016e+01 (1.1016e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.068 ( 0.068)\tLoss 7.9261e+00 (7.9261e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 56: No improvement. Patience counter: 17/50\n",
      "Epoch: [56][0/2]\tTime  0.057 ( 0.057)\tData  0.028 ( 0.028)\tLoss 1.1469e+01 (1.1469e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 2.0294e+00 (2.0294e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 57: No improvement. Patience counter: 18/50\n",
      "Epoch: [57][0/2]\tTime  0.055 ( 0.055)\tData  0.027 ( 0.027)\tLoss 3.4973e+01 (3.4973e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.049 ( 0.049)\tLoss 1.9931e+01 (1.9931e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 58: No improvement. Patience counter: 19/50\n",
      "Epoch: [58][0/2]\tTime  0.050 ( 0.050)\tData  0.022 ( 0.022)\tLoss 4.8059e+01 (4.8059e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 1.3544e+01 (1.3544e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 59: No improvement. Patience counter: 20/50\n",
      "Epoch: [59][0/2]\tTime  0.060 ( 0.060)\tData  0.030 ( 0.030)\tLoss 3.6434e+01 (3.6434e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 2.7252e+01 (2.7252e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 60: No improvement. Patience counter: 21/50\n",
      "Epoch: [60][0/2]\tTime  0.050 ( 0.050)\tData  0.021 ( 0.021)\tLoss 5.1860e+00 (5.1860e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 2.3265e+01 (2.3265e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 61: No improvement. Patience counter: 22/50\n",
      "Epoch: [61][0/2]\tTime  0.057 ( 0.057)\tData  0.022 ( 0.022)\tLoss 1.9083e+01 (1.9083e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.067 ( 0.067)\tLoss 5.2069e+01 (5.2069e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 62: No improvement. Patience counter: 23/50\n",
      "Epoch: [62][0/2]\tTime  0.057 ( 0.057)\tData  0.027 ( 0.027)\tLoss 3.9837e+01 (3.9837e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 8.2522e+00 (8.2522e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 63: No improvement. Patience counter: 24/50\n",
      "Epoch: [63][0/2]\tTime  0.059 ( 0.059)\tData  0.029 ( 0.029)\tLoss 1.2025e+01 (1.2025e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 1.3729e+01 (1.3729e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 64: No improvement. Patience counter: 25/50\n",
      "Epoch: [64][0/2]\tTime  0.063 ( 0.063)\tData  0.034 ( 0.034)\tLoss 3.7198e+01 (3.7198e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.061 ( 0.061)\tLoss 5.7977e+00 (5.7977e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 85.000\n",
      "❌ Epoch 65: No improvement. Patience counter: 26/50\n",
      "Epoch: [65][0/2]\tTime  0.060 ( 0.060)\tData  0.026 ( 0.026)\tLoss 3.2659e+01 (3.2659e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.074 ( 0.074)\tLoss 1.2748e+01 (1.2748e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 66: No improvement. Patience counter: 27/50\n",
      "Epoch: [66][0/2]\tTime  0.058 ( 0.058)\tData  0.028 ( 0.028)\tLoss 1.6662e+00 (1.6662e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([45.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 2.6189e+01 (2.6189e+01)\tAcc@1  45.00 ( 45.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 62.500\n",
      "❌ Epoch 67: No improvement. Patience counter: 28/50\n",
      "Epoch: [67][0/2]\tTime  0.058 ( 0.058)\tData  0.028 ( 0.028)\tLoss 2.4927e+01 (2.4927e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([95.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 1.5981e+00 (1.5981e+00)\tAcc@1  95.00 ( 95.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 68: No improvement. Patience counter: 29/50\n",
      "Epoch: [68][0/2]\tTime  0.060 ( 0.060)\tData  0.032 ( 0.032)\tLoss 5.1463e+00 (5.1463e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 1.7948e+01 (1.7948e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 69: No improvement. Patience counter: 30/50\n",
      "Epoch: [69][0/2]\tTime  0.052 ( 0.052)\tData  0.025 ( 0.025)\tLoss 1.6673e+01 (1.6673e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 8.3492e+00 (8.3492e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 70: No improvement. Patience counter: 31/50\n",
      "Epoch: [70][0/2]\tTime  0.058 ( 0.058)\tData  0.028 ( 0.028)\tLoss 7.3631e+00 (7.3631e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.058 ( 0.058)\tLoss 5.8749e+00 (5.8749e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 71: No improvement. Patience counter: 32/50\n",
      "Epoch: [71][0/2]\tTime  0.053 ( 0.053)\tData  0.024 ( 0.024)\tLoss 1.4040e+01 (1.4040e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.061 ( 0.061)\tLoss 9.4654e+00 (9.4654e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 72: No improvement. Patience counter: 33/50\n",
      "Epoch: [72][0/2]\tTime  0.057 ( 0.057)\tData  0.028 ( 0.028)\tLoss 6.2752e+01 (6.2752e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.058 ( 0.058)\tLoss 1.9805e+01 (1.9805e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 62.500\n",
      "❌ Epoch 73: No improvement. Patience counter: 34/50\n",
      "Epoch: [73][0/2]\tTime  0.055 ( 0.055)\tData  0.021 ( 0.021)\tLoss 2.7177e+01 (2.7177e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.071 ( 0.071)\tLoss 1.8878e+01 (1.8878e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 74: No improvement. Patience counter: 35/50\n",
      "Epoch: [74][0/2]\tTime  0.056 ( 0.056)\tData  0.028 ( 0.028)\tLoss 1.0707e+01 (1.0707e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 3.7499e+01 (3.7499e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 75: No improvement. Patience counter: 36/50\n",
      "Epoch: [75][0/2]\tTime  0.058 ( 0.058)\tData  0.030 ( 0.030)\tLoss 4.5542e+01 (4.5542e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 1.1040e+01 (1.1040e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 76: No improvement. Patience counter: 37/50\n",
      "Epoch: [76][0/2]\tTime  0.058 ( 0.058)\tData  0.028 ( 0.028)\tLoss 1.6147e+01 (1.6147e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 2.6932e+01 (2.6932e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 77: No improvement. Patience counter: 38/50\n",
      "Epoch: [77][0/2]\tTime  0.059 ( 0.059)\tData  0.028 ( 0.028)\tLoss 2.3451e+01 (2.3451e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 3.1718e+00 (3.1718e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 78: No improvement. Patience counter: 39/50\n",
      "Epoch: [78][0/2]\tTime  0.052 ( 0.052)\tData  0.027 ( 0.027)\tLoss 1.8726e+01 (1.8726e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 5.5670e+00 (5.5670e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 79: No improvement. Patience counter: 40/50\n",
      "Epoch: [79][0/2]\tTime  0.057 ( 0.057)\tData  0.029 ( 0.029)\tLoss 7.0537e+00 (7.0537e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.056 ( 0.056)\tLoss 1.2238e+01 (1.2238e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 80: No improvement. Patience counter: 41/50\n",
      "Epoch: [80][0/2]\tTime  0.056 ( 0.056)\tData  0.022 ( 0.022)\tLoss 7.6577e+00 (7.6577e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 6.1428e+00 (6.1428e+00)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 81: No improvement. Patience counter: 42/50\n",
      "Epoch: [81][0/2]\tTime  0.046 ( 0.046)\tData  0.024 ( 0.024)\tLoss 1.8407e+01 (1.8407e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 4.0779e+00 (4.0779e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 82: No improvement. Patience counter: 43/50\n",
      "Epoch: [82][0/2]\tTime  0.054 ( 0.054)\tData  0.028 ( 0.028)\tLoss 1.8105e+01 (1.8105e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.055 ( 0.055)\tLoss 7.4790e-01 (7.4790e-01)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 83: No improvement. Patience counter: 44/50\n",
      "Epoch: [83][0/2]\tTime  0.052 ( 0.052)\tData  0.023 ( 0.023)\tLoss 1.2146e+01 (1.2146e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.056 ( 0.056)\tLoss 1.2418e+01 (1.2418e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 84: No improvement. Patience counter: 45/50\n",
      "Epoch: [84][0/2]\tTime  0.059 ( 0.059)\tData  0.029 ( 0.029)\tLoss 1.3101e+01 (1.3101e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 6.1227e+00 (6.1227e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 87.500\n",
      "✅ Epoch 85: New best Acc@1: 87.50. Model saved.\n",
      "Epoch: [85][0/2]\tTime  0.071 ( 0.071)\tData  0.028 ( 0.028)\tLoss 6.8674e+00 (6.8674e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.072 ( 0.072)\tLoss 2.0850e+01 (2.0850e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 86: No improvement. Patience counter: 1/50\n",
      "Epoch: [86][0/2]\tTime  0.055 ( 0.055)\tData  0.026 ( 0.026)\tLoss 3.5933e+01 (3.5933e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 5.6994e+00 (5.6994e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 85.000\n",
      "❌ Epoch 87: No improvement. Patience counter: 2/50\n",
      "Epoch: [87][0/2]\tTime  0.061 ( 0.061)\tData  0.033 ( 0.033)\tLoss 5.0221e+00 (5.0221e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.058 ( 0.058)\tLoss 2.7603e+00 (2.7603e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 88: No improvement. Patience counter: 3/50\n",
      "Epoch: [88][0/2]\tTime  0.057 ( 0.057)\tData  0.029 ( 0.029)\tLoss 4.6691e+00 (4.6691e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 4.0303e+00 (4.0303e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 89: No improvement. Patience counter: 4/50\n",
      "Epoch: [89][0/2]\tTime  0.059 ( 0.059)\tData  0.027 ( 0.027)\tLoss 4.5897e+00 (4.5897e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 1.6812e+00 (1.6812e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 90: No improvement. Patience counter: 5/50\n",
      "Epoch: [90][0/2]\tTime  0.079 ( 0.079)\tData  0.047 ( 0.047)\tLoss 4.6388e+00 (4.6388e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.156 ( 0.156)\tLoss 1.4945e+01 (1.4945e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 91: No improvement. Patience counter: 6/50\n",
      "Epoch: [91][0/2]\tTime  0.057 ( 0.057)\tData  0.025 ( 0.025)\tLoss 2.6040e+01 (2.6040e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 1.7949e+01 (1.7949e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 92: No improvement. Patience counter: 7/50\n",
      "Epoch: [92][0/2]\tTime  0.056 ( 0.056)\tData  0.027 ( 0.027)\tLoss 2.9939e+01 (2.9939e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 3.5426e+00 (3.5426e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 93: No improvement. Patience counter: 8/50\n",
      "Epoch: [93][0/2]\tTime  0.049 ( 0.049)\tData  0.024 ( 0.024)\tLoss 4.8782e+00 (4.8782e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 1.8403e+01 (1.8403e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 94: No improvement. Patience counter: 9/50\n",
      "Epoch: [94][0/2]\tTime  0.054 ( 0.054)\tData  0.025 ( 0.025)\tLoss 2.3107e+01 (2.3107e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 2.6587e+00 (2.6587e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 95: No improvement. Patience counter: 10/50\n",
      "Epoch: [95][0/2]\tTime  0.053 ( 0.053)\tData  0.023 ( 0.023)\tLoss 1.0508e+01 (1.0508e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.052 ( 0.052)\tLoss 5.3252e+00 (5.3252e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 96: No improvement. Patience counter: 11/50\n",
      "Epoch: [96][0/2]\tTime  0.056 ( 0.056)\tData  0.031 ( 0.031)\tLoss 9.2473e+00 (9.2473e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 4.0038e+00 (4.0038e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([95.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 97: No improvement. Patience counter: 12/50\n",
      "Epoch: [97][0/2]\tTime  0.058 ( 0.058)\tData  0.026 ( 0.026)\tLoss 8.5180e-01 (8.5180e-01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 4.9583e+00 (4.9583e+00)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 98: No improvement. Patience counter: 13/50\n",
      "Epoch: [98][0/2]\tTime  0.055 ( 0.055)\tData  0.029 ( 0.029)\tLoss 1.2233e+01 (1.2233e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 9.4616e+00 (9.4616e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 99: No improvement. Patience counter: 14/50\n",
      "Epoch: [99][0/2]\tTime  0.057 ( 0.057)\tData  0.029 ( 0.029)\tLoss 1.0046e+00 (1.0046e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 2.9476e+00 (2.9476e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 100: No improvement. Patience counter: 15/50\n",
      "Epoch: [100][0/2]\tTime  0.063 ( 0.063)\tData  0.033 ( 0.033)\tLoss 3.9276e+00 (3.9276e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 1.7557e+01 (1.7557e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 101: No improvement. Patience counter: 16/50\n",
      "Epoch: [101][0/2]\tTime  0.064 ( 0.064)\tData  0.036 ( 0.036)\tLoss 2.4490e+01 (2.4490e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 3.2929e+00 (3.2929e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 102: No improvement. Patience counter: 17/50\n",
      "Epoch: [102][0/2]\tTime  0.063 ( 0.063)\tData  0.037 ( 0.037)\tLoss 2.8369e+01 (2.8369e+01)\tAcc@1  60.00 ( 60.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.063 ( 0.063)\tLoss 4.3890e+00 (4.3890e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 103: No improvement. Patience counter: 18/50\n",
      "Epoch: [103][0/2]\tTime  0.062 ( 0.062)\tData  0.028 ( 0.028)\tLoss 3.6568e+00 (3.6568e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.069 ( 0.069)\tLoss 7.5049e+00 (7.5049e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 70.000\n",
      "❌ Epoch 104: No improvement. Patience counter: 19/50\n",
      "Epoch: [104][0/2]\tTime  0.058 ( 0.058)\tData  0.028 ( 0.028)\tLoss 6.5924e+00 (6.5924e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 5.4970e+00 (5.4970e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 105: No improvement. Patience counter: 20/50\n",
      "Epoch: [105][0/2]\tTime  0.056 ( 0.056)\tData  0.032 ( 0.032)\tLoss 1.6487e+01 (1.6487e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.056 ( 0.056)\tLoss 1.6944e+01 (1.6944e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 106: No improvement. Patience counter: 21/50\n",
      "Epoch: [106][0/2]\tTime  0.055 ( 0.055)\tData  0.022 ( 0.022)\tLoss 1.0883e+01 (1.0883e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.067 ( 0.067)\tLoss 4.0158e+00 (4.0158e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 107: No improvement. Patience counter: 22/50\n",
      "Epoch: [107][0/2]\tTime  0.049 ( 0.049)\tData  0.021 ( 0.021)\tLoss 5.9664e+00 (5.9664e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 6.7677e+00 (6.7677e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 108: No improvement. Patience counter: 23/50\n",
      "Epoch: [108][0/2]\tTime  0.058 ( 0.058)\tData  0.029 ( 0.029)\tLoss 5.5148e+00 (5.5148e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.063 ( 0.063)\tLoss 1.4807e+01 (1.4807e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 109: No improvement. Patience counter: 24/50\n",
      "Epoch: [109][0/2]\tTime  0.056 ( 0.056)\tData  0.029 ( 0.029)\tLoss 1.2218e+01 (1.2218e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 1.3627e+01 (1.3627e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      " * Acc@1 65.000\n",
      "❌ Epoch 110: No improvement. Patience counter: 25/50\n",
      "Epoch: [110][0/2]\tTime  0.070 ( 0.070)\tData  0.026 ( 0.026)\tLoss 5.9867e+00 (5.9867e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.100 ( 0.100)\tLoss 9.9283e+00 (9.9283e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 111: No improvement. Patience counter: 26/50\n",
      "Epoch: [111][0/2]\tTime  0.061 ( 0.061)\tData  0.027 ( 0.027)\tLoss 1.5977e+01 (1.5977e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 5.3585e+00 (5.3585e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 112: No improvement. Patience counter: 27/50\n",
      "Epoch: [112][0/2]\tTime  0.055 ( 0.055)\tData  0.020 ( 0.020)\tLoss 2.7773e+01 (2.7773e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.053 ( 0.053)\tLoss 2.8111e+01 (2.8111e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 113: No improvement. Patience counter: 28/50\n",
      "Epoch: [113][0/2]\tTime  0.049 ( 0.049)\tData  0.021 ( 0.021)\tLoss 2.3735e+01 (2.3735e+01)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 6.7301e+00 (6.7301e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 114: No improvement. Patience counter: 29/50\n",
      "Epoch: [114][0/2]\tTime  0.054 ( 0.054)\tData  0.026 ( 0.026)\tLoss 8.3366e+00 (8.3366e+00)\tAcc@1  55.00 ( 55.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 5.0965e+00 (5.0965e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 115: No improvement. Patience counter: 30/50\n",
      "Epoch: [115][0/2]\tTime  0.054 ( 0.054)\tData  0.024 ( 0.024)\tLoss 2.8052e+00 (2.8052e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 2.0719e+00 (2.0719e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 116: No improvement. Patience counter: 31/50\n",
      "Epoch: [116][0/2]\tTime  0.053 ( 0.053)\tData  0.028 ( 0.028)\tLoss 7.2641e+00 (7.2641e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.064 ( 0.064)\tLoss 1.6909e+00 (1.6909e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 117: No improvement. Patience counter: 32/50\n",
      "Epoch: [117][0/2]\tTime  0.056 ( 0.056)\tData  0.030 ( 0.030)\tLoss 8.7837e+00 (8.7837e+00)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.063 ( 0.063)\tLoss 4.3674e+00 (4.3674e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 118: No improvement. Patience counter: 33/50\n",
      "Epoch: [118][0/2]\tTime  0.055 ( 0.055)\tData  0.035 ( 0.035)\tLoss 6.4914e+00 (6.4914e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.070 ( 0.070)\tLoss 2.1011e+00 (2.1011e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 87.500\n",
      "❌ Epoch 119: No improvement. Patience counter: 34/50\n",
      "Epoch: [119][0/2]\tTime  0.055 ( 0.055)\tData  0.021 ( 0.021)\tLoss 3.4707e+00 (3.4707e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.056 ( 0.056)\tLoss 2.0930e+00 (2.0930e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 120: No improvement. Patience counter: 35/50\n",
      "Epoch: [120][0/2]\tTime  0.047 ( 0.047)\tData  0.025 ( 0.025)\tLoss 6.6964e+00 (6.6964e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.059 ( 0.059)\tLoss 1.4018e+01 (1.4018e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 121: No improvement. Patience counter: 36/50\n",
      "Epoch: [121][0/2]\tTime  0.055 ( 0.055)\tData  0.021 ( 0.021)\tLoss 3.3039e+00 (3.3039e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.053 ( 0.053)\tLoss 1.1558e+00 (1.1558e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 122: No improvement. Patience counter: 37/50\n",
      "Epoch: [122][0/2]\tTime  0.053 ( 0.053)\tData  0.024 ( 0.024)\tLoss 3.7332e+00 (3.7332e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.058 ( 0.058)\tLoss 1.8472e+00 (1.8472e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 123: No improvement. Patience counter: 38/50\n",
      "Epoch: [123][0/2]\tTime  0.057 ( 0.057)\tData  0.031 ( 0.031)\tLoss 6.7781e+00 (6.7781e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 8.8934e+00 (8.8934e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 124: No improvement. Patience counter: 39/50\n",
      "Epoch: [124][0/2]\tTime  0.061 ( 0.061)\tData  0.033 ( 0.033)\tLoss 5.4113e+00 (5.4113e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.063 ( 0.063)\tLoss 3.6642e+00 (3.6642e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 125: No improvement. Patience counter: 40/50\n",
      "Epoch: [125][0/2]\tTime  0.058 ( 0.058)\tData  0.030 ( 0.030)\tLoss 5.8739e+00 (5.8739e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.060 ( 0.060)\tLoss 2.0288e+00 (2.0288e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 126: No improvement. Patience counter: 41/50\n",
      "Epoch: [126][0/2]\tTime  0.057 ( 0.057)\tData  0.029 ( 0.029)\tLoss 7.7452e+00 (7.7452e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.066 ( 0.066)\tLoss 4.7437e+00 (4.7437e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 127: No improvement. Patience counter: 42/50\n",
      "Epoch: [127][0/2]\tTime  0.062 ( 0.062)\tData  0.034 ( 0.034)\tLoss 1.7436e+01 (1.7436e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 3.4217e+00 (3.4217e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 85.000\n",
      "❌ Epoch 128: No improvement. Patience counter: 43/50\n",
      "Epoch: [128][0/2]\tTime  0.056 ( 0.056)\tData  0.031 ( 0.031)\tLoss 9.5240e+00 (9.5240e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 4.0224e+00 (4.0224e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 77.500\n",
      "❌ Epoch 129: No improvement. Patience counter: 44/50\n",
      "Epoch: [129][0/2]\tTime  0.055 ( 0.055)\tData  0.020 ( 0.020)\tLoss 6.5786e+00 (6.5786e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([65.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.065 ( 0.065)\tLoss 9.6068e+00 (9.6068e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      " * Acc@1 67.500\n",
      "❌ Epoch 130: No improvement. Patience counter: 45/50\n",
      "Epoch: [130][0/2]\tTime  0.057 ( 0.057)\tData  0.025 ( 0.025)\tLoss 3.7689e+00 (3.7689e+00)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.136 ( 0.136)\tLoss 9.1151e+00 (9.1151e+00)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 80.000\n",
      "❌ Epoch 131: No improvement. Patience counter: 46/50\n",
      "Epoch: [131][0/2]\tTime  0.057 ( 0.057)\tData  0.028 ( 0.028)\tLoss 9.4772e+00 (9.4772e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([90.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.062 ( 0.062)\tLoss 2.2008e+00 (2.2008e+00)\tAcc@1  90.00 ( 90.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 132: No improvement. Patience counter: 47/50\n",
      "Epoch: [132][0/2]\tTime  0.063 ( 0.063)\tData  0.035 ( 0.035)\tLoss 8.5702e+00 (8.5702e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 1.5921e+00 (1.5921e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      " * Acc@1 85.000\n",
      "❌ Epoch 133: No improvement. Patience counter: 48/50\n",
      "Epoch: [133][0/2]\tTime  0.057 ( 0.057)\tData  0.025 ( 0.025)\tLoss 8.8691e+00 (8.8691e+00)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.057 ( 0.057)\tLoss 1.5744e+00 (1.5744e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([80.], device='cuda:0')\n",
      " * Acc@1 82.500\n",
      "❌ Epoch 134: No improvement. Patience counter: 49/50\n",
      "Epoch: [134][0/2]\tTime  0.059 ( 0.059)\tData  0.030 ( 0.030)\tLoss 8.9751e+00 (8.9751e+00)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([70.], device='cuda:0')\n",
      "Test: [0/2]\tTime  0.070 ( 0.070)\tLoss 1.1377e+01 (1.1377e+01)\tAcc@1  70.00 ( 70.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      " * Acc@1 72.500\n",
      "❌ Epoch 135: No improvement. Patience counter: 50/50\n",
      "⏹️ Early stopping triggered at epoch 135. Best Acc@1: 87.50\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0.0\n",
    "patience = 50  # Adjust as needed\n",
    "patience_counter = 0\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "start_epoch = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # Train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # Check if current accuracy is the best\n",
    "    is_best = acc1 > best_acc1\n",
    "\n",
    "    if is_best:\n",
    "        best_acc1 = acc1\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Save best model only\n",
    "        save_checkpoint(timestamp, epoch, {\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'vgg16',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best=True)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}: New best Acc@1: {best_acc1:.2f}. Model saved.\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"❌ Epoch {epoch+1}: No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⏹️ Early stopping triggered at epoch {epoch+1}. Best Acc@1: {best_acc1:.2f}\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
