{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a5a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "import simsiam.builder\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "epochs = 200\n",
    "\n",
    "batch_size = 20\n",
    "test_size = 0.5\n",
    "\n",
    "random_indice = 1\n",
    "\n",
    "seeded_run = True\n",
    "seed = 55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25da8a0f-8f90-4f9e-9a04-991692e9ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed has been set\n",
      "seet used: 55\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # PyTorch determinism\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if seeded_run:\n",
    "    set_seed(seed)\n",
    "    print(\"seed has been set\")\n",
    "    print(f\"seet used: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af2ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception_v3', 'list_models', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n",
      "=> creating model 'vgg16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\FathanAbi\\fathanvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\FathanAbi\\fathanvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)\n",
    "# create model\n",
    "arch = 'vgg16' \n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = simsiam.builder.SimSiam(\n",
    "    models.__dict__[arch])\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "init_lr = lr * batch_size / 256\n",
    "gpu = 0\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578786fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: D:\\FathanAbi\\tugas-akhir-model-deteksi-tumpahan-minyakl\\Hyperspectral oil spill detection datasets\\GM01.mat\n",
      "random: 1\n",
      "generating random sample\n",
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "creating 5 Randomly chosen 0 indices:\n",
      "creating 5 Randomly chosen 1 indices:\n",
      "indices 0 used: [(np.int64(144), np.int64(561)), (np.int64(319), np.int64(409)), (np.int64(244), np.int64(479)), (np.int64(1175), np.int64(174)), (np.int64(488), np.int64(196))]\n",
      "indices 1 used: [(np.int64(1102), np.int64(103)), (np.int64(113), np.int64(43)), (np.int64(963), np.int64(39)), (np.int64(160), np.int64(195)), (np.int64(232), np.int64(222))]\n",
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"D:\\FathanAbi\\tugas-akhir-model-deteksi-tumpahan-minyakl\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 0:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1\n",
    "\n",
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random_indice}\")\n",
    "\n",
    "if random_indice:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(188), np.int64(124)), (np.int64(523), np.int64(150)), (np.int64(1003), np.int64(474)), (np.int64(616), np.int64(508)), (np.int64(905), np.int64(552))]\n",
    "    indices_1 = [(np.int64(106), np.int64(606)), (np.int64(297), np.int64(468)), (np.int64(926), np.int64(35)), (np.int64(536), np.int64(519)), (np.int64(508), np.int64(442))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21c1c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-245 -455  358  410  441  591  649  646  622  601  559  504  461  430\n",
      "  400  382  349  326  304  281  246  227  200  177  148  141  127  119\n",
      "  112  108   97   88   91   85   78   72   58   51   40   14   31   45\n",
      "   51   23   37   46   40   38   37   24   22   26   43   40   38   34\n",
      "   38   20    8    1    8   -3 -143  -49    9   19   31   40   40   46\n",
      "   47   39   45   44   47   42   32   27   15  -16 -191 -193  -93  -85\n",
      "  -20    9   25   18   27   18   38   31   43   39   34   47   36   26\n",
      "   22   11   17   10    0  152  -18  -54    0    0    0    0    0    0\n",
      "    0    0 -342 -304  -89  -50 -109  -46   -3   -2    5   10   10   17\n",
      "   16   10    8   19   16    9   18   16   14   15   12   13   14   16\n",
      "   19   20   19   10   17   -1    6    5    1   -9    3   47    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -23  -42  -11    3   -8  -18  -47   -2    8   10    9   12    9   11\n",
      "   14   14   10   18   14   21   17   20   18   18   11   17   16   15\n",
      "   16   13   15   14   18   14    9   12   11   21    6    8    0    0\n",
      "    1  -12   -3  -10  -11    5   -7    2  -30  -20  -16  -29  -40  -21]\n",
      "[-245. -455.  358.  410.  441.  591.  649.  646.  622.  601.  559.  504.\n",
      "  461.  430.  400.  382.  349.  326.  304.  281.  246.  227.  200.  177.\n",
      "  148.  141.  127.  119.  112.  108.   97.   88.   91.   85.   78.   72.\n",
      "   58.   51.   40.   14.   31.   45.   51.   23.   37.   46.   40.   38.\n",
      "   37.   24.   22.   26.   43.   40.   38.   34.   38.   20.    8.    1.\n",
      "    8.   -3. -143.  -49.    9.   19.   31.   40.   40.   46.   47.   39.\n",
      "   45.   44.   47.   42.   32.   27.   15.  -16. -191. -193.  -93.  -85.\n",
      "  -20.    9.   25.   18.   27.   18.   38.   31.   43.   39.   34.   47.\n",
      "   36.   26.   22.   11.   17.   10.    0.  152.  -18.  -54.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -342. -304.  -89.  -50. -109.  -46.\n",
      "   -3.   -2.    5.   10.   10.   17.   16.   10.    8.   19.   16.    9.\n",
      "   18.   16.   14.   15.   12.   13.   14.   16.   19.   20.   19.   10.\n",
      "   17.   -1.    6.    5.    1.   -9.    3.   47.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -23.  -42.  -11.    3.   -8.  -18.  -47.   -2.    8.   10.    9.   12.\n",
      "    9.   11.   14.   14.   10.   18.   14.   21.   17.   20.   18.   18.\n",
      "   11.   17.   16.   15.   16.   13.   15.   14.   18.   14.    9.   12.\n",
      "   11.   21.    6.    8.    0.    0.    1.  -12.   -3.  -10.  -11.    5.\n",
      "   -7.    2.  -30.  -20.  -16.  -29.  -40.  -21.]\n",
      "[-244 -460  355  379  412  545  642  657  659  640  607  560  517  487\n",
      "  460  438  421  405  392  370  355  336  328  314  298  289  274  270\n",
      "  263  262  259  249  249  241  239  222  210  202  200  168  162  171\n",
      "  177  154  175  177  172  168  171  163  151  157  163  157  156  146\n",
      "  143  117   92   80   75   23  -89    7   78   91  102  119  132  137\n",
      "  141  139  148  141  144  127  112   96   78   37 -131 -169  -64  -81\n",
      "    7   47   62   70   85   90  103  103  109  113  107  119  113   95\n",
      "  110   90   80   78   74  177   41   10    0    0    0    0    0    0\n",
      "    0    0   13 -185  -18  -47  -30  -20   16   40   55   60   71   77\n",
      "   86   82   81   88   86   85   86   89   88   93   81   81   84   87\n",
      "   88   90   92   91   87   67   63   54   44   38   20   -6    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -40 -102    0   14   19   23    2   39   47   42   47   56   56   61\n",
      "   58   60   69   65   70   65   71   64   69   68   64   66   74   67\n",
      "   71   69   66   64   69   77   82   73   72   45   49   43   30   19\n",
      "   37   13   27   11   19   24    6   -4  -32   -2  -30  -54  -47  -23]\n",
      "[-244. -460.  355.  379.  412.  545.  642.  657.  659.  640.  607.  560.\n",
      "  517.  487.  460.  438.  421.  405.  392.  370.  355.  336.  328.  314.\n",
      "  298.  289.  274.  270.  263.  262.  259.  249.  249.  241.  239.  222.\n",
      "  210.  202.  200.  168.  162.  171.  177.  154.  175.  177.  172.  168.\n",
      "  171.  163.  151.  157.  163.  157.  156.  146.  143.  117.   92.   80.\n",
      "   75.   23.  -89.    7.   78.   91.  102.  119.  132.  137.  141.  139.\n",
      "  148.  141.  144.  127.  112.   96.   78.   37. -131. -169.  -64.  -81.\n",
      "    7.   47.   62.   70.   85.   90.  103.  103.  109.  113.  107.  119.\n",
      "  113.   95.  110.   90.   80.   78.   74.  177.   41.   10.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.   13. -185.  -18.  -47.  -30.  -20.\n",
      "   16.   40.   55.   60.   71.   77.   86.   82.   81.   88.   86.   85.\n",
      "   86.   89.   88.   93.   81.   81.   84.   87.   88.   90.   92.   91.\n",
      "   87.   67.   63.   54.   44.   38.   20.   -6.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -40. -102.    0.   14.   19.   23.    2.   39.   47.   42.   47.   56.\n",
      "   56.   61.   58.   60.   69.   65.   70.   65.   71.   64.   69.   68.\n",
      "   64.   66.   74.   67.   71.   69.   66.   64.   69.   77.   82.   73.\n",
      "   72.   45.   49.   43.   30.   19.   37.   13.   27.   11.   19.   24.\n",
      "    6.   -4.  -32.   -2.  -30.  -54.  -47.  -23.]\n",
      "[-146 -242  318  372  409  560  636  647  624  582  558  521  469  445\n",
      "  415  389  366  339  314  283  259  237  214  187  166  162  148  136\n",
      "  131  125  124  114  112  109   98   88   70   65   53   17   38   55\n",
      "   63   26   43   50   44   40   37   20   23   31   43   43   41   35\n",
      "   35   17    5    0    1  -29 -158  -60    9   17   29   35   39   42\n",
      "   43   43   41   37   38   31   24   21    6  -15 -205 -205 -103 -109\n",
      "  -34    2   12    9   17   10   23   21   34   28   31   36   32   29\n",
      "   36   20   18   14    9   72  -41  -53    0    0    0    0    0    0\n",
      "    0    0 -222 -211  -78  -33  -28  -53   -2   15   15   22   24   26\n",
      "   27   33   28   28   32   26   32   26   28   29   24   27   26   22\n",
      "   32   32   33   25   20   25   10   13   12  -25  -36 -118    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " -108   11   -1    3    5    6    1   -1   13   24   26   36   32   20\n",
      "   26   28   31   26   28   28   28   24   31   27   25   22   29   32\n",
      "   24   26   20   23   27   28   25   29   26   20   17    9    7   13\n",
      "   -5   21   -3   -3  -11  -24    0  -14   28    6  -34  -16    9 -124]\n",
      "[-146. -242.  318.  372.  409.  560.  636.  647.  624.  582.  558.  521.\n",
      "  469.  445.  415.  389.  366.  339.  314.  283.  259.  237.  214.  187.\n",
      "  166.  162.  148.  136.  131.  125.  124.  114.  112.  109.   98.   88.\n",
      "   70.   65.   53.   17.   38.   55.   63.   26.   43.   50.   44.   40.\n",
      "   37.   20.   23.   31.   43.   43.   41.   35.   35.   17.    5.    0.\n",
      "    1.  -29. -158.  -60.    9.   17.   29.   35.   39.   42.   43.   43.\n",
      "   41.   37.   38.   31.   24.   21.    6.  -15. -205. -205. -103. -109.\n",
      "  -34.    2.   12.    9.   17.   10.   23.   21.   34.   28.   31.   36.\n",
      "   32.   29.   36.   20.   18.   14.    9.   72.  -41.  -53.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -222. -211.  -78.  -33.  -28.  -53.\n",
      "   -2.   15.   15.   22.   24.   26.   27.   33.   28.   28.   32.   26.\n",
      "   32.   26.   28.   29.   24.   27.   26.   22.   32.   32.   33.   25.\n",
      "   20.   25.   10.   13.   12.  -25.  -36. -118.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " -108.   11.   -1.    3.    5.    6.    1.   -1.   13.   24.   26.   36.\n",
      "   32.   20.   26.   28.   31.   26.   28.   28.   28.   24.   31.   27.\n",
      "   25.   22.   29.   32.   24.   26.   20.   23.   27.   28.   25.   29.\n",
      "   26.   20.   17.    9.    7.   13.   -5.   21.   -3.   -3.  -11.  -24.\n",
      "    0.  -14.   28.    6.  -34.  -16.    9. -124.]\n",
      "[-135 -180  313  378  420  575  652  675  657  631  599  551  526  494\n",
      "  471  456  432  418  404  378  360  348  334  316  298  292  283  277\n",
      "  278  270  266  265  262  257  248  234  224  212  205  175  178  183\n",
      "  190  170  181  184  177  175  175  160  159  161  169  165  164  157\n",
      "  157  127  109   93   86   44  -49   47  101  116  130  144  157  160\n",
      "  166  165  166  161  160  145  129  112   89   52 -125 -181  -52  -27\n",
      "   31   63   82   94  110  111  129  134  137  133  138  149  145  150\n",
      "  134  135  127  103  102   62   64   30    0    0    0    0    0    0\n",
      "    0    0 -105 -304  -63    9  -11   14   43   66   75   88   99  108\n",
      "  119  114  123  124  126  128  127  127  124  125  124  116  120  123\n",
      "  121  124  120  108  113   94   86   74   87   63   33  -62    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " -177 -153   25   24   32   36   23   35   70   76   73   85   83   90\n",
      "   98   93  101   98  105   99  101  100  106   97  103   93   99  103\n",
      "   95  103  110  106  109  107  107  112   91   99   87   73   74   72\n",
      "   65   56   37   48   29   38   22   12   12    6  -18  -47 -134  -36]\n",
      "[-135. -180.  313.  378.  420.  575.  652.  675.  657.  631.  599.  551.\n",
      "  526.  494.  471.  456.  432.  418.  404.  378.  360.  348.  334.  316.\n",
      "  298.  292.  283.  277.  278.  270.  266.  265.  262.  257.  248.  234.\n",
      "  224.  212.  205.  175.  178.  183.  190.  170.  181.  184.  177.  175.\n",
      "  175.  160.  159.  161.  169.  165.  164.  157.  157.  127.  109.   93.\n",
      "   86.   44.  -49.   47.  101.  116.  130.  144.  157.  160.  166.  165.\n",
      "  166.  161.  160.  145.  129.  112.   89.   52. -125. -181.  -52.  -27.\n",
      "   31.   63.   82.   94.  110.  111.  129.  134.  137.  133.  138.  149.\n",
      "  145.  150.  134.  135.  127.  103.  102.   62.   64.   30.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -105. -304.  -63.    9.  -11.   14.\n",
      "   43.   66.   75.   88.   99.  108.  119.  114.  123.  124.  126.  128.\n",
      "  127.  127.  124.  125.  124.  116.  120.  123.  121.  124.  120.  108.\n",
      "  113.   94.   86.   74.   87.   63.   33.  -62.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " -177. -153.   25.   24.   32.   36.   23.   35.   70.   76.   73.   85.\n",
      "   83.   90.   98.   93.  101.   98.  105.   99.  101.  100.  106.   97.\n",
      "  103.   93.   99.  103.   95.  103.  110.  106.  109.  107.  107.  112.\n",
      "   91.   99.   87.   73.   74.   72.   65.   56.   37.   48.   29.   38.\n",
      "   22.   12.   12.    6.  -18.  -47. -134.  -36.]\n"
     ]
    }
   ],
   "source": [
    "i =1\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])\n",
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828871fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi 1 shape: (20, 9, 9, 224)\n",
      "label augmentai 1 shape: (20,)\n",
      "hasil augmentasi 2 shape: (20, 9, 9, 224)\n",
      "label augmentasi 2 shape: (20,)\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "hasil augmentasi gabungan untuk training: (40, 9, 9, 224)\n",
      "label augmentasi gabungan: (40,)\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# # Count occurrences of each unique element\n",
    "# counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# # Print results\n",
    "# for i, count in enumerate(counts1):\n",
    "#     print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "# counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# # Print results\n",
    "# for i, count in enumerate(counts2):\n",
    "#     print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "# print(label_augment1[3])\n",
    "\n",
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ffc071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[-0.0525,  0.0192, -0.0105,  ...,  0.0196,  0.0282, -0.0058]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0526,  0.0191, -0.0105,  ...,  0.0196,  0.0283, -0.0058]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0000, 0.0388, 0.0000,  ..., 0.0191, 0.0907, 0.0000]])\n",
      "tensor([[0.0000, 0.0388, 0.0000,  ..., 0.0191, 0.0911, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0224d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd6786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([40, 224, 9, 9])\n",
      "generate data loader using seed\n",
      "bacth size: torch.Size([20, 224, 9, 9])\n",
      "length batch: 20\n",
      "Train loader size: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "            img2 = self.transform(img)  # Second augmentation\n",
    "        \n",
    "            return img1, img2  # Return both augmented versions\n",
    "        \n",
    "        return img, img  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pretrain_preloaded_image = data_augment \n",
    "\n",
    "pretrain_X_train = torch.tensor(pretrain_preloaded_image)\n",
    "pretrain_X_train = pretrain_X_train.to(torch.float32)\n",
    "pretrain_X_train = pretrain_X_train.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {pretrain_X_train.shape}\")\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example normalization\n",
    "])\n",
    "\n",
    "pretrain_train_dataset = CustomDataset(pretrain_X_train, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "if seeded_run:\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    pretrain_train_loader = DataLoader(\n",
    "        pretrain_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    print(\"generate data loader using seed\")\n",
    "else:\n",
    "    pretrain_train_loader = DataLoader(\n",
    "        pretrain_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1, batch2 = next(iter(pretrain_train_loader))\n",
    "\n",
    "print(f\"bacth size: {batch1.size()}\")\n",
    "print(f\"length batch: {len(batch1)}\")  # Should print 2 (Two transformed views per image)\n",
    "print(f\"Train loader size: {len(pretrain_train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c59999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr\n",
    "\n",
    "class Pretrain_AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "\n",
    "class Pretrain_ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def pretrain_save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6abedcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = Pretrain_AverageMeter('Time', ':6.3f')\n",
    "    data_time = Pretrain_AverageMeter('Data', ':6.3f')\n",
    "    losses = Pretrain_AverageMeter('Loss', ':.4f')\n",
    "    progress = Pretrain_ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images1, images2) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input1 = images1.to(device, non_blocking=True)\n",
    "        input2 = images2.to(device, non_blocking=True)\n",
    "\n",
    "        p1, p2, z1, z2 = model(x1=input1, x2=input2) \n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), input1.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "    # Return average training loss for early stopping\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1714672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/2]\tTime  0.903 ( 0.903)\tData  0.004 ( 0.004)\tLoss 0.0008 (0.0008)\n",
      "Epoch 1: Average Training Loss: 0.001501\n",
      "✅ New best model saved with loss 0.001501\n",
      "Epoch: [1][0/2]\tTime  0.015 ( 0.015)\tData  0.004 ( 0.004)\tLoss -0.0025 (-0.0025)\n",
      "Epoch 2: Average Training Loss: -0.001311\n",
      "✅ New best model saved with loss -0.001311\n",
      "Epoch: [2][0/2]\tTime  0.030 ( 0.030)\tData  0.000 ( 0.000)\tLoss 0.0008 (0.0008)\n",
      "Epoch 3: Average Training Loss: 0.000291\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [3][0/2]\tTime  0.094 ( 0.094)\tData  0.015 ( 0.015)\tLoss 0.0044 (0.0044)\n",
      "Epoch 4: Average Training Loss: 0.000138\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [4][0/2]\tTime  0.078 ( 0.078)\tData  0.000 ( 0.000)\tLoss 0.0001 (0.0001)\n",
      "Epoch 5: Average Training Loss: -0.002136\n",
      "✅ New best model saved with loss -0.002136\n",
      "Epoch: [5][0/2]\tTime  0.022 ( 0.022)\tData  0.003 ( 0.003)\tLoss 0.0036 (0.0036)\n",
      "Epoch 6: Average Training Loss: 0.001960\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [6][0/2]\tTime  0.084 ( 0.084)\tData  0.014 ( 0.014)\tLoss 0.0004 (0.0004)\n",
      "Epoch 7: Average Training Loss: -0.000849\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [7][0/2]\tTime  0.081 ( 0.081)\tData  0.003 ( 0.003)\tLoss 0.0008 (0.0008)\n",
      "Epoch 8: Average Training Loss: -0.001569\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [8][0/2]\tTime  0.087 ( 0.087)\tData  0.002 ( 0.002)\tLoss 0.0021 (0.0021)\n",
      "Epoch 9: Average Training Loss: 0.002186\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [9][0/2]\tTime  0.072 ( 0.072)\tData  0.002 ( 0.002)\tLoss -0.0029 (-0.0029)\n",
      "Epoch 10: Average Training Loss: -0.000215\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [10][0/2]\tTime  0.079 ( 0.079)\tData  0.004 ( 0.004)\tLoss -0.0036 (-0.0036)\n",
      "Epoch 11: Average Training Loss: 0.000092\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [11][0/2]\tTime  0.087 ( 0.087)\tData  0.003 ( 0.003)\tLoss -0.0010 (-0.0010)\n",
      "Epoch 12: Average Training Loss: 0.000104\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [12][0/2]\tTime  0.092 ( 0.092)\tData  0.000 ( 0.000)\tLoss -0.0028 (-0.0028)\n",
      "Epoch 13: Average Training Loss: -0.000473\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [13][0/2]\tTime  0.081 ( 0.081)\tData  0.000 ( 0.000)\tLoss 0.0019 (0.0019)\n",
      "Epoch 14: Average Training Loss: 0.001276\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [14][0/2]\tTime  0.079 ( 0.079)\tData  0.000 ( 0.000)\tLoss -0.0062 (-0.0062)\n",
      "Epoch 15: Average Training Loss: -0.004946\n",
      "✅ New best model saved with loss -0.004946\n",
      "Epoch: [15][0/2]\tTime  0.032 ( 0.032)\tData  0.003 ( 0.003)\tLoss 0.0041 (0.0041)\n",
      "Epoch 16: Average Training Loss: 0.003123\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [16][0/2]\tTime  0.095 ( 0.095)\tData  0.016 ( 0.016)\tLoss 0.0013 (0.0013)\n",
      "Epoch 17: Average Training Loss: 0.001932\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [17][0/2]\tTime  0.084 ( 0.084)\tData  0.005 ( 0.005)\tLoss 0.0001 (0.0001)\n",
      "Epoch 18: Average Training Loss: 0.000071\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [18][0/2]\tTime  0.084 ( 0.084)\tData  0.000 ( 0.000)\tLoss 0.0013 (0.0013)\n",
      "Epoch 19: Average Training Loss: 0.002979\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [19][0/2]\tTime  0.091 ( 0.091)\tData  0.000 ( 0.000)\tLoss 0.0026 (0.0026)\n",
      "Epoch 20: Average Training Loss: 0.001428\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [20][0/2]\tTime  0.089 ( 0.089)\tData  0.018 ( 0.018)\tLoss 0.0023 (0.0023)\n",
      "Epoch 21: Average Training Loss: 0.002103\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [21][0/2]\tTime  0.086 ( 0.086)\tData  0.003 ( 0.003)\tLoss 0.0029 (0.0029)\n",
      "Epoch 22: Average Training Loss: 0.002643\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [22][0/2]\tTime  0.092 ( 0.092)\tData  0.016 ( 0.016)\tLoss -0.0038 (-0.0038)\n",
      "Epoch 23: Average Training Loss: -0.002452\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [23][0/2]\tTime  0.084 ( 0.084)\tData  0.003 ( 0.003)\tLoss 0.0003 (0.0003)\n",
      "Epoch 24: Average Training Loss: 0.002137\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [24][0/2]\tTime  0.078 ( 0.078)\tData  0.004 ( 0.004)\tLoss 0.0015 (0.0015)\n",
      "Epoch 25: Average Training Loss: 0.000171\n",
      "❌ No improvement. Patience: 10/50\n",
      "Epoch: [25][0/2]\tTime  0.084 ( 0.084)\tData  0.002 ( 0.002)\tLoss 0.0044 (0.0044)\n",
      "Epoch 26: Average Training Loss: 0.001333\n",
      "❌ No improvement. Patience: 11/50\n",
      "Epoch: [26][0/2]\tTime  0.079 ( 0.079)\tData  0.003 ( 0.003)\tLoss -0.0017 (-0.0017)\n",
      "Epoch 27: Average Training Loss: -0.001211\n",
      "❌ No improvement. Patience: 12/50\n",
      "Epoch: [27][0/2]\tTime  0.074 ( 0.074)\tData  0.006 ( 0.006)\tLoss -0.0052 (-0.0052)\n",
      "Epoch 28: Average Training Loss: -0.001036\n",
      "❌ No improvement. Patience: 13/50\n",
      "Epoch: [28][0/2]\tTime  0.075 ( 0.075)\tData  0.005 ( 0.005)\tLoss -0.0031 (-0.0031)\n",
      "Epoch 29: Average Training Loss: -0.001644\n",
      "❌ No improvement. Patience: 14/50\n",
      "Epoch: [29][0/2]\tTime  0.081 ( 0.081)\tData  0.000 ( 0.000)\tLoss -0.0029 (-0.0029)\n",
      "Epoch 30: Average Training Loss: -0.001796\n",
      "❌ No improvement. Patience: 15/50\n",
      "Epoch: [30][0/2]\tTime  0.094 ( 0.094)\tData  0.000 ( 0.000)\tLoss -0.0044 (-0.0044)\n",
      "Epoch 31: Average Training Loss: -0.002312\n",
      "❌ No improvement. Patience: 16/50\n",
      "Epoch: [31][0/2]\tTime  0.075 ( 0.075)\tData  0.003 ( 0.003)\tLoss 0.0002 (0.0002)\n",
      "Epoch 32: Average Training Loss: 0.002690\n",
      "❌ No improvement. Patience: 17/50\n",
      "Epoch: [32][0/2]\tTime  0.086 ( 0.086)\tData  0.008 ( 0.008)\tLoss 0.0020 (0.0020)\n",
      "Epoch 33: Average Training Loss: 0.000688\n",
      "❌ No improvement. Patience: 18/50\n",
      "Epoch: [33][0/2]\tTime  0.089 ( 0.089)\tData  0.002 ( 0.002)\tLoss -0.0042 (-0.0042)\n",
      "Epoch 34: Average Training Loss: -0.003245\n",
      "❌ No improvement. Patience: 19/50\n",
      "Epoch: [34][0/2]\tTime  0.087 ( 0.087)\tData  0.016 ( 0.016)\tLoss 0.0005 (0.0005)\n",
      "Epoch 35: Average Training Loss: 0.001508\n",
      "❌ No improvement. Patience: 20/50\n",
      "Epoch: [35][0/2]\tTime  0.076 ( 0.076)\tData  0.000 ( 0.000)\tLoss -0.0060 (-0.0060)\n",
      "Epoch 36: Average Training Loss: -0.002666\n",
      "❌ No improvement. Patience: 21/50\n",
      "Epoch: [36][0/2]\tTime  0.082 ( 0.082)\tData  0.000 ( 0.000)\tLoss -0.0023 (-0.0023)\n",
      "Epoch 37: Average Training Loss: -0.000286\n",
      "❌ No improvement. Patience: 22/50\n",
      "Epoch: [37][0/2]\tTime  0.084 ( 0.084)\tData  0.000 ( 0.000)\tLoss -0.0020 (-0.0020)\n",
      "Epoch 38: Average Training Loss: -0.002230\n",
      "❌ No improvement. Patience: 23/50\n",
      "Epoch: [38][0/2]\tTime  0.070 ( 0.070)\tData  0.004 ( 0.004)\tLoss -0.0022 (-0.0022)\n",
      "Epoch 39: Average Training Loss: 0.000288\n",
      "❌ No improvement. Patience: 24/50\n",
      "Epoch: [39][0/2]\tTime  0.079 ( 0.079)\tData  0.000 ( 0.000)\tLoss -0.0007 (-0.0007)\n",
      "Epoch 40: Average Training Loss: -0.002539\n",
      "❌ No improvement. Patience: 25/50\n",
      "Epoch: [40][0/2]\tTime  0.095 ( 0.095)\tData  0.000 ( 0.000)\tLoss -0.0004 (-0.0004)\n",
      "Epoch 41: Average Training Loss: 0.001973\n",
      "❌ No improvement. Patience: 26/50\n",
      "Epoch: [41][0/2]\tTime  0.089 ( 0.089)\tData  0.015 ( 0.015)\tLoss 0.0011 (0.0011)\n",
      "Epoch 42: Average Training Loss: -0.000027\n",
      "❌ No improvement. Patience: 27/50\n",
      "Epoch: [42][0/2]\tTime  0.094 ( 0.094)\tData  0.014 ( 0.014)\tLoss 0.0004 (0.0004)\n",
      "Epoch 43: Average Training Loss: 0.001371\n",
      "❌ No improvement. Patience: 28/50\n",
      "Epoch: [43][0/2]\tTime  0.084 ( 0.084)\tData  0.000 ( 0.000)\tLoss -0.0007 (-0.0007)\n",
      "Epoch 44: Average Training Loss: -0.001022\n",
      "❌ No improvement. Patience: 29/50\n",
      "Epoch: [44][0/2]\tTime  0.079 ( 0.079)\tData  0.003 ( 0.003)\tLoss 0.0023 (0.0023)\n",
      "Epoch 45: Average Training Loss: 0.002691\n",
      "❌ No improvement. Patience: 30/50\n",
      "Epoch: [45][0/2]\tTime  0.082 ( 0.082)\tData  0.003 ( 0.003)\tLoss 0.0004 (0.0004)\n",
      "Epoch 46: Average Training Loss: 0.001266\n",
      "❌ No improvement. Patience: 31/50\n",
      "Epoch: [46][0/2]\tTime  0.084 ( 0.084)\tData  0.000 ( 0.000)\tLoss -0.0006 (-0.0006)\n",
      "Epoch 47: Average Training Loss: -0.002387\n",
      "❌ No improvement. Patience: 32/50\n",
      "Epoch: [47][0/2]\tTime  0.079 ( 0.079)\tData  0.003 ( 0.003)\tLoss -0.0039 (-0.0039)\n",
      "Epoch 48: Average Training Loss: -0.003190\n",
      "❌ No improvement. Patience: 33/50\n",
      "Epoch: [48][0/2]\tTime  0.091 ( 0.091)\tData  0.010 ( 0.010)\tLoss 0.0021 (0.0021)\n",
      "Epoch 49: Average Training Loss: 0.000967\n",
      "❌ No improvement. Patience: 34/50\n",
      "Epoch: [49][0/2]\tTime  0.079 ( 0.079)\tData  0.000 ( 0.000)\tLoss -0.0031 (-0.0031)\n",
      "Epoch 50: Average Training Loss: -0.001889\n",
      "❌ No improvement. Patience: 35/50\n",
      "Epoch: [50][0/2]\tTime  0.072 ( 0.072)\tData  0.003 ( 0.003)\tLoss -0.0090 (-0.0090)\n",
      "Epoch 51: Average Training Loss: -0.004513\n",
      "❌ No improvement. Patience: 36/50\n",
      "Epoch: [51][0/2]\tTime  0.084 ( 0.084)\tData  0.010 ( 0.010)\tLoss -0.0037 (-0.0037)\n",
      "Epoch 52: Average Training Loss: -0.002680\n",
      "❌ No improvement. Patience: 37/50\n",
      "Epoch: [52][0/2]\tTime  0.079 ( 0.079)\tData  0.000 ( 0.000)\tLoss 0.0003 (0.0003)\n",
      "Epoch 53: Average Training Loss: -0.000686\n",
      "❌ No improvement. Patience: 38/50\n",
      "Epoch: [53][0/2]\tTime  0.073 ( 0.073)\tData  0.003 ( 0.003)\tLoss -0.0005 (-0.0005)\n",
      "Epoch 54: Average Training Loss: -0.001637\n",
      "❌ No improvement. Patience: 39/50\n",
      "Epoch: [54][0/2]\tTime  0.091 ( 0.091)\tData  0.000 ( 0.000)\tLoss 0.0063 (0.0063)\n",
      "Epoch 55: Average Training Loss: 0.003023\n",
      "❌ No improvement. Patience: 40/50\n",
      "Epoch: [55][0/2]\tTime  0.083 ( 0.083)\tData  0.000 ( 0.000)\tLoss 0.0031 (0.0031)\n",
      "Epoch 56: Average Training Loss: 0.003294\n",
      "❌ No improvement. Patience: 41/50\n",
      "Epoch: [56][0/2]\tTime  0.085 ( 0.085)\tData  0.003 ( 0.003)\tLoss -0.0017 (-0.0017)\n",
      "Epoch 57: Average Training Loss: -0.002342\n",
      "❌ No improvement. Patience: 42/50\n",
      "Epoch: [57][0/2]\tTime  0.086 ( 0.086)\tData  0.000 ( 0.000)\tLoss 0.0026 (0.0026)\n",
      "Epoch 58: Average Training Loss: -0.001804\n",
      "❌ No improvement. Patience: 43/50\n",
      "Epoch: [58][0/2]\tTime  0.066 ( 0.066)\tData  0.000 ( 0.000)\tLoss -0.0043 (-0.0043)\n",
      "Epoch 59: Average Training Loss: -0.000605\n",
      "❌ No improvement. Patience: 44/50\n",
      "Epoch: [59][0/2]\tTime  0.079 ( 0.079)\tData  0.004 ( 0.004)\tLoss -0.0031 (-0.0031)\n",
      "Epoch 60: Average Training Loss: -0.003912\n",
      "❌ No improvement. Patience: 45/50\n",
      "Epoch: [60][0/2]\tTime  0.098 ( 0.098)\tData  0.000 ( 0.000)\tLoss -0.0080 (-0.0080)\n",
      "Epoch 61: Average Training Loss: -0.005310\n",
      "✅ New best model saved with loss -0.005310\n",
      "Epoch: [61][0/2]\tTime  0.021 ( 0.021)\tData  0.000 ( 0.000)\tLoss -0.0015 (-0.0015)\n",
      "Epoch 62: Average Training Loss: -0.002148\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [62][0/2]\tTime  0.089 ( 0.089)\tData  0.016 ( 0.016)\tLoss 0.0006 (0.0006)\n",
      "Epoch 63: Average Training Loss: 0.000385\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [63][0/2]\tTime  0.075 ( 0.075)\tData  0.004 ( 0.004)\tLoss -0.0019 (-0.0019)\n",
      "Epoch 64: Average Training Loss: -0.001723\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [64][0/2]\tTime  0.082 ( 0.082)\tData  0.006 ( 0.006)\tLoss 0.0014 (0.0014)\n",
      "Epoch 65: Average Training Loss: 0.000779\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [65][0/2]\tTime  0.078 ( 0.078)\tData  0.000 ( 0.000)\tLoss -0.0036 (-0.0036)\n",
      "Epoch 66: Average Training Loss: -0.002059\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [66][0/2]\tTime  0.070 ( 0.070)\tData  0.003 ( 0.003)\tLoss -0.0014 (-0.0014)\n",
      "Epoch 67: Average Training Loss: -0.004130\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [67][0/2]\tTime  0.096 ( 0.096)\tData  0.011 ( 0.011)\tLoss 0.0028 (0.0028)\n",
      "Epoch 68: Average Training Loss: 0.000447\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [68][0/2]\tTime  0.089 ( 0.089)\tData  0.015 ( 0.015)\tLoss -0.0067 (-0.0067)\n",
      "Epoch 69: Average Training Loss: -0.003396\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [69][0/2]\tTime  0.076 ( 0.076)\tData  0.003 ( 0.003)\tLoss -0.0003 (-0.0003)\n",
      "Epoch 70: Average Training Loss: -0.000267\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [70][0/2]\tTime  0.081 ( 0.081)\tData  0.001 ( 0.001)\tLoss -0.0034 (-0.0034)\n",
      "Epoch 71: Average Training Loss: -0.001791\n",
      "❌ No improvement. Patience: 10/50\n",
      "Epoch: [71][0/2]\tTime  0.091 ( 0.091)\tData  0.000 ( 0.000)\tLoss -0.0030 (-0.0030)\n",
      "Epoch 72: Average Training Loss: -0.002664\n",
      "❌ No improvement. Patience: 11/50\n",
      "Epoch: [72][0/2]\tTime  0.082 ( 0.082)\tData  0.003 ( 0.003)\tLoss -0.0064 (-0.0064)\n",
      "Epoch 73: Average Training Loss: -0.005028\n",
      "❌ No improvement. Patience: 12/50\n",
      "Epoch: [73][0/2]\tTime  0.086 ( 0.086)\tData  0.000 ( 0.000)\tLoss -0.0037 (-0.0037)\n",
      "Epoch 74: Average Training Loss: -0.003821\n",
      "❌ No improvement. Patience: 13/50\n",
      "Epoch: [74][0/2]\tTime  0.067 ( 0.067)\tData  0.000 ( 0.000)\tLoss 0.0033 (0.0033)\n",
      "Epoch 75: Average Training Loss: 0.000585\n",
      "❌ No improvement. Patience: 14/50\n",
      "Epoch: [75][0/2]\tTime  0.078 ( 0.078)\tData  0.004 ( 0.004)\tLoss -0.0020 (-0.0020)\n",
      "Epoch 76: Average Training Loss: -0.001095\n",
      "❌ No improvement. Patience: 15/50\n",
      "Epoch: [76][0/2]\tTime  0.086 ( 0.086)\tData  0.000 ( 0.000)\tLoss 0.0009 (0.0009)\n",
      "Epoch 77: Average Training Loss: -0.000389\n",
      "❌ No improvement. Patience: 16/50\n",
      "Epoch: [77][0/2]\tTime  0.080 ( 0.080)\tData  0.009 ( 0.009)\tLoss -0.0005 (-0.0005)\n",
      "Epoch 78: Average Training Loss: -0.002194\n",
      "❌ No improvement. Patience: 17/50\n",
      "Epoch: [78][0/2]\tTime  0.069 ( 0.069)\tData  0.004 ( 0.004)\tLoss -0.0081 (-0.0081)\n",
      "Epoch 79: Average Training Loss: -0.010840\n",
      "✅ New best model saved with loss -0.010840\n",
      "Epoch: [79][0/2]\tTime  0.026 ( 0.026)\tData  0.000 ( 0.000)\tLoss 0.0000 (0.0000)\n",
      "Epoch 80: Average Training Loss: -0.002886\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [80][0/2]\tTime  0.080 ( 0.080)\tData  0.016 ( 0.016)\tLoss 0.0025 (0.0025)\n",
      "Epoch 81: Average Training Loss: 0.003300\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [81][0/2]\tTime  0.085 ( 0.085)\tData  0.000 ( 0.000)\tLoss -0.0007 (-0.0007)\n",
      "Epoch 82: Average Training Loss: -0.002833\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [82][0/2]\tTime  0.082 ( 0.082)\tData  0.000 ( 0.000)\tLoss 0.0013 (0.0013)\n",
      "Epoch 83: Average Training Loss: -0.000470\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [83][0/2]\tTime  0.083 ( 0.083)\tData  0.000 ( 0.000)\tLoss -0.0010 (-0.0010)\n",
      "Epoch 84: Average Training Loss: -0.000040\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [84][0/2]\tTime  0.091 ( 0.091)\tData  0.010 ( 0.010)\tLoss -0.0046 (-0.0046)\n",
      "Epoch 85: Average Training Loss: -0.004694\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [85][0/2]\tTime  0.086 ( 0.086)\tData  0.000 ( 0.000)\tLoss -0.0031 (-0.0031)\n",
      "Epoch 86: Average Training Loss: -0.002009\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [86][0/2]\tTime  0.079 ( 0.079)\tData  0.000 ( 0.000)\tLoss -0.0038 (-0.0038)\n",
      "Epoch 87: Average Training Loss: -0.005017\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [87][0/2]\tTime  0.075 ( 0.075)\tData  0.008 ( 0.008)\tLoss -0.0058 (-0.0058)\n",
      "Epoch 88: Average Training Loss: -0.004027\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [88][0/2]\tTime  0.081 ( 0.081)\tData  0.000 ( 0.000)\tLoss -0.0034 (-0.0034)\n",
      "Epoch 89: Average Training Loss: -0.002801\n",
      "❌ No improvement. Patience: 10/50\n",
      "Epoch: [89][0/2]\tTime  0.089 ( 0.089)\tData  0.000 ( 0.000)\tLoss -0.0085 (-0.0085)\n",
      "Epoch 90: Average Training Loss: -0.002816\n",
      "❌ No improvement. Patience: 11/50\n",
      "Epoch: [90][0/2]\tTime  0.074 ( 0.074)\tData  0.004 ( 0.004)\tLoss -0.0004 (-0.0004)\n",
      "Epoch 91: Average Training Loss: -0.000338\n",
      "❌ No improvement. Patience: 12/50\n",
      "Epoch: [91][0/2]\tTime  0.082 ( 0.082)\tData  0.012 ( 0.012)\tLoss -0.0033 (-0.0033)\n",
      "Epoch 92: Average Training Loss: -0.000215\n",
      "❌ No improvement. Patience: 13/50\n",
      "Epoch: [92][0/2]\tTime  0.083 ( 0.083)\tData  0.000 ( 0.000)\tLoss -0.0034 (-0.0034)\n",
      "Epoch 93: Average Training Loss: -0.005801\n",
      "❌ No improvement. Patience: 14/50\n",
      "Epoch: [93][0/2]\tTime  0.078 ( 0.078)\tData  0.004 ( 0.004)\tLoss 0.0012 (0.0012)\n",
      "Epoch 94: Average Training Loss: 0.001748\n",
      "❌ No improvement. Patience: 15/50\n",
      "Epoch: [94][0/2]\tTime  0.082 ( 0.082)\tData  0.000 ( 0.000)\tLoss -0.0043 (-0.0043)\n",
      "Epoch 95: Average Training Loss: -0.005330\n",
      "❌ No improvement. Patience: 16/50\n",
      "Epoch: [95][0/2]\tTime  0.064 ( 0.064)\tData  0.004 ( 0.004)\tLoss -0.0010 (-0.0010)\n",
      "Epoch 96: Average Training Loss: -0.002679\n",
      "❌ No improvement. Patience: 17/50\n",
      "Epoch: [96][0/2]\tTime  0.089 ( 0.089)\tData  0.003 ( 0.003)\tLoss -0.0049 (-0.0049)\n",
      "Epoch 97: Average Training Loss: -0.004570\n",
      "❌ No improvement. Patience: 18/50\n",
      "Epoch: [97][0/2]\tTime  0.080 ( 0.080)\tData  0.000 ( 0.000)\tLoss -0.0067 (-0.0067)\n",
      "Epoch 98: Average Training Loss: -0.007392\n",
      "❌ No improvement. Patience: 19/50\n",
      "Epoch: [98][0/2]\tTime  0.092 ( 0.092)\tData  0.010 ( 0.010)\tLoss -0.0066 (-0.0066)\n",
      "Epoch 99: Average Training Loss: -0.006961\n",
      "❌ No improvement. Patience: 20/50\n",
      "Epoch: [99][0/2]\tTime  0.075 ( 0.075)\tData  0.003 ( 0.003)\tLoss -0.0002 (-0.0002)\n",
      "Epoch 100: Average Training Loss: -0.001390\n",
      "❌ No improvement. Patience: 21/50\n",
      "Epoch: [100][0/2]\tTime  0.088 ( 0.088)\tData  0.010 ( 0.010)\tLoss -0.0035 (-0.0035)\n",
      "Epoch 101: Average Training Loss: -0.004697\n",
      "❌ No improvement. Patience: 22/50\n",
      "Epoch: [101][0/2]\tTime  0.089 ( 0.089)\tData  0.000 ( 0.000)\tLoss -0.0057 (-0.0057)\n",
      "Epoch 102: Average Training Loss: -0.005248\n",
      "❌ No improvement. Patience: 23/50\n",
      "Epoch: [102][0/2]\tTime  0.075 ( 0.075)\tData  0.004 ( 0.004)\tLoss -0.0067 (-0.0067)\n",
      "Epoch 103: Average Training Loss: -0.008033\n",
      "❌ No improvement. Patience: 24/50\n",
      "Epoch: [103][0/2]\tTime  0.085 ( 0.085)\tData  0.004 ( 0.004)\tLoss -0.0014 (-0.0014)\n",
      "Epoch 104: Average Training Loss: -0.002098\n",
      "❌ No improvement. Patience: 25/50\n",
      "Epoch: [104][0/2]\tTime  0.086 ( 0.086)\tData  0.000 ( 0.000)\tLoss -0.0025 (-0.0025)\n",
      "Epoch 105: Average Training Loss: -0.007099\n",
      "❌ No improvement. Patience: 26/50\n",
      "Epoch: [105][0/2]\tTime  0.076 ( 0.076)\tData  0.004 ( 0.004)\tLoss -0.0046 (-0.0046)\n",
      "Epoch 106: Average Training Loss: -0.002901\n",
      "❌ No improvement. Patience: 27/50\n",
      "Epoch: [106][0/2]\tTime  0.089 ( 0.089)\tData  0.000 ( 0.000)\tLoss 0.0009 (0.0009)\n",
      "Epoch 107: Average Training Loss: -0.003371\n",
      "❌ No improvement. Patience: 28/50\n",
      "Epoch: [107][0/2]\tTime  0.082 ( 0.082)\tData  0.000 ( 0.000)\tLoss -0.0011 (-0.0011)\n",
      "Epoch 108: Average Training Loss: -0.003908\n",
      "❌ No improvement. Patience: 29/50\n",
      "Epoch: [108][0/2]\tTime  0.091 ( 0.091)\tData  0.010 ( 0.010)\tLoss -0.0018 (-0.0018)\n",
      "Epoch 109: Average Training Loss: -0.004639\n",
      "❌ No improvement. Patience: 30/50\n",
      "Epoch: [109][0/2]\tTime  0.089 ( 0.089)\tData  0.000 ( 0.000)\tLoss -0.0085 (-0.0085)\n",
      "Epoch 110: Average Training Loss: -0.004261\n",
      "❌ No improvement. Patience: 31/50\n",
      "Epoch: [110][0/2]\tTime  0.093 ( 0.093)\tData  0.016 ( 0.016)\tLoss -0.0079 (-0.0079)\n",
      "Epoch 111: Average Training Loss: -0.008248\n",
      "❌ No improvement. Patience: 32/50\n",
      "Epoch: [111][0/2]\tTime  0.076 ( 0.076)\tData  0.004 ( 0.004)\tLoss -0.0045 (-0.0045)\n",
      "Epoch 112: Average Training Loss: -0.005804\n",
      "❌ No improvement. Patience: 33/50\n",
      "Epoch: [112][0/2]\tTime  0.091 ( 0.091)\tData  0.000 ( 0.000)\tLoss -0.0020 (-0.0020)\n",
      "Epoch 113: Average Training Loss: -0.004990\n",
      "❌ No improvement. Patience: 34/50\n",
      "Epoch: [113][0/2]\tTime  0.094 ( 0.094)\tData  0.015 ( 0.015)\tLoss -0.0067 (-0.0067)\n",
      "Epoch 114: Average Training Loss: -0.006504\n",
      "❌ No improvement. Patience: 35/50\n",
      "Epoch: [114][0/2]\tTime  0.090 ( 0.090)\tData  0.016 ( 0.016)\tLoss -0.0071 (-0.0071)\n",
      "Epoch 115: Average Training Loss: -0.009003\n",
      "❌ No improvement. Patience: 36/50\n",
      "Epoch: [115][0/2]\tTime  0.087 ( 0.087)\tData  0.001 ( 0.001)\tLoss 0.0011 (0.0011)\n",
      "Epoch 116: Average Training Loss: -0.003188\n",
      "❌ No improvement. Patience: 37/50\n",
      "Epoch: [116][0/2]\tTime  0.093 ( 0.093)\tData  0.000 ( 0.000)\tLoss -0.0016 (-0.0016)\n",
      "Epoch 117: Average Training Loss: -0.003511\n",
      "❌ No improvement. Patience: 38/50\n",
      "Epoch: [117][0/2]\tTime  0.070 ( 0.070)\tData  0.007 ( 0.007)\tLoss -0.0025 (-0.0025)\n",
      "Epoch 118: Average Training Loss: -0.006362\n",
      "❌ No improvement. Patience: 39/50\n",
      "Epoch: [118][0/2]\tTime  0.083 ( 0.083)\tData  0.000 ( 0.000)\tLoss -0.0086 (-0.0086)\n",
      "Epoch 119: Average Training Loss: -0.005328\n",
      "❌ No improvement. Patience: 40/50\n",
      "Epoch: [119][0/2]\tTime  0.094 ( 0.094)\tData  0.000 ( 0.000)\tLoss -0.0056 (-0.0056)\n",
      "Epoch 120: Average Training Loss: -0.005137\n",
      "❌ No improvement. Patience: 41/50\n",
      "Epoch: [120][0/2]\tTime  0.071 ( 0.071)\tData  0.000 ( 0.000)\tLoss -0.0048 (-0.0048)\n",
      "Epoch 121: Average Training Loss: -0.006804\n",
      "❌ No improvement. Patience: 42/50\n",
      "Epoch: [121][0/2]\tTime  0.084 ( 0.084)\tData  0.000 ( 0.000)\tLoss -0.0051 (-0.0051)\n",
      "Epoch 122: Average Training Loss: -0.005135\n",
      "❌ No improvement. Patience: 43/50\n",
      "Epoch: [122][0/2]\tTime  0.094 ( 0.094)\tData  0.000 ( 0.000)\tLoss -0.0016 (-0.0016)\n",
      "Epoch 123: Average Training Loss: -0.004254\n",
      "❌ No improvement. Patience: 44/50\n",
      "Epoch: [123][0/2]\tTime  0.072 ( 0.072)\tData  0.004 ( 0.004)\tLoss -0.0084 (-0.0084)\n",
      "Epoch 124: Average Training Loss: -0.005965\n",
      "❌ No improvement. Patience: 45/50\n",
      "Epoch: [124][0/2]\tTime  0.068 ( 0.068)\tData  0.002 ( 0.002)\tLoss -0.0086 (-0.0086)\n",
      "Epoch 125: Average Training Loss: -0.007513\n",
      "❌ No improvement. Patience: 46/50\n",
      "Epoch: [125][0/2]\tTime  0.082 ( 0.082)\tData  0.000 ( 0.000)\tLoss -0.0075 (-0.0075)\n",
      "Epoch 126: Average Training Loss: -0.008418\n",
      "❌ No improvement. Patience: 47/50\n",
      "Epoch: [126][0/2]\tTime  0.073 ( 0.073)\tData  0.003 ( 0.003)\tLoss -0.0045 (-0.0045)\n",
      "Epoch 127: Average Training Loss: -0.006025\n",
      "❌ No improvement. Patience: 48/50\n",
      "Epoch: [127][0/2]\tTime  0.083 ( 0.083)\tData  0.000 ( 0.000)\tLoss -0.0034 (-0.0034)\n",
      "Epoch 128: Average Training Loss: -0.004886\n",
      "❌ No improvement. Patience: 49/50\n",
      "Epoch: [128][0/2]\tTime  0.096 ( 0.096)\tData  0.016 ( 0.016)\tLoss -0.0073 (-0.0073)\n",
      "Epoch 129: Average Training Loss: -0.004375\n",
      "❌ No improvement. Patience: 50/50\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Early stopping parameters\n",
    "best_loss = float('inf')\n",
    "patience = 50  # Number of epochs to wait for improvement\n",
    "patience_counter = 0\n",
    "\n",
    "start_epoch = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "filename = f\"{timestamp}_model.pth.tar\"\n",
    "filepath = f\"models/pretrain/{filename}\"\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    pretrain_adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # Train and get average loss\n",
    "    avg_loss = pretrain_train(pretrain_train_loader, model, criterion, optimizer, epoch, device)\n",
    "    print(f\"Epoch {epoch + 1}: Average Training Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'vgg16',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_loss': best_loss\n",
    "        }, filepath)\n",
    "\n",
    "        print(f\"✅ New best model saved with loss {best_loss:.6f}\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"❌ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d5f40d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\pretrain\\20250610_195820_model.pth.tar\n"
     ]
    }
   ],
   "source": [
    "pretrained = rf'C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\pretrain\\{filename}'\n",
    "print(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ba8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "class VGG16_HSI(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16_HSI, self).__init__()\n",
    "\n",
    "         # Custom Convolutional Layer: Process 9x9x224 input\n",
    "        self.pre_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=224, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Reduce to (256, 1, 1)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer to reshape to (64, 56, 56)\n",
    "        self.fc = nn.Linear(256 * 1 * 1, 64 * 56 * 56)\n",
    "\n",
    "        # Load VGG-16 Model\n",
    "        self.encoder = vgg16(pretrained=False)\n",
    "\n",
    "        # Remove first VGG-16 conv layer\n",
    "        self.encoder.features = nn.Sequential(*list(self.encoder.features.children())[1:])\n",
    "\n",
    "        # Modify classifier to output 2 classes\n",
    "        self.encoder.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'before {x.shape}')\n",
    "        x = self.pre_conv(x)  # Process hyperspectral input\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # print(f'after preconv {x.shape}')\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        # print(f'after fc {x.shape}')\n",
    "        # Reshape to (batch_size, 64, 56, 56) before passing to VGG\n",
    "        x = x.view(x.size(0), 64, 56, 56)\n",
    "        # print(f'after reshape, before vgg second layer {x.shape}')\n",
    "\n",
    "        x = self.encoder.features(x)  # Pass to VGG-16\n",
    "        x = self.encoder.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.encoder.classifier(x)  # Final classification layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65498039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "=> creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\FathanAbi\\fathanvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_conv.0.weight: requires_grad=False\n",
      "pre_conv.0.bias: requires_grad=False\n",
      "pre_conv.2.weight: requires_grad=False\n",
      "pre_conv.2.bias: requires_grad=False\n",
      "pre_conv.3.weight: requires_grad=False\n",
      "pre_conv.3.bias: requires_grad=False\n",
      "pre_conv.5.weight: requires_grad=False\n",
      "pre_conv.5.bias: requires_grad=False\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "encoder.features.1.weight: requires_grad=False\n",
      "encoder.features.1.bias: requires_grad=False\n",
      "encoder.features.4.weight: requires_grad=False\n",
      "encoder.features.4.bias: requires_grad=False\n",
      "encoder.features.6.weight: requires_grad=False\n",
      "encoder.features.6.bias: requires_grad=False\n",
      "encoder.features.9.weight: requires_grad=False\n",
      "encoder.features.9.bias: requires_grad=False\n",
      "encoder.features.11.weight: requires_grad=False\n",
      "encoder.features.11.bias: requires_grad=False\n",
      "encoder.features.13.weight: requires_grad=False\n",
      "encoder.features.13.bias: requires_grad=False\n",
      "encoder.features.16.weight: requires_grad=False\n",
      "encoder.features.16.bias: requires_grad=False\n",
      "encoder.features.18.weight: requires_grad=False\n",
      "encoder.features.18.bias: requires_grad=False\n",
      "encoder.features.20.weight: requires_grad=False\n",
      "encoder.features.20.bias: requires_grad=False\n",
      "encoder.features.23.weight: requires_grad=False\n",
      "encoder.features.23.bias: requires_grad=False\n",
      "encoder.features.25.weight: requires_grad=False\n",
      "encoder.features.25.bias: requires_grad=False\n",
      "encoder.features.27.weight: requires_grad=False\n",
      "encoder.features.27.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=True\n",
      "encoder.classifier.6.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "\n",
    "print(\"Use GPU: {} for training\".format(gpu))\n",
    "\n",
    "print(\"=> creating model\")\n",
    "\n",
    "model_finetune = VGG16_HSI()\n",
    "\n",
    "# Freeze all layers except the last fully connected layer\n",
    "for param in model_finetune.pre_conv.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "for param in model_finetune.fc.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "for param in model_finetune.encoder.features.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "\n",
    "for param in model_finetune.encoder.classifier[:-1].parameters():\n",
    "    param.requires_grad = False  # Freeze all but the last FC layer\n",
    "\n",
    "# Initialize the last FC layer\n",
    "# Initialize the last FC layer\n",
    "torch.nn.init.normal_(model_finetune.encoder.classifier[6].weight, mean=0.0, std=0.01)\n",
    "torch.nn.init.zeros_(model_finetune.encoder.classifier[6].bias)\n",
    "\n",
    "# Check which layers are trainable\n",
    "for name, param in model_finetune.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fa84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> no checkpoint found at 'C:\\Users\\Asus TUF\\Documents\\code\\TA\\simsiam\\simsiam\\models\\pretrain\\20250610_195820_model.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "if pretrained:\n",
    "    if os.path.isfile(pretrained):\n",
    "        print(\"=> loading checkpoint '{}'\".format(pretrained))\n",
    "        checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "\n",
    "        # rename moco pre-trained keys\n",
    "        state_dict = checkpoint['state_dict']\n",
    "\n",
    "        print(\"Model state_dict keys:\", model.state_dict().keys())  # Debugging\n",
    "        print(\"Checkpoint state_dict keys:\", state_dict.keys())  \n",
    "        # for k in list(state_dict.keys()):\n",
    "        #     print(f\"Processing key: {k}\")  # Debugging\n",
    "        #     if k.startswith('module.encoder') and not k.startswith('module.encoder.fc'):\n",
    "        #         state_dict[k[len(\"module.encoder.\"):]] = state_dict[k]\n",
    "        #     del state_dict[k]\n",
    "\n",
    "        # Remove the final classification layer from state_dict\n",
    "        state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder.classifier.6\")}\n",
    "\n",
    "        # Load the modified state_dict (ignoring the missing classification layer)\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        # Check missing keys\n",
    "        print(\"Missing keys:\", msg.missing_keys)\n",
    "\n",
    "        start_epoch = 0\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "  \n",
    "        assert set(msg.missing_keys) == {\"encoder.classifier.6.weight\", \"encoder.classifier.6.bias\"}\n",
    "\n",
    "        print(\"=> loaded pre-trained model '{}'\".format(pretrained))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(pretrained))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28417fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5f67fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[-1.3970, -0.3227]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model_finetune.eval()\n",
    "output = model_finetune(input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7321178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 0\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "torch.cuda.set_device(gpu)\n",
    "model_finetune = model_finetune.cuda(gpu)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "\n",
    "# optimize only the linear classifier\n",
    "parameters = list(filter(lambda p: p.requires_grad, model_finetune.parameters()))\n",
    "assert len(parameters) == 2  # fc.weight, fc.bias\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, init_lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "finetune_augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "finetune_transform = transforms.Compose(finetune_augmentation)\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d915ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_X_train shape: torch.Size([40, 224, 9, 9])\n",
      "Train shape: torch.Size([20, 224, 9, 9]), Validation shape: torch.Size([20, 224, 9, 9])\n",
      "generate data loader using seed\n",
      "torch.Size([20])\n",
      "Train loader size: 1, Validation loader size: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Example usage\n",
    "class CustomDatasetFinetune(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "        \n",
    "            return img1, label  # Return both augmented versions\n",
    "        \n",
    "        return img, label  # If no transform is provided, return the original image twice\n",
    "    \n",
    "finetune_preloaded_images = data_augment  \n",
    "finetune_X = torch.tensor(finetune_preloaded_images)\n",
    "finetune_X= finetune_X.to(torch.float32)\n",
    "finetune_X = finetune_X.permute(0, 3, 1, 2)\n",
    "print(f\"finetune_X_train shape: {finetune_X.shape}\")\n",
    "\n",
    "finetune_y = torch.tensor(label_augment)\n",
    "#\n",
    "# Define transformations if needed\n",
    "finetune_transform = transforms.Compose(finetune_augmentation)\n",
    "testSize = test_size\n",
    "finetune_X_train, finetune_X_val, finetune_y_train, finetune_y_val = train_test_split(finetune_X, finetune_y, test_size = testSize, random_state=seed, stratify=finetune_y)\n",
    "print(f\"Train shape: {finetune_X_train.shape}, Validation shape: {finetune_X_val.shape}\")\n",
    "\n",
    "finetune_train_dataset = CustomDatasetFinetune(finetune_X_train, finetune_y_train, transform=finetune_transform)\n",
    "finetune_val_dataset = CustomDatasetFinetune(finetune_X_val, finetune_y_val, transform=finetune_transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "if seeded_run:\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    finetune_train_loader = DataLoader(\n",
    "        finetune_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    finetune_val_loader = DataLoader(\n",
    "        finetune_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    \n",
    "    print(\"generate data loader using seed\")\n",
    "else:\n",
    "    finetune_train_loader = DataLoader(finetune_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=False)\n",
    "    finetune_val_loader = DataLoader(finetune_val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1 = next(iter(finetune_train_loader))\n",
    "\n",
    "print(batch1[1].size())\n",
    "print(f\"Train loader size: {len(finetune_train_loader)}, Validation loader size: {len(finetune_val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b237e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = FinetuneAverageMeter('Time', ':6.3f')\n",
    "    data_time = FinetuneAverageMeter('Data', ':6.3f')\n",
    "    losses = FinetuneAverageMeter('Loss', ':.4e')\n",
    "    top1 = FinetuneAverageMeter('Acc@1', ':6.2f')\n",
    "    progress = FinetuneProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    \"\"\"\n",
    "    Switch to eval mode:\n",
    "    Under the protocol of linear classification on frozen features/models,\n",
    "    it is not legitimate to change any part of the pre-trained model.\n",
    "    BatchNorm in train mode may revise running mean/std (even if it receives\n",
    "    no gradient), which are part of the model parameters too.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        gpu = 0\n",
    "        images = images.cuda(gpu, non_blocking=True)\n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, = finetune_accuracy(output, target, topk=(1,))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_freq = 10\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "\n",
    "def finetune_validate(val_loader, model, criterion):\n",
    "    batch_time = FinetuneAverageMeter('Time', ':6.3f')\n",
    "    losses = FinetuneAverageMeter('Loss', ':.4e')\n",
    "    top1 = FinetuneAverageMeter('Acc@1', ':6.2f')\n",
    "  \n",
    "    progress = FinetuneProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "      \n",
    "            gpu = 0\n",
    "            images = images.cuda(gpu, non_blocking=True)\n",
    "            target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, = finetune_accuracy(output, target, topk=(1,))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            # top5.update(acc5[0], images.size(0))\n",
    "            print(f\"in validation finction {acc1}\")\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 10\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def finetune_save_checkpoint(timestamp, epoch, state, is_best, filename='models/checkpoint.pth.tar'):\n",
    "    filename='models/finetune/{}_model.pth.tar'.format(timestamp)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def finetune_sanity_check(state_dict, pretrained_weights):\n",
    "    \"\"\"\n",
    "    Linear classifier should not change any weights other than the linear layer.\n",
    "    This sanity check asserts nothing wrong happens (e.g., BN stats updated).\n",
    "    \"\"\"\n",
    "    print(\"=> loading '{}' for sanity check\".format(pretrained_weights))\n",
    "    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "    state_dict_pre = checkpoint['state_dict']\n",
    "\n",
    "    for k in list(state_dict.keys()):\n",
    "        # Ignore fc layer\n",
    "        if 'fc.weight' in k or 'fc.bias' in k:\n",
    "            continue\n",
    "\n",
    "        # Adjust key mapping to match checkpoint format\n",
    "        k_pre = k.replace('module.encoder.', '')  # Remove unnecessary prefix\n",
    "\n",
    "        # Skip missing keys\n",
    "        if k_pre not in state_dict_pre:\n",
    "            print(f\"Warning: {k_pre} not found in pretrained model. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Check if tensor shapes match before comparing values\n",
    "        if state_dict[k].shape != state_dict_pre[k_pre].shape:\n",
    "            print(f\"Warning: Shape mismatch for {k}: {state_dict[k].shape} vs {state_dict_pre[k_pre].shape}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Assert that the weights remain unchanged\n",
    "        assert ((state_dict[k].cpu() == state_dict_pre[k_pre]).all()), \\\n",
    "            '{} is changed in linear classifier training.'.format(k)\n",
    "\n",
    "    print(\"=> sanity check passed.\")\n",
    "\n",
    "\n",
    "\n",
    "class FinetuneAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class FinetuneProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def finetune_adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = cur_lr\n",
    "\n",
    "\n",
    "def finetune_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff8982a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_conv.0.weight: requires_grad=False\n",
      "pre_conv.0.bias: requires_grad=False\n",
      "pre_conv.2.weight: requires_grad=False\n",
      "pre_conv.2.bias: requires_grad=False\n",
      "pre_conv.3.weight: requires_grad=False\n",
      "pre_conv.3.bias: requires_grad=False\n",
      "pre_conv.5.weight: requires_grad=False\n",
      "pre_conv.5.bias: requires_grad=False\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "encoder.features.1.weight: requires_grad=False\n",
      "encoder.features.1.bias: requires_grad=False\n",
      "encoder.features.4.weight: requires_grad=False\n",
      "encoder.features.4.bias: requires_grad=False\n",
      "encoder.features.6.weight: requires_grad=False\n",
      "encoder.features.6.bias: requires_grad=False\n",
      "encoder.features.9.weight: requires_grad=False\n",
      "encoder.features.9.bias: requires_grad=False\n",
      "encoder.features.11.weight: requires_grad=False\n",
      "encoder.features.11.bias: requires_grad=False\n",
      "encoder.features.13.weight: requires_grad=False\n",
      "encoder.features.13.bias: requires_grad=False\n",
      "encoder.features.16.weight: requires_grad=False\n",
      "encoder.features.16.bias: requires_grad=False\n",
      "encoder.features.18.weight: requires_grad=False\n",
      "encoder.features.18.bias: requires_grad=False\n",
      "encoder.features.20.weight: requires_grad=False\n",
      "encoder.features.20.bias: requires_grad=False\n",
      "encoder.features.23.weight: requires_grad=False\n",
      "encoder.features.23.bias: requires_grad=False\n",
      "encoder.features.25.weight: requires_grad=False\n",
      "encoder.features.25.bias: requires_grad=False\n",
      "encoder.features.27.weight: requires_grad=False\n",
      "encoder.features.27.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=True\n",
      "encoder.classifier.6.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_finetune.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa72c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/1]\tTime  0.058 ( 0.058)\tData  0.032 ( 0.032)\tLoss 1.2122e+00 (1.2122e+00)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.039 ( 0.039)\tLoss 3.8144e+02 (3.8144e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "✅ Epoch 1: New best Acc@1: 50.00. Model saved.\n",
      "Epoch: [1][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 3.6684e+02 (3.6684e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 2.5101e+02 (2.5101e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 2: No improvement. Patience counter: 1/50\n",
      "Epoch: [2][0/1]\tTime  0.017 ( 0.017)\tData  0.000 ( 0.000)\tLoss 2.4039e+02 (2.4039e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 2.9692e+02 (2.9692e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 3: No improvement. Patience counter: 2/50\n",
      "Epoch: [3][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 3.0437e+02 (3.0437e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.5829e+02 (3.5829e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 4: No improvement. Patience counter: 3/50\n",
      "Epoch: [4][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 3.6522e+02 (3.6522e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([100.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 8.1212e-03 (8.1212e-03)\tAcc@1 100.00 (100.00)\n",
      " * Acc@1 100.000\n",
      "✅ Epoch 5: New best Acc@1: 100.00. Model saved.\n",
      "Epoch: [5][0/1]\tTime  0.030 ( 0.030)\tData  0.015 ( 0.015)\tLoss 2.0006e-03 (2.0006e-03)\tAcc@1 100.00 (100.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 3.5260e+02 (3.5260e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 6: No improvement. Patience counter: 1/50\n",
      "Epoch: [6][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 3.4049e+02 (3.4049e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.0162e+02 (2.0162e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 7: No improvement. Patience counter: 2/50\n",
      "Epoch: [7][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 1.9478e+02 (1.9478e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.5645e+02 (3.5645e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 8: No improvement. Patience counter: 3/50\n",
      "Epoch: [8][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 3.5994e+02 (3.5994e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.024 ( 0.024)\tLoss 4.3672e+02 (4.3672e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 9: No improvement. Patience counter: 4/50\n",
      "Epoch: [9][0/1]\tTime  0.008 ( 0.008)\tData  0.008 ( 0.008)\tLoss 4.4953e+02 (4.4953e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 9.9217e+01 (9.9217e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 10: No improvement. Patience counter: 5/50\n",
      "Epoch: [10][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 9.5106e+01 (9.5106e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 6.4676e+02 (6.4676e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 11: No improvement. Patience counter: 6/50\n",
      "Epoch: [11][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 6.1315e+02 (6.1315e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 8.7873e+02 (8.7873e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 12: No improvement. Patience counter: 7/50\n",
      "Epoch: [12][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 8.4506e+02 (8.4506e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 6.2067e+02 (6.2067e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 13: No improvement. Patience counter: 8/50\n",
      "Epoch: [13][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 5.9047e+02 (5.9047e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.2396e+01 (2.2396e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 14: No improvement. Patience counter: 9/50\n",
      "Epoch: [14][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 1.3572e+01 (1.3572e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 1.8311e+02 (1.8311e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 15: No improvement. Patience counter: 10/50\n",
      "Epoch: [15][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 1.7619e+02 (1.7619e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 2.2968e+01 (2.2968e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 16: No improvement. Patience counter: 11/50\n",
      "Epoch: [16][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 1.9765e+01 (1.9765e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 9.5831e+01 (9.5831e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 17: No improvement. Patience counter: 12/50\n",
      "Epoch: [17][0/1]\tTime  0.031 ( 0.031)\tData  0.016 ( 0.016)\tLoss 8.5346e+01 (8.5346e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 8.3822e+01 (8.3822e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 18: No improvement. Patience counter: 13/50\n",
      "Epoch: [18][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 7.7815e+01 (7.7815e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 4.6201e+01 (4.6201e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 19: No improvement. Patience counter: 14/50\n",
      "Epoch: [19][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 3.0714e+01 (3.0714e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 9.7529e+01 (9.7529e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 20: No improvement. Patience counter: 15/50\n",
      "Epoch: [20][0/1]\tTime  0.016 ( 0.016)\tData  0.015 ( 0.015)\tLoss 8.7036e+01 (8.7036e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([55.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 3.8505e+01 (3.8505e+01)\tAcc@1  55.00 ( 55.00)\n",
      " * Acc@1 55.000\n",
      "❌ Epoch 21: No improvement. Patience counter: 16/50\n",
      "Epoch: [21][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 2.0646e+01 (2.0646e+01)\tAcc@1  65.00 ( 65.00)\n",
      "in validation finction tensor([100.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      " * Acc@1 100.000\n",
      "❌ Epoch 22: No improvement. Patience counter: 17/50\n",
      "Epoch: [22][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.001 ( 0.001)\tLoss 6.5677e+01 (6.5677e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 23: No improvement. Patience counter: 18/50\n",
      "Epoch: [23][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 5.5943e+01 (5.5943e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 1.4766e+02 (1.4766e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 24: No improvement. Patience counter: 19/50\n",
      "Epoch: [24][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 1.2967e+02 (1.2967e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.8827e+01 (3.8827e+01)\tAcc@1  60.00 ( 60.00)\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 25: No improvement. Patience counter: 20/50\n",
      "Epoch: [25][0/1]\tTime  0.032 ( 0.032)\tData  0.015 ( 0.015)\tLoss 2.0856e+01 (2.0856e+01)\tAcc@1  80.00 ( 80.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 9.1719e+01 (9.1719e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 26: No improvement. Patience counter: 21/50\n",
      "Epoch: [26][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 8.4226e+01 (8.4226e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 9.9408e+00 (9.9408e+00)\tAcc@1  85.00 ( 85.00)\n",
      " * Acc@1 85.000\n",
      "❌ Epoch 27: No improvement. Patience counter: 22/50\n",
      "Epoch: [27][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 3.8249e+00 (3.8249e+00)\tAcc@1  95.00 ( 95.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.031 ( 0.031)\tLoss 9.6845e+01 (9.6845e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 28: No improvement. Patience counter: 23/50\n",
      "Epoch: [28][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 7.2717e+01 (7.2717e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.7846e+01 (3.7846e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 29: No improvement. Patience counter: 24/50\n",
      "Epoch: [29][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 3.1496e+01 (3.1496e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([85.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.017 ( 0.017)\tLoss 1.2554e+01 (1.2554e+01)\tAcc@1  85.00 ( 85.00)\n",
      " * Acc@1 85.000\n",
      "❌ Epoch 30: No improvement. Patience counter: 25/50\n",
      "Epoch: [30][0/1]\tTime  0.031 ( 0.031)\tData  0.015 ( 0.015)\tLoss 4.3461e+00 (4.3461e+00)\tAcc@1  95.00 ( 95.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 7.1455e+01 (7.1455e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 31: No improvement. Patience counter: 26/50\n",
      "Epoch: [31][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 4.4247e+01 (4.4247e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 7.0101e+01 (7.0101e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 32: No improvement. Patience counter: 27/50\n",
      "Epoch: [32][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 5.9371e+01 (5.9371e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([100.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      " * Acc@1 100.000\n",
      "❌ Epoch 33: No improvement. Patience counter: 28/50\n",
      "Epoch: [33][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 3.6744e+01 (3.6744e+01)\tAcc@1  75.00 ( 75.00)\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 34: No improvement. Patience counter: 29/50\n",
      "Epoch: [34][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 1.7075e+01 (1.7075e+01)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 3.3039e+01 (3.3039e+01)\tAcc@1  75.00 ( 75.00)\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 35: No improvement. Patience counter: 30/50\n",
      "Epoch: [35][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 1.4587e+01 (1.4587e+01)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([100.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      " * Acc@1 100.000\n",
      "❌ Epoch 36: No improvement. Patience counter: 31/50\n",
      "Epoch: [36][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.5886e+01 (3.5886e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 37: No improvement. Patience counter: 32/50\n",
      "Epoch: [37][0/1]\tTime  0.031 ( 0.031)\tData  0.015 ( 0.015)\tLoss 2.7135e+01 (2.7135e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([60.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 6.5592e+01 (6.5592e+01)\tAcc@1  60.00 ( 60.00)\n",
      " * Acc@1 60.000\n",
      "❌ Epoch 38: No improvement. Patience counter: 33/50\n",
      "Epoch: [38][0/1]\tTime  0.021 ( 0.021)\tData  0.007 ( 0.007)\tLoss 3.4272e+01 (3.4272e+01)\tAcc@1  75.00 ( 75.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.009 ( 0.009)\tLoss 1.3294e+02 (1.3294e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 39: No improvement. Patience counter: 34/50\n",
      "Epoch: [39][0/1]\tTime  0.016 ( 0.016)\tData  0.016 ( 0.016)\tLoss 1.0004e+02 (1.0004e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([100.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 0.0000e+00 (0.0000e+00)\tAcc@1 100.00 (100.00)\n",
      " * Acc@1 100.000\n",
      "❌ Epoch 40: No improvement. Patience counter: 35/50\n",
      "Epoch: [40][0/1]\tTime  0.032 ( 0.032)\tData  0.016 ( 0.016)\tLoss 1.5093e+00 (1.5093e+00)\tAcc@1  95.00 ( 95.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.4342e+02 (2.4342e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 41: No improvement. Patience counter: 36/50\n",
      "Epoch: [41][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 2.2607e+02 (2.2607e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 5.3554e+01 (5.3554e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 42: No improvement. Patience counter: 37/50\n",
      "Epoch: [42][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 4.1065e+01 (4.1065e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.7314e+02 (2.7314e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 43: No improvement. Patience counter: 38/50\n",
      "Epoch: [43][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 2.4499e+02 (2.4499e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.8530e+02 (3.8530e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 44: No improvement. Patience counter: 39/50\n",
      "Epoch: [44][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 3.5188e+02 (3.5188e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 1.1278e+02 (1.1278e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 45: No improvement. Patience counter: 40/50\n",
      "Epoch: [45][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 7.2999e+01 (7.2999e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.9687e+02 (2.9687e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 46: No improvement. Patience counter: 41/50\n",
      "Epoch: [46][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 2.7470e+02 (2.7470e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 4.8347e+02 (4.8347e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 47: No improvement. Patience counter: 42/50\n",
      "Epoch: [47][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 4.5874e+02 (4.5874e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.3395e+02 (2.3395e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 48: No improvement. Patience counter: 43/50\n",
      "Epoch: [48][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 2.0260e+02 (2.0260e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 1.2138e+02 (1.2138e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 49: No improvement. Patience counter: 44/50\n",
      "Epoch: [49][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 7.9425e+01 (7.9425e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.017 ( 0.017)\tLoss 2.7416e+02 (2.7416e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 50: No improvement. Patience counter: 45/50\n",
      "Epoch: [50][0/1]\tTime  0.002 ( 0.002)\tData  0.002 ( 0.002)\tLoss 2.3310e+02 (2.3310e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([75.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.027 ( 0.027)\tLoss 6.6372e+01 (6.6372e+01)\tAcc@1  75.00 ( 75.00)\n",
      " * Acc@1 75.000\n",
      "❌ Epoch 51: No improvement. Patience counter: 46/50\n",
      "Epoch: [51][0/1]\tTime  0.015 ( 0.015)\tData  0.015 ( 0.015)\tLoss 3.4674e+01 (3.4674e+01)\tAcc@1  85.00 ( 85.00)\n",
      "in validation finction tensor([95.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.023 ( 0.023)\tLoss 1.6915e-01 (1.6915e-01)\tAcc@1  95.00 ( 95.00)\n",
      " * Acc@1 95.000\n",
      "❌ Epoch 52: No improvement. Patience counter: 47/50\n",
      "Epoch: [52][0/1]\tTime  0.009 ( 0.009)\tData  0.009 ( 0.009)\tLoss 3.6862e-01 (3.6862e-01)\tAcc@1  95.00 ( 95.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.2600e+02 (2.2600e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 53: No improvement. Patience counter: 48/50\n",
      "Epoch: [53][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 2.1634e+02 (2.1634e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss 6.0166e+01 (6.0166e+01)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 54: No improvement. Patience counter: 49/50\n",
      "Epoch: [54][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 4.4283e+01 (4.4283e+01)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.023 ( 0.023)\tLoss 1.6278e+02 (1.6278e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 55: No improvement. Patience counter: 50/50\n",
      "⏹️ Early stopping triggered at epoch 55. Best Acc@1: 100.00\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0.0\n",
    "patience = 50  # Adjust as needed\n",
    "patience_counter = 0\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "start_epoch = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_finetune.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    finetune_adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # Train for one epoch\n",
    "    finetune_train(finetune_train_loader, model_finetune, criterion, optimizer, epoch)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    acc1 = finetune_validate(finetune_val_loader, model_finetune, criterion)\n",
    "\n",
    "    # Check if current accuracy is the best\n",
    "    is_best = acc1 > best_acc1\n",
    "\n",
    "    if is_best:\n",
    "        best_acc1 = acc1\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Save best model only\n",
    "        finetune_save_checkpoint(timestamp, epoch, {\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'vgg16',\n",
    "            'state_dict': model_finetune.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best=True)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}: New best Acc@1: {best_acc1:.2f}. Model saved.\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"❌ Epoch {epoch+1}: No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⏹️ Early stopping triggered at epoch {epoch+1}. Best Acc@1: {best_acc1:.2f}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "828ce5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 40.8428 seconds\n",
      "20250610_195820\n",
      "seet used: 55\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Run time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(timestamp)\n",
    "print(f\"seet used: {seed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fathanvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
