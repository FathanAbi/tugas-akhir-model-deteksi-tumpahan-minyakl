{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import builtins\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vgg16\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "import simsiam.builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 0:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception_v3', 'list_models', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'vgg16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    " # create model\n",
    "arch = 'vgg16' \n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = simsiam.builder.SimSiam(\n",
    "    models.__dict__[arch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "creating 5 Randomly chosen 0 indices:\n",
      "creating 5 Randomly chosen 1 indices:\n",
      "[-183 -224  361  417  422  571  653  650  626  598  561  519  475  440\n",
      "  404  383  360  337  315  287  260  237  214  200  177  163  156  143\n",
      "  139  130  123  121  117  112  102   90   77   73   64   28   53   60\n",
      "   64   38   53   55   51   47   45   31   32   39   50   46   48   41\n",
      "   38   16    4    0   -2  -91  -96  -89   -1   10   25   34   38   43\n",
      "   44   41   43   39   41   30   23   17    4  -20  -98  -98 -115 -115\n",
      "  -37   -2    8    8   13    4   17   26   30   27   31   33   33   25\n",
      "   41   32   27   18    8   14  -38  -70 -164    0    0    0    0    0\n",
      "    0  -40  -57 -111  -86  -44  -74  -51    0    5   15   19   22   37\n",
      "   33   32   31   33   32   34   30   32   36   35   33   39   35   39\n",
      "   31   30   27   29   25   11   18   10    2   -5  -17  -78  -32    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   -9  -21  -14    0    1   -4    0   -3   15   15   20   29   25   27\n",
      "   28   27   24   25   28   30   30   27   34   30   33   30   32   35\n",
      "   36   30   29   36   30   21   29   28   24   25   16   19    6   11\n",
      "   19   18    3    1  -11  -21  -11   -3  -25   -3  -16  -12   -7  -11]\n",
      "[-183. -224.  361.  417.  422.  571.  653.  650.  626.  598.  561.  519.\n",
      "  475.  440.  404.  383.  360.  337.  315.  287.  260.  237.  214.  200.\n",
      "  177.  163.  156.  143.  139.  130.  123.  121.  117.  112.  102.   90.\n",
      "   77.   73.   64.   28.   53.   60.   64.   38.   53.   55.   51.   47.\n",
      "   45.   31.   32.   39.   50.   46.   48.   41.   38.   16.    4.    0.\n",
      "   -2.  -91.  -96.  -89.   -1.   10.   25.   34.   38.   43.   44.   41.\n",
      "   43.   39.   41.   30.   23.   17.    4.  -20.  -98.  -98. -115. -115.\n",
      "  -37.   -2.    8.    8.   13.    4.   17.   26.   30.   27.   31.   33.\n",
      "   33.   25.   41.   32.   27.   18.    8.   14.  -38.  -70. -164.    0.\n",
      "    0.    0.    0.    0.    0.  -40.  -57. -111.  -86.  -44.  -74.  -51.\n",
      "    0.    5.   15.   19.   22.   37.   33.   32.   31.   33.   32.   34.\n",
      "   30.   32.   36.   35.   33.   39.   35.   39.   31.   30.   27.   29.\n",
      "   25.   11.   18.   10.    2.   -5.  -17.  -78.  -32.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   -9.  -21.  -14.    0.    1.   -4.    0.   -3.   15.   15.   20.   29.\n",
      "   25.   27.   28.   27.   24.   25.   28.   30.   30.   27.   34.   30.\n",
      "   33.   30.   32.   35.   36.   30.   29.   36.   30.   21.   29.   28.\n",
      "   24.   25.   16.   19.    6.   11.   19.   18.    3.    1.  -11.  -21.\n",
      "  -11.   -3.  -25.   -3.  -16.  -12.   -7.  -11.]\n",
      "[-171 -269  315  343  364  512  588  615  594  587  538  504  463  430\n",
      "  408  396  378  372  367  348  338  328  330  339  338  328  329  332\n",
      "  336  344  346  348  340  332  323  314  308  300  299  305  272  264\n",
      "  252  275  276  273  270  266  271  281  265  256  242  228  220  208\n",
      "  190  163  124   92   68  -28  -66  -68   16   31   47   64   76   94\n",
      "   98  107  116  121  118  106   79   57   34   -7  -56  -44  -58  -63\n",
      "  -40   -5    3   -2    4    1   10   14   21   21   22   19   20   31\n",
      "   30   35   23   18    1   27  -54  -18  -36    0    0    0    0    0\n",
      "    0    0    0    0    0    0  -17  -16  -15    0    4   10   11   18\n",
      "   19   15   20   11   16   23   18   21   14   15   19   10   18   22\n",
      "   24   10   16   14   10    4   -1    3   -7  -17  -19  -19  -33    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -14  -14  -14   -6   -5  -24  -15  -15    0   -4    0    5    8    3\n",
      "    3    3   10    4    4   10    9    0    1    1    7    4    8    0\n",
      "    8    6    3    9    6    6    3    5    9  -15   11   13    0   -6\n",
      "    0   -5  -23  -20  -24  -24   -5  -10  -15  -13   -3    0   -8  -64]\n",
      "[-171. -269.  315.  343.  364.  512.  588.  615.  594.  587.  538.  504.\n",
      "  463.  430.  408.  396.  378.  372.  367.  348.  338.  328.  330.  339.\n",
      "  338.  328.  329.  332.  336.  344.  346.  348.  340.  332.  323.  314.\n",
      "  308.  300.  299.  305.  272.  264.  252.  275.  276.  273.  270.  266.\n",
      "  271.  281.  265.  256.  242.  228.  220.  208.  190.  163.  124.   92.\n",
      "   68.  -28.  -66.  -68.   16.   31.   47.   64.   76.   94.   98.  107.\n",
      "  116.  121.  118.  106.   79.   57.   34.   -7.  -56.  -44.  -58.  -63.\n",
      "  -40.   -5.    3.   -2.    4.    1.   10.   14.   21.   21.   22.   19.\n",
      "   20.   31.   30.   35.   23.   18.    1.   27.  -54.  -18.  -36.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  -17.  -16.\n",
      "  -15.    0.    4.   10.   11.   18.   19.   15.   20.   11.   16.   23.\n",
      "   18.   21.   14.   15.   19.   10.   18.   22.   24.   10.   16.   14.\n",
      "   10.    4.   -1.    3.   -7.  -17.  -19.  -19.  -33.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -14.  -14.  -14.   -6.   -5.  -24.  -15.  -15.    0.   -4.    0.    5.\n",
      "    8.    3.    3.    3.   10.    4.    4.   10.    9.    0.    1.    1.\n",
      "    7.    4.    8.    0.    8.    6.    3.    9.    6.    6.    3.    5.\n",
      "    9.  -15.   11.   13.    0.   -6.    0.   -5.  -23.  -20.  -24.  -24.\n",
      "   -5.  -10.  -15.  -13.   -3.    0.   -8.  -64.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = 5\n",
    "selected_patch_0, selected_patch_1, random_indices_0, random_indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[random_indices_0[i][0]][random_indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[random_indices_1[i][0]][random_indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "indices = random_indices_0 +  random_indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  100\n",
      "hasil augmentasi 1 shape: (100, 9, 9, 224)\n",
      "label augmentai 1 shape: (100,)\n",
      "hasil augmentasi 2 shape: (400, 9, 9, 224)\n",
      "label augmentasi 2 shape: (400,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Element 0 occurs 50 times.\n",
      "Element 1 occurs 50 times.\n",
      "Element 0 occurs 200 times.\n",
      "Element 1 occurs 200 times.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category = 50\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, 200)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts1):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts2):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "print(label_augment1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi gabungan untuk training: (500, 9, 9, 224)\n",
      "label augmentasi gabungan: (500,)\n",
      "Element 0 occurs 250 times.\n",
      "Element 1 occurs 250 times.\n"
     ]
    }
   ],
   "source": [
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[ 0.0017, -0.0442, -0.0391,  ...,  0.0216, -0.0052, -0.0078]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0016, -0.0442, -0.0391,  ...,  0.0216, -0.0051, -0.0079]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0682, 0.0023]])\n",
      "tensor([[0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0679, 0.0019]])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "gpu = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([500, 224, 9, 9])\n",
      "tes\n",
      "tes2\n",
      "torch.Size([50, 224, 9, 9])\n",
      "50\n",
      "torch.Size([9, 9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "            img2 = self.transform(img)  # Second augmentation\n",
    "        \n",
    "            return img1, img2  # Return both augmented versions\n",
    "        \n",
    "        return img, img  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "preloaded_images = data_augment  # Example tensor with 100 images\n",
    "X_train = torch.tensor(preloaded_images)\n",
    "X_train = X_train.to(torch.float32)\n",
    "X_train = X_train.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example normalization\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# 7. Check Output\n",
    "print(\"tes\")\n",
    "batch1, batch2 = next(iter(train_loader))\n",
    "print(\"tes2\")\n",
    "print(batch1.size())\n",
    "print(len(batch1))  # Should print 2 (Two transformed views per image)\n",
    "print(batch1[0][0].shape)  # Should print torch.Size([9, 9, 224]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, j in enumerate(train_loader):\n",
    "#     print(j.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images1, images2) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input1 = images1\n",
    "        input2 = images2\n",
    "\n",
    "      \n",
    "        input1 = input1.to(device, non_blocking=True)\n",
    "        input2 = input2.to(device, non_blocking=True)\n",
    "           \n",
    "\n",
    "        p1, p2, z1, z2 = model(x1=input1, x2=input2) \n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), input1.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 0/10]\tTime  4.519 ( 4.519)\tData  0.066 ( 0.066)\tLoss -0.0010 (-0.0010)\n",
      "Epoch: [1][ 0/10]\tTime  0.307 ( 0.307)\tData  0.049 ( 0.049)\tLoss -0.0000 (-0.0000)\n",
      "Epoch: [2][ 0/10]\tTime  0.252 ( 0.252)\tData  0.049 ( 0.049)\tLoss -0.0008 (-0.0008)\n",
      "Epoch: [3][ 0/10]\tTime  0.415 ( 0.415)\tData  0.040 ( 0.040)\tLoss -0.0001 (-0.0001)\n",
      "Epoch: [4][ 0/10]\tTime  0.199 ( 0.199)\tData  0.033 ( 0.033)\tLoss -0.0017 (-0.0017)\n",
      "Epoch: [5][ 0/10]\tTime  0.340 ( 0.340)\tData  0.039 ( 0.039)\tLoss -0.0031 (-0.0031)\n",
      "Epoch: [6][ 0/10]\tTime  0.446 ( 0.446)\tData  0.031 ( 0.031)\tLoss -0.0100 (-0.0100)\n",
      "Epoch: [7][ 0/10]\tTime  0.444 ( 0.444)\tData  0.016 ( 0.016)\tLoss -0.0055 (-0.0055)\n",
      "Epoch: [8][ 0/10]\tTime  0.167 ( 0.167)\tData  0.025 ( 0.025)\tLoss -0.0047 (-0.0047)\n",
      "Epoch: [9][ 0/10]\tTime  0.398 ( 0.398)\tData  0.016 ( 0.016)\tLoss -0.0058 (-0.0058)\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # train for one epoch\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': 'vgg16',\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best=False, filename='models/checkpoint_{:04d}.pth.tar'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
