{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab9c9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from HSI_class import HSI\n",
    "import zeroPadding\n",
    "import augmentation as aug\n",
    "import createSample as CS\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "seeded_run = True\n",
    "seed = 2025\n",
    "\n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "epochs = 200\n",
    "\n",
    "batch_size =40\n",
    "test_size = 0.5\n",
    "\n",
    "random_indices = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60782994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed has been set\n",
      "seet used: 2025\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Might slow down training, but ensures determinism\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if seeded_run:\n",
    "    set_seed(seed)\n",
    "    print(\"seed has been set\")\n",
    "    print(f\"seet used: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f79fe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Asus TUF\\\\Documents\\\\code\\\\TA\\\\Hyperspectral oil spill detection datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m datasets = []\n\u001b[32m      5\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i>\u001b[32m1\u001b[39m:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Asus TUF\\\\Documents\\\\code\\\\TA\\\\Hyperspectral oil spill detection datasets'"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "datasets = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i>1:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        datasets.append(hsi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d40bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithDataset(n): \n",
    "    hsi_test = datasets[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    test_gt = hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {test_gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(test_gt == 0)\n",
    "    indices1 = np.argwhere(test_gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    num_samples = 50\n",
    "\n",
    "    random_indices0 = indices0[np.random.choice(len(indices0), num_samples, replace=False)]\n",
    "    random_indices1 = indices1[np.random.choice(len(indices1), num_samples, replace=False)]\n",
    "\n",
    "    test_indices = np.vstack((random_indices0, random_indices1))\n",
    "\n",
    "    print(test_indices.shape)\n",
    "\n",
    "    return test_indices, test_gt, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsi_ = datasets[0]\n",
    "patch_size = 9\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random_indices}\")\n",
    "random_indices = 1\n",
    "if random_indices:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(188), np.int64(124)), (np.int64(523), np.int64(150)), (np.int64(1003), np.int64(474)), (np.int64(616), np.int64(508)), (np.int64(905), np.int64(552))]\n",
    "    indices_1 = [(np.int64(106), np.int64(606)), (np.int64(297), np.int64(468)), (np.int64(926), np.int64(35)), (np.int64(536), np.int64(519)), (np.int64(508), np.int64(442))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "\n",
    "\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73544b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =1\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])\n",
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ccef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e55134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvTo2D(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvTo2D, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=224, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((3, 3))  # control output spatial size (e.g. 3x3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))      # -> (batch, 64, 9, 9)\n",
    "#         x = F.relu(self.conv2(x))      # -> (batch, 128, 9, 9)\n",
    "#         x = self.pool(x)               # -> (batch, 128, 3, 3)\n",
    "#         x = x.permute(0, 2, 3, 1)      # -> (batch, 3, 3, 128)\n",
    "#         x = x.reshape(x.size(0), -1, 128)  # -> (batch, 9, 128) == (batch, n1, n2)\n",
    "#         return x\n",
    "\n",
    "# feature_extractor = ConvTo2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59567562",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_augment.shape)\n",
    "print(label_augment.shape)\n",
    "\n",
    "x_data = data_augment \n",
    "y_labels = label_augment\n",
    "\n",
    "x_data = torch.tensor(x_data)\n",
    "x_data = x_data.to(torch.float32)\n",
    "x_data = x_data.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {x_data.shape}\")\n",
    "features_np = x_data.numpy()\n",
    "\n",
    "\n",
    "    \n",
    "print(features_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_np\n",
    "y = y_labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Define the 1DCNN model ---\n",
    "class TwoDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoDCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=224, out_channels=32, kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=48, out_channels=120, kernel_size=(2, 2))\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855624ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor )\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "if seeded_run:\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    print(\"generate data loader using seed\")\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "input_channels = X_train_tensor.shape[1]\n",
    "print(input_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoDCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        # print(outputs.shape)\n",
    "        # print(batch_y.shape)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/300, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"finetune Parameter {parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor   = torch.tensor(y_val, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_tensor.to(device)).cpu().squeeze()\n",
    "    val_probs = torch.sigmoid(val_outputs)\n",
    "    val_preds = (val_probs >= 0.5).int().numpy()\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Print predictions and probabilities\n",
    "for i, (pred, prob) in enumerate(zip(val_preds, val_probs)):\n",
    "    print(f\"Sample {i}: Predicted = {pred}, Prob(class 1) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_whole(model, batch_input, device):\n",
    "    batch_input = batch_input.to(device)\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "     \n",
    "        test_features_np = batch_input   # convert to NumPy\n",
    "\n",
    "        X_train_tensor = torch.tensor(test_features_np, dtype=torch.float32)\n",
    "        # X_train_tensor = X_train_tensor.to(device)\n",
    "\n",
    "        val_outputs = model(X_train_tensor).cpu().squeeze()\n",
    "        val_probs = torch.sigmoid(val_outputs)\n",
    "        val_preds = (val_probs >= 0.5).int().numpy()\n",
    "\n",
    "    \n",
    "\n",
    "    return val_preds, val_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithWholeDataset(n): \n",
    "    hsi_test = datasets[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    gt= hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(gt == 0)\n",
    "    indices1 = np.argwhere(gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    return matrix, gt, indices0.shape, indices1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2427a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, matrix, gt, half_patch, expected_shape):\n",
    "        self.matrix = matrix\n",
    "        self.gt = gt\n",
    "        self.half_patch = half_patch\n",
    "        self.expected_shape = expected_shape\n",
    "        self.size_x, self.size_y = matrix.shape[0], matrix.shape[1]\n",
    "        self.valid_coords = [\n",
    "            (x, y)\n",
    "            for x in range(half_patch, self.size_x - half_patch)\n",
    "            for y in range(half_patch, self.size_y - half_patch)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.valid_coords[idx]\n",
    "        true_label = self.gt[x - self.half_patch, y - self.half_patch]\n",
    "\n",
    "        selected_rows = self.matrix[x- self.half_patch:x + 2 * self.half_patch + 1 - self.half_patch, :]\n",
    "        testing_patch = selected_rows[:, y - self.half_patch:y + 2 * self.half_patch + 1 - self.half_patch]\n",
    "\n",
    "        # Verify patch size\n",
    "        if testing_patch.shape != self.expected_shape:\n",
    "            raise ValueError(f\"Patch at ({x},{y}) has wrong shape {testing_patch.shape}\")\n",
    "\n",
    "        patch_tensor = torch.tensor(testing_patch, dtype=torch.float32)\n",
    "        patch_tensor = patch_tensor.permute(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "        return patch_tensor, true_label, x, y  # Also return (x, y) for positioning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "scores = []\n",
    "groundtruth = []\n",
    "prediction = []\n",
    "y_probs = []\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "os.makedirs(f\"predictions/{timestamp}\", exist_ok=True)\n",
    "for dataset in range(len(datasets)):\n",
    "\n",
    "    score = []\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    data_sampler = None\n",
    "    batch_size = 64\n",
    "\n",
    "    correct0 = 0\n",
    "    correct1 = 0\n",
    "    matrix = []\n",
    "    gt = []\n",
    "    expected_patch_shape = []\n",
    "    dataset_patches = []\n",
    "    data_loader = []\n",
    "    patch_tensor = []\n",
    "    true_label = [] \n",
    "    x = []\n",
    "    y = []\n",
    "    pred_matrix = []\n",
    "\n",
    "    matrix, gt, indices_0_shape, indices_1_shape = testWithWholeDataset(dataset)\n",
    "    print(indices_0_shape[0])\n",
    "    print(indices_1_shape[0])\n",
    "\n",
    "    expected_patch_shape = (2 * half_patch + 1, 2 * half_patch + 1, matrix.shape[2])\n",
    "    dataset_patches = PatchDataset(matrix, gt, half_patch, expected_patch_shape)\n",
    "\n",
    "    if seeded_run:\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset_patches,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,  # set to True if needed\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "            generator=g\n",
    "        )\n",
    "        print(\"generate data loader using seed\")\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset_patches, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    patch_tensor, true_label, x, y = next(iter(data_loader))\n",
    "\n",
    "    print(patch_tensor.size())\n",
    "    print(true_label.size())\n",
    "    print(f\"data loader size: {len(data_loader)}\")\n",
    "\n",
    "    pred_matrix = np.full(gt.shape, -1, dtype=np.int32)\n",
    "    correct = 0\n",
    "\n",
    "    for input_batch, label_batch, x_batch, y_batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "\n",
    "\n",
    "        preds, confs = predict_batch_whole(model, input_batch, device)\n",
    "\n",
    "        prediction.append(preds)\n",
    "        y_probs.append(confs)\n",
    "        \n",
    "        label_batch = label_batch.numpy()\n",
    "        x_batch = x_batch.numpy()\n",
    "        y_batch = y_batch.numpy()\n",
    "\n",
    "        for pred, label, x, y in zip(preds, label_batch, x_batch, y_batch):\n",
    "            groundtruth.append(label)\n",
    "            pred_matrix[x - half_patch, y - half_patch] = pred\n",
    "            if pred == label:\n",
    "                if label == 0:\n",
    "                    correct0 += 1\n",
    "                elif label == 1:\n",
    "                    correct1 += 1\n",
    "                \n",
    "    correct = correct0+correct1\n",
    "    print(f\"correct0 = {correct0}\")\n",
    "    print(f\"correct1 = {correct1}\")\n",
    "    total = gt.shape[0] * gt.shape[1]\n",
    "    print(f\"Score: {correct}/{total}\")\n",
    "\n",
    "    score = {\n",
    "        'dataset': dataset,\n",
    "        'class0_size': indices_0_shape[0],\n",
    "        'class1_size': indices_1_shape[0],\n",
    "        'correct_0': correct0,\n",
    "        'correct_1': correct1,\n",
    "        'correct_total': correct,\n",
    "        'total': total\n",
    "    }\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    # Save prediction matrix\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    np.save(f\"predictions/{timestamp}/results {dataset} TwoDCNN.npy\", pred_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b3333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correct = 0\n",
    "all_total = 0\n",
    "all_correct0 = 0\n",
    "all_correct1 = 0\n",
    "class0_total = 0\n",
    "class1_total = 0\n",
    "\n",
    "for score in scores:\n",
    "    dataset = score['dataset']\n",
    "    correct0 = score['correct_0']\n",
    "    correct1 = score['correct_1']\n",
    "    class0_size = score['class0_size']\n",
    "    class1_size = score['class1_size']\n",
    "    correct = score['correct_total']\n",
    "    total = score['total']\n",
    "    print(f\"dataset: {dataset}\\t\", f'{correct0}/{class0_size}\\t', f'{correct1}/{class1_size}\\t', f'{correct}/{total}\\t')\n",
    "\n",
    "    all_correct += correct\n",
    "    all_total += total\n",
    "    all_correct0 += correct0\n",
    "    all_correct1 += correct1\n",
    "    class0_total += class0_size\n",
    "    class1_total += class1_size\n",
    "\n",
    "\n",
    "\n",
    "print(f\"total: \\t\\t {all_correct0}/{class0_total/2} \\t {all_correct1}/{class1_total/2} \\t {all_correct}/{all_total}\")\n",
    "\n",
    "print(f\"acc: {all_correct/all_total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff770dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_total_score = {\n",
    "    'dataset': 'Total Dataset',\n",
    "    'correct_0': all_correct0,\n",
    "    'correct_1': all_correct1,\n",
    "    'class0_total': class0_total,\n",
    "    'class1_total': class1_total,\n",
    "    'correct_total': all_correct,\n",
    "    'total': all_total\n",
    "}\n",
    "\n",
    "scores.append(all_total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee05689",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruths = groundtruth\n",
    "groundtruth_in = []\n",
    "\n",
    "for x in groundtruths:\n",
    "    groundtruth_in.append(x)\n",
    "\n",
    "predictions = prediction\n",
    "prediction_in = []\n",
    "\n",
    "for x in predictions:\n",
    "    for y in x:\n",
    "        prediction_in.append(y)\n",
    "\n",
    "\n",
    "y_prob_in = []\n",
    "\n",
    "for x in y_probs:\n",
    "    for y in x:\n",
    "        y_prob_in.append(y)\n",
    "\n",
    "print(len(groundtruth_in))\n",
    "print(len(prediction_in))\n",
    "print(len(y_prob_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e42082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = groundtruth_in\n",
    "y_pred = prediction_in\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for x, y in zip(y_test, y_pred):\n",
    "    total += 1\n",
    "    if x == y:\n",
    "        correct += 1\n",
    "\n",
    "print(f'{correct}/{total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = np.array([label.item() for label in y_test])\n",
    "# Ensure labels are binary (0 and 1)\n",
    "print(\"Unique values in y_test:\", pd.Series(y_test_np).unique())\n",
    "\n",
    "# Check if y_pred is probability (float) or hard prediction (int)\n",
    "print(\"Sample y_pred values:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e48bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(\n",
    "    {'True': y_test_np, 'Model': y_prob_in})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_df['True'], test_df['Model'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'Model (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Two Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_true = np.array([int(label) for label in y_test_np])  # true labels\n",
    "y_pred = prediction_in                         # predicted class labels (e.g., from predict_batch)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # Use 'binary' if binary task\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Overall Accuracy (OA)\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Average Accuracy (AA) â€” mean of per-class accuracies\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "aa = per_class_acc.mean()\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"OA:        {oa:.4f}\")\n",
    "print(f\"AA:        {aa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63022330",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {\n",
    "    'AUC': float(roc_auc),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'F1 Score': float(f1),\n",
    "    'OA': float(oa),\n",
    "    'AA': float(aa),\n",
    "}\n",
    "result_json = {\n",
    "    'prediction' : scores,\n",
    "    'performance' : performance,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711abcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(result_json)\n",
    "\n",
    "with open(f\"performance/TwoDCNN {timestamp}_results.json\", \"w\") as f:\n",
    "    json.dump(result_json, f, indent=2)\n",
    "\n",
    "print(\"JSON saved to results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Run time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(timestamp)\n",
    "print(f\"seet used: {seed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fathanvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
