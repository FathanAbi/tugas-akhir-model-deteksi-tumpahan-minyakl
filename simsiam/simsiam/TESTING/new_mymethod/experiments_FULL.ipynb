{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a5a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "import random\n",
    "import zeroPadding\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "patch_size = 9\n",
    "n_category = 2\n",
    "band_size = 224\n",
    "base_encoder = 'vgg16'\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "batch_size = 20\n",
    "test_size = 0.5\n",
    "\n",
    "random_indices = 1\n",
    "\n",
    "seeded_run = True\n",
    "seed = 10\n",
    "\n",
    "mode = \"test\"\n",
    "project_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\"\n",
    "# project_path = r\"D:\\FathanAbi\\tugas-akhir-model-deteksi-tumpahan-minyakl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25da8a0f-8f90-4f9e-9a04-991692e9ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed has been set\n",
      "seet used: 10\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # PyTorch determinism\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if seeded_run:\n",
    "    set_seed(seed)\n",
    "    print(\"seed has been set\")\n",
    "    print(f\"seet used: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578786fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM02.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM03.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM04.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM05.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM06.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM07.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM08.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM09.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM10.mat\n",
      "random: 1\n",
      "generating random indices\n",
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "creating 5 Randomly chosen 0 indices:\n",
      "creating 5 Randomly chosen 1 indices:\n",
      "indices 0 used: [(np.int64(910), np.int64(192)), (np.int64(51), np.int64(255)), (np.int64(689), np.int64(202)), (np.int64(772), np.int64(547)), (np.int64(920), np.int64(471))]\n",
      "indices 1 used: [(np.int64(22), np.int64(455)), (np.int64(170), np.int64(145)), (np.int64(410), np.int64(233)), (np.int64(1055), np.int64(123)), (np.int64(469), np.int64(582))]\n",
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n",
      "hasil augmentasi 1 shape: (20, 9, 9, 224)\n",
      "label augmentai 1 shape: (20,)\n",
      "hasil augmentasi 2 shape: (20, 9, 9, 224)\n",
      "label augmentasi 2 shape: (20,)\n",
      "label augment:\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "hasil augmentasi gabungan untuk training: (40, 9, 9, 224)\n",
      "label augmentasi gabungan: (40,)\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = rf\"{project_path}\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 9:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1\n",
    "\n",
    "train_hsi = dataset[0]\n",
    "patch_size = patch_size\n",
    "half_patch = patch_size // 2\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "train_indices_0 = []\n",
    "train_indices_1 = []\n",
    "\n",
    "print(f\"random: {random_indices}\")\n",
    "\n",
    "if random_indices:\n",
    "    print(\"generating random indices\")\n",
    "    selected_patches_0, selected_patches_1, train_indices_0, train_indices_1 = CS.createSample(train_hsi, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    train_indices_0 = [(np.int64(188), np.int64(124)), (np.int64(523), np.int64(150)), (np.int64(1003), np.int64(474)), (np.int64(616), np.int64(508)), (np.int64(905), np.int64(552))]\n",
    "    train_indices_1 = [(np.int64(106), np.int64(606)), (np.int64(297), np.int64(468)), (np.int64(926), np.int64(35)), (np.int64(536), np.int64(519)), (np.int64(508), np.int64(442))]\n",
    "\n",
    "    selected_patches_0, selected_patches_1 = CS.getSample(train_hsi, patch_size, sample_per_class, train_indices_0, train_indices_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_indices = train_indices_0 +  train_indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patches_0, selected_patches_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = train_hsi.gt\n",
    "for indice in train_indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n",
    "\n",
    "\n",
    "n_category = n_category\n",
    "band_size = band_size\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(\"label augment:\")\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cab1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a SimSiam model.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, spectral_band, dim=2048, pred_dim=512):\n",
    "        \"\"\"\n",
    "        dim: feature dimension (default: 2048)\n",
    "        pred_dim: hidden dimension of the predictor (default: 512)\n",
    "        \"\"\"\n",
    "        super(SimSiam, self).__init__()\n",
    "    \n",
    "        self.encoder = base_encoder(pretrained=True)\n",
    "\n",
    "        self.encoder.features = nn.Sequential(*list(self.encoder.features.children())[28:])\n",
    "        self.encoder.features[0] = nn.Conv2d(spectral_band, 512, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.encoder.features[1] = nn.ReLU(inplace=True)\n",
    "        self.encoder.features[2] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    \n",
    "        # Modify the classifier to match the desired output dimensions\n",
    "        # self.encoder.classifier[0] = nn.Linear(512, 4096, bias=True)\n",
    "        self.encoder.classifier[6] = nn.Linear(4096, dim)\n",
    "\n",
    "        # # Fix: Get the correct input dimension from VGG16 classifier\n",
    "        prev_dim = self.encoder.classifier[3].out_features\n",
    "\n",
    "        # Fix: Assign modified layers to classifier instead of non-existing 'fc'\n",
    "        self.encoder.classifier[6] = nn.Sequential(\n",
    "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(prev_dim),\n",
    "                                        nn.ReLU(inplace=True), # first layer\n",
    "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(prev_dim),\n",
    "                                        nn.ReLU(inplace=True), # second layer\n",
    "                                        self.encoder.classifier[6],\n",
    "                                        nn.BatchNorm1d(dim, affine=False)) # output layer# output layer\n",
    "                                        \n",
    "\n",
    "        # self.projector[6].bias.requires_grad = False\n",
    "\n",
    "        # build a 3-layer projector\n",
    "        # prev_dim = self.encoder.fc.weight.shape[1]\n",
    "        # self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "        #                                 nn.BatchNorm1d(prev_dim),\n",
    "        #                                 nn.ReLU(inplace=True), # first layer\n",
    "        #                                 nn.Linear(prev_dim, prev_dim, bias=False),\n",
    "        #                                 nn.BatchNorm1d(prev_dim),\n",
    "        #                                 nn.ReLU(inplace=True), # second layer\n",
    "        #                                 self.encoder.fc,\n",
    "        #                                 nn.BatchNorm1d(dim, affine=False)) # output layer\n",
    "        # self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
    "\n",
    "        # build a 2-layer predictor\n",
    "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(pred_dim),\n",
    "                                        nn.ReLU(inplace=True), # hidden layer\n",
    "                                        nn.Linear(pred_dim, dim)) # output layer\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x1: first views of images\n",
    "            x2: second views of images\n",
    "        Output:\n",
    "            p1, p2, z1, z2: predictors and targets of the network\n",
    "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "        z1 = self.encoder.features(x1) # NxC\n",
    "        z2 = self.encoder.features(x2) # NxC\n",
    "      \n",
    "\n",
    "        z1 = self.encoder.avgpool(z1)\n",
    "        z2 = self.encoder.avgpool(z2)\n",
    "\n",
    "\n",
    "        z1 = torch.flatten(z1, 1)\n",
    "        z2 = torch.flatten(z2, 1)\n",
    "   \n",
    "        z1 = self.encoder.classifier(z1)\n",
    "        z2 = self.encoder.classifier(z2)\n",
    "\n",
    "\n",
    "\n",
    "        p1 = self.predictor(z1) # NxC\n",
    "        p2 = self.predictor(z2) # NxC\n",
    "\n",
    "        return p1, p2, z1.detach(), z2.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4613ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'vgg16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(224, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (4): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "        (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "        (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "# create model\n",
    "base_encoder = base_encoder\n",
    "print(\"=> creating model '{}'\".format(base_encoder))\n",
    "model = SimSiam(models.__dict__[base_encoder],224)\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "init_lr = lr * batch_size / 256\n",
    "gpu = 0\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ffc071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "p1 shape torch.Size([1, 2048]), p2 shape torch.Size([1, 2048])\n",
      "z1 shape torch.Size([1, 2048]), z2 shape torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "input = torch.tensor(test).to(torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2).to(torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "input2 = test2\n",
    "\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(f\"p1 shape {p1.shape}, p2 shape {p2.shape}\")\n",
    "print(f\"z1 shape {z1.shape}, z2 shape {z2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0224d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd6786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([40, 224, 9, 9])\n",
      "generate data loader using seed\n",
      "bacth size: torch.Size([20, 224, 9, 9])\n",
      "length batch: 20\n",
      "Train loader size: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "            img2 = self.transform(img)  # Second augmentation\n",
    "        \n",
    "            return img1, img2  # Return both augmented versions\n",
    "        \n",
    "        return img, img  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pretrain_preloaded_image = data_augment \n",
    "\n",
    "pretrain_X_train = torch.tensor(pretrain_preloaded_image)\n",
    "pretrain_X_train = pretrain_X_train.to(torch.float32)\n",
    "pretrain_X_train = pretrain_X_train.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {pretrain_X_train.shape}\")\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example normalization\n",
    "])\n",
    "\n",
    "pretrain_train_dataset = CustomDataset(pretrain_X_train, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "if seeded_run:\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    pretrain_train_loader = DataLoader(\n",
    "        pretrain_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    print(\"generate data loader using seed\")\n",
    "else:\n",
    "    pretrain_train_loader = DataLoader(\n",
    "        pretrain_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1, batch2 = next(iter(pretrain_train_loader))\n",
    "\n",
    "print(f\"bacth size: {batch1.size()}\")\n",
    "print(f\"length batch: {len(batch1)}\")  # Should print 2 (Two transformed views per image)\n",
    "print(f\"Train loader size: {len(pretrain_train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c59999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr\n",
    "\n",
    "class Pretrain_AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "\n",
    "class Pretrain_ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def pretrain_save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abedcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = Pretrain_AverageMeter('Time', ':6.3f')\n",
    "    data_time = Pretrain_AverageMeter('Data', ':6.3f')\n",
    "    losses = Pretrain_AverageMeter('Loss', ':.4f')\n",
    "    progress = Pretrain_ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images1, images2) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input1 = images1.to(device, non_blocking=True)\n",
    "        input2 = images2.to(device, non_blocking=True)\n",
    "\n",
    "        p1, p2, z1, z2 = model(x1=input1, x2=input2) \n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), input1.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "    # Return average training loss for early stopping\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1714672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: [0][0/2]\tTime  1.129 ( 1.129)\tData  0.015 ( 0.015)\tLoss -0.0009 (-0.0009)\n",
      "Epoch 1: Average Training Loss: -0.003146\n",
      "✅ New best model saved with loss -0.003146\n",
      "Epoch: [1][0/2]\tTime  0.081 ( 0.081)\tData  0.024 ( 0.024)\tLoss 0.0014 (0.0014)\n",
      "Epoch 2: Average Training Loss: 0.001921\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [2][0/2]\tTime  0.146 ( 0.146)\tData  0.027 ( 0.027)\tLoss -0.0017 (-0.0017)\n",
      "Epoch 3: Average Training Loss: -0.004656\n",
      "✅ New best model saved with loss -0.004656\n",
      "Epoch: [3][0/2]\tTime  0.062 ( 0.062)\tData  0.012 ( 0.012)\tLoss 0.0024 (0.0024)\n",
      "Epoch 4: Average Training Loss: -0.000480\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [4][0/2]\tTime  0.139 ( 0.139)\tData  0.014 ( 0.014)\tLoss -0.0021 (-0.0021)\n",
      "Epoch 5: Average Training Loss: -0.005445\n",
      "✅ New best model saved with loss -0.005445\n",
      "Epoch: [5][0/2]\tTime  0.045 ( 0.045)\tData  0.008 ( 0.008)\tLoss -0.0033 (-0.0033)\n",
      "Epoch 6: Average Training Loss: -0.002825\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [6][0/2]\tTime  0.140 ( 0.140)\tData  0.009 ( 0.009)\tLoss -0.0033 (-0.0033)\n",
      "Epoch 7: Average Training Loss: -0.001300\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [7][0/2]\tTime  0.140 ( 0.140)\tData  0.012 ( 0.012)\tLoss 0.0023 (0.0023)\n",
      "Epoch 8: Average Training Loss: -0.001813\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [8][0/2]\tTime  0.137 ( 0.137)\tData  0.014 ( 0.014)\tLoss -0.0024 (-0.0024)\n",
      "Epoch 9: Average Training Loss: -0.003848\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [9][0/2]\tTime  0.129 ( 0.129)\tData  0.014 ( 0.014)\tLoss -0.0025 (-0.0025)\n",
      "Epoch 10: Average Training Loss: -0.005257\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [10][0/2]\tTime  0.139 ( 0.139)\tData  0.015 ( 0.015)\tLoss -0.0062 (-0.0062)\n",
      "Epoch 11: Average Training Loss: -0.004779\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [11][0/2]\tTime  0.148 ( 0.148)\tData  0.009 ( 0.009)\tLoss -0.0066 (-0.0066)\n",
      "Epoch 12: Average Training Loss: -0.006723\n",
      "✅ New best model saved with loss -0.006723\n",
      "Epoch: [12][0/2]\tTime  0.056 ( 0.056)\tData  0.012 ( 0.012)\tLoss -0.0041 (-0.0041)\n",
      "Epoch 13: Average Training Loss: -0.005293\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [13][0/2]\tTime  0.136 ( 0.136)\tData  0.013 ( 0.013)\tLoss -0.0078 (-0.0078)\n",
      "Epoch 14: Average Training Loss: -0.009063\n",
      "✅ New best model saved with loss -0.009063\n",
      "Epoch: [14][0/2]\tTime  0.049 ( 0.049)\tData  0.009 ( 0.009)\tLoss -0.0051 (-0.0051)\n",
      "Epoch 15: Average Training Loss: -0.006758\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [15][0/2]\tTime  0.136 ( 0.136)\tData  0.015 ( 0.015)\tLoss -0.0097 (-0.0097)\n",
      "Epoch 16: Average Training Loss: -0.007854\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [16][0/2]\tTime  0.145 ( 0.145)\tData  0.014 ( 0.014)\tLoss -0.0082 (-0.0082)\n",
      "Epoch 17: Average Training Loss: -0.006517\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [17][0/2]\tTime  0.146 ( 0.146)\tData  0.020 ( 0.020)\tLoss -0.0105 (-0.0105)\n",
      "Epoch 18: Average Training Loss: -0.011159\n",
      "✅ New best model saved with loss -0.011159\n",
      "Epoch: [18][0/2]\tTime  0.032 ( 0.032)\tData  0.000 ( 0.000)\tLoss -0.0099 (-0.0099)\n",
      "Epoch 19: Average Training Loss: -0.011743\n",
      "✅ New best model saved with loss -0.011743\n",
      "Epoch: [19][0/2]\tTime  0.047 ( 0.047)\tData  0.009 ( 0.009)\tLoss -0.0111 (-0.0111)\n",
      "Epoch 20: Average Training Loss: -0.012326\n",
      "✅ New best model saved with loss -0.012326\n",
      "Epoch: [20][0/2]\tTime  0.050 ( 0.050)\tData  0.013 ( 0.013)\tLoss -0.0088 (-0.0088)\n",
      "Epoch 21: Average Training Loss: -0.013672\n",
      "✅ New best model saved with loss -0.013672\n",
      "Epoch: [21][0/2]\tTime  0.058 ( 0.058)\tData  0.007 ( 0.007)\tLoss -0.0146 (-0.0146)\n",
      "Epoch 22: Average Training Loss: -0.016448\n",
      "✅ New best model saved with loss -0.016448\n",
      "Epoch: [22][0/2]\tTime  0.048 ( 0.048)\tData  0.008 ( 0.008)\tLoss -0.0144 (-0.0144)\n",
      "Epoch 23: Average Training Loss: -0.013464\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [23][0/2]\tTime  0.147 ( 0.147)\tData  0.018 ( 0.018)\tLoss -0.0159 (-0.0159)\n",
      "Epoch 24: Average Training Loss: -0.015239\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [24][0/2]\tTime  0.139 ( 0.139)\tData  0.012 ( 0.012)\tLoss -0.0138 (-0.0138)\n",
      "Epoch 25: Average Training Loss: -0.015214\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [25][0/2]\tTime  0.141 ( 0.141)\tData  0.011 ( 0.011)\tLoss -0.0105 (-0.0105)\n",
      "Epoch 26: Average Training Loss: -0.017308\n",
      "✅ New best model saved with loss -0.017308\n",
      "Epoch: [26][0/2]\tTime  0.058 ( 0.058)\tData  0.016 ( 0.016)\tLoss -0.0233 (-0.0233)\n",
      "Epoch 27: Average Training Loss: -0.020942\n",
      "✅ New best model saved with loss -0.020942\n",
      "Epoch: [27][0/2]\tTime  0.041 ( 0.041)\tData  0.005 ( 0.005)\tLoss -0.0168 (-0.0168)\n",
      "Epoch 28: Average Training Loss: -0.021229\n",
      "✅ New best model saved with loss -0.021229\n",
      "Epoch: [28][0/2]\tTime  0.038 ( 0.038)\tData  0.008 ( 0.008)\tLoss -0.0200 (-0.0200)\n",
      "Epoch 29: Average Training Loss: -0.022173\n",
      "✅ New best model saved with loss -0.022173\n",
      "Epoch: [29][0/2]\tTime  0.038 ( 0.038)\tData  0.006 ( 0.006)\tLoss -0.0211 (-0.0211)\n",
      "Epoch 30: Average Training Loss: -0.022683\n",
      "✅ New best model saved with loss -0.022683\n",
      "Epoch: [30][0/2]\tTime  0.083 ( 0.083)\tData  0.007 ( 0.007)\tLoss -0.0239 (-0.0239)\n",
      "Epoch 31: Average Training Loss: -0.025572\n",
      "✅ New best model saved with loss -0.025572\n",
      "Epoch: [31][0/2]\tTime  0.049 ( 0.049)\tData  0.014 ( 0.014)\tLoss -0.0288 (-0.0288)\n",
      "Epoch 32: Average Training Loss: -0.029806\n",
      "✅ New best model saved with loss -0.029806\n",
      "Epoch: [32][0/2]\tTime  0.043 ( 0.043)\tData  0.007 ( 0.007)\tLoss -0.0254 (-0.0254)\n",
      "Epoch 33: Average Training Loss: -0.030969\n",
      "✅ New best model saved with loss -0.030969\n",
      "Epoch: [33][0/2]\tTime  0.042 ( 0.042)\tData  0.007 ( 0.007)\tLoss -0.0287 (-0.0287)\n",
      "Epoch 34: Average Training Loss: -0.032368\n",
      "✅ New best model saved with loss -0.032368\n",
      "Epoch: [34][0/2]\tTime  0.051 ( 0.051)\tData  0.011 ( 0.011)\tLoss -0.0369 (-0.0369)\n",
      "Epoch 35: Average Training Loss: -0.038108\n",
      "✅ New best model saved with loss -0.038108\n",
      "Epoch: [35][0/2]\tTime  0.052 ( 0.052)\tData  0.009 ( 0.009)\tLoss -0.0417 (-0.0417)\n",
      "Epoch 36: Average Training Loss: -0.040797\n",
      "✅ New best model saved with loss -0.040797\n",
      "Epoch: [36][0/2]\tTime  0.095 ( 0.095)\tData  0.010 ( 0.010)\tLoss -0.0402 (-0.0402)\n",
      "Epoch 37: Average Training Loss: -0.044406\n",
      "✅ New best model saved with loss -0.044406\n",
      "Epoch: [37][0/2]\tTime  0.047 ( 0.047)\tData  0.008 ( 0.008)\tLoss -0.0516 (-0.0516)\n",
      "Epoch 38: Average Training Loss: -0.053199\n",
      "✅ New best model saved with loss -0.053199\n",
      "Epoch: [38][0/2]\tTime  0.050 ( 0.050)\tData  0.011 ( 0.011)\tLoss -0.0579 (-0.0579)\n",
      "Epoch 39: Average Training Loss: -0.058252\n",
      "✅ New best model saved with loss -0.058252\n",
      "Epoch: [39][0/2]\tTime  0.047 ( 0.047)\tData  0.008 ( 0.008)\tLoss -0.0555 (-0.0555)\n",
      "Epoch 40: Average Training Loss: -0.059297\n",
      "✅ New best model saved with loss -0.059297\n",
      "Epoch: [40][0/2]\tTime  0.042 ( 0.042)\tData  0.007 ( 0.007)\tLoss -0.0652 (-0.0652)\n",
      "Epoch 41: Average Training Loss: -0.064729\n",
      "✅ New best model saved with loss -0.064729\n",
      "Epoch: [41][0/2]\tTime  0.042 ( 0.042)\tData  0.009 ( 0.009)\tLoss -0.0663 (-0.0663)\n",
      "Epoch 42: Average Training Loss: -0.069743\n",
      "✅ New best model saved with loss -0.069743\n",
      "Epoch: [42][0/2]\tTime  0.096 ( 0.096)\tData  0.006 ( 0.006)\tLoss -0.0740 (-0.0740)\n",
      "Epoch 43: Average Training Loss: -0.076521\n",
      "✅ New best model saved with loss -0.076521\n",
      "Epoch: [43][0/2]\tTime  0.041 ( 0.041)\tData  0.005 ( 0.005)\tLoss -0.0833 (-0.0833)\n",
      "Epoch 44: Average Training Loss: -0.084042\n",
      "✅ New best model saved with loss -0.084042\n",
      "Epoch: [44][0/2]\tTime  0.043 ( 0.043)\tData  0.006 ( 0.006)\tLoss -0.0869 (-0.0869)\n",
      "Epoch 45: Average Training Loss: -0.090008\n",
      "✅ New best model saved with loss -0.090008\n",
      "Epoch: [45][0/2]\tTime  0.052 ( 0.052)\tData  0.003 ( 0.003)\tLoss -0.0957 (-0.0957)\n",
      "Epoch 46: Average Training Loss: -0.094900\n",
      "✅ New best model saved with loss -0.094900\n",
      "Epoch: [46][0/2]\tTime  0.037 ( 0.037)\tData  0.008 ( 0.008)\tLoss -0.1040 (-0.1040)\n",
      "Epoch 47: Average Training Loss: -0.105089\n",
      "✅ New best model saved with loss -0.105089\n",
      "Epoch: [47][0/2]\tTime  0.042 ( 0.042)\tData  0.007 ( 0.007)\tLoss -0.1121 (-0.1121)\n",
      "Epoch 48: Average Training Loss: -0.114601\n",
      "✅ New best model saved with loss -0.114601\n",
      "Epoch: [48][0/2]\tTime  0.040 ( 0.040)\tData  0.004 ( 0.004)\tLoss -0.1160 (-0.1160)\n",
      "Epoch 49: Average Training Loss: -0.118131\n",
      "✅ New best model saved with loss -0.118131\n",
      "Epoch: [49][0/2]\tTime  0.044 ( 0.044)\tData  0.007 ( 0.007)\tLoss -0.1214 (-0.1214)\n",
      "Epoch 50: Average Training Loss: -0.127991\n",
      "✅ New best model saved with loss -0.127991\n",
      "Epoch: [50][0/2]\tTime  0.047 ( 0.047)\tData  0.007 ( 0.007)\tLoss -0.1360 (-0.1360)\n",
      "Epoch 51: Average Training Loss: -0.138695\n",
      "✅ New best model saved with loss -0.138695\n",
      "Epoch: [51][0/2]\tTime  0.047 ( 0.047)\tData  0.013 ( 0.013)\tLoss -0.1398 (-0.1398)\n",
      "Epoch 52: Average Training Loss: -0.148980\n",
      "✅ New best model saved with loss -0.148980\n",
      "Epoch: [52][0/2]\tTime  0.043 ( 0.043)\tData  0.005 ( 0.005)\tLoss -0.1472 (-0.1472)\n",
      "Epoch 53: Average Training Loss: -0.149300\n",
      "✅ New best model saved with loss -0.149300\n",
      "Epoch: [53][0/2]\tTime  0.042 ( 0.042)\tData  0.011 ( 0.011)\tLoss -0.1475 (-0.1475)\n",
      "Epoch 54: Average Training Loss: -0.153320\n",
      "✅ New best model saved with loss -0.153320\n",
      "Epoch: [54][0/2]\tTime  0.047 ( 0.047)\tData  0.007 ( 0.007)\tLoss -0.1593 (-0.1593)\n",
      "Epoch 55: Average Training Loss: -0.163283\n",
      "✅ New best model saved with loss -0.163283\n",
      "Epoch: [55][0/2]\tTime  0.037 ( 0.037)\tData  0.008 ( 0.008)\tLoss -0.1726 (-0.1726)\n",
      "Epoch 56: Average Training Loss: -0.174215\n",
      "✅ New best model saved with loss -0.174215\n",
      "Epoch: [56][0/2]\tTime  0.053 ( 0.053)\tData  0.012 ( 0.012)\tLoss -0.1767 (-0.1767)\n",
      "Epoch 57: Average Training Loss: -0.179122\n",
      "✅ New best model saved with loss -0.179122\n",
      "Epoch: [57][0/2]\tTime  0.041 ( 0.041)\tData  0.005 ( 0.005)\tLoss -0.1803 (-0.1803)\n",
      "Epoch 58: Average Training Loss: -0.182105\n",
      "✅ New best model saved with loss -0.182105\n",
      "Epoch: [58][0/2]\tTime  0.043 ( 0.043)\tData  0.008 ( 0.008)\tLoss -0.1854 (-0.1854)\n",
      "Epoch 59: Average Training Loss: -0.190879\n",
      "✅ New best model saved with loss -0.190879\n",
      "Epoch: [59][0/2]\tTime  0.049 ( 0.049)\tData  0.012 ( 0.012)\tLoss -0.1890 (-0.1890)\n",
      "Epoch 60: Average Training Loss: -0.193386\n",
      "✅ New best model saved with loss -0.193386\n",
      "Epoch: [60][0/2]\tTime  0.092 ( 0.092)\tData  0.007 ( 0.007)\tLoss -0.1979 (-0.1979)\n",
      "Epoch 61: Average Training Loss: -0.200696\n",
      "✅ New best model saved with loss -0.200696\n",
      "Epoch: [61][0/2]\tTime  0.053 ( 0.053)\tData  0.014 ( 0.014)\tLoss -0.1949 (-0.1949)\n",
      "Epoch 62: Average Training Loss: -0.200102\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [62][0/2]\tTime  0.139 ( 0.139)\tData  0.013 ( 0.013)\tLoss -0.2044 (-0.2044)\n",
      "Epoch 63: Average Training Loss: -0.210416\n",
      "✅ New best model saved with loss -0.210416\n",
      "Epoch: [63][0/2]\tTime  0.043 ( 0.043)\tData  0.004 ( 0.004)\tLoss -0.2119 (-0.2119)\n",
      "Epoch 64: Average Training Loss: -0.214992\n",
      "✅ New best model saved with loss -0.214992\n",
      "Epoch: [64][0/2]\tTime  0.093 ( 0.093)\tData  0.009 ( 0.009)\tLoss -0.2100 (-0.2100)\n",
      "Epoch 65: Average Training Loss: -0.213809\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [65][0/2]\tTime  0.144 ( 0.144)\tData  0.011 ( 0.011)\tLoss -0.2152 (-0.2152)\n",
      "Epoch 66: Average Training Loss: -0.221078\n",
      "✅ New best model saved with loss -0.221078\n",
      "Epoch: [66][0/2]\tTime  0.095 ( 0.095)\tData  0.011 ( 0.011)\tLoss -0.2102 (-0.2102)\n",
      "Epoch 67: Average Training Loss: -0.214010\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [67][0/2]\tTime  0.138 ( 0.138)\tData  0.009 ( 0.009)\tLoss -0.2240 (-0.2240)\n",
      "Epoch 68: Average Training Loss: -0.226868\n",
      "✅ New best model saved with loss -0.226868\n",
      "Epoch: [68][0/2]\tTime  0.049 ( 0.049)\tData  0.006 ( 0.006)\tLoss -0.2318 (-0.2318)\n",
      "Epoch 69: Average Training Loss: -0.232974\n",
      "✅ New best model saved with loss -0.232974\n",
      "Epoch: [69][0/2]\tTime  0.042 ( 0.042)\tData  0.013 ( 0.013)\tLoss -0.2299 (-0.2299)\n",
      "Epoch 70: Average Training Loss: -0.230685\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [70][0/2]\tTime  0.142 ( 0.142)\tData  0.016 ( 0.016)\tLoss -0.2242 (-0.2242)\n",
      "Epoch 71: Average Training Loss: -0.227446\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [71][0/2]\tTime  0.138 ( 0.138)\tData  0.011 ( 0.011)\tLoss -0.2401 (-0.2401)\n",
      "Epoch 72: Average Training Loss: -0.238790\n",
      "✅ New best model saved with loss -0.238790\n",
      "Epoch: [72][0/2]\tTime  0.042 ( 0.042)\tData  0.014 ( 0.014)\tLoss -0.2402 (-0.2402)\n",
      "Epoch 73: Average Training Loss: -0.238856\n",
      "✅ New best model saved with loss -0.238856\n",
      "Epoch: [73][0/2]\tTime  0.046 ( 0.046)\tData  0.007 ( 0.007)\tLoss -0.2427 (-0.2427)\n",
      "Epoch 74: Average Training Loss: -0.246084\n",
      "✅ New best model saved with loss -0.246084\n",
      "Epoch: [74][0/2]\tTime  0.045 ( 0.045)\tData  0.006 ( 0.006)\tLoss -0.2406 (-0.2406)\n",
      "Epoch 75: Average Training Loss: -0.248274\n",
      "✅ New best model saved with loss -0.248274\n",
      "Epoch: [75][0/2]\tTime  0.048 ( 0.048)\tData  0.007 ( 0.007)\tLoss -0.2480 (-0.2480)\n",
      "Epoch 76: Average Training Loss: -0.249206\n",
      "✅ New best model saved with loss -0.249206\n",
      "Epoch: [76][0/2]\tTime  0.036 ( 0.036)\tData  0.008 ( 0.008)\tLoss -0.2489 (-0.2489)\n",
      "Epoch 77: Average Training Loss: -0.250727\n",
      "✅ New best model saved with loss -0.250727\n",
      "Epoch: [77][0/2]\tTime  0.042 ( 0.042)\tData  0.007 ( 0.007)\tLoss -0.2483 (-0.2483)\n",
      "Epoch 78: Average Training Loss: -0.252860\n",
      "✅ New best model saved with loss -0.252860\n",
      "Epoch: [78][0/2]\tTime  0.051 ( 0.051)\tData  0.009 ( 0.009)\tLoss -0.2576 (-0.2576)\n",
      "Epoch 79: Average Training Loss: -0.260586\n",
      "✅ New best model saved with loss -0.260586\n",
      "Epoch: [79][0/2]\tTime  0.047 ( 0.047)\tData  0.013 ( 0.013)\tLoss -0.2619 (-0.2619)\n",
      "Epoch 80: Average Training Loss: -0.264160\n",
      "✅ New best model saved with loss -0.264160\n",
      "Epoch: [80][0/2]\tTime  0.041 ( 0.041)\tData  0.005 ( 0.005)\tLoss -0.2620 (-0.2620)\n",
      "Epoch 81: Average Training Loss: -0.264753\n",
      "✅ New best model saved with loss -0.264753\n",
      "Epoch: [81][0/2]\tTime  0.049 ( 0.049)\tData  0.013 ( 0.013)\tLoss -0.2644 (-0.2644)\n",
      "Epoch 82: Average Training Loss: -0.267113\n",
      "✅ New best model saved with loss -0.267113\n",
      "Epoch: [82][0/2]\tTime  0.058 ( 0.058)\tData  0.014 ( 0.014)\tLoss -0.2673 (-0.2673)\n",
      "Epoch 83: Average Training Loss: -0.269231\n",
      "✅ New best model saved with loss -0.269231\n",
      "Epoch: [83][0/2]\tTime  0.043 ( 0.043)\tData  0.010 ( 0.010)\tLoss -0.2627 (-0.2627)\n",
      "Epoch 84: Average Training Loss: -0.266302\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [84][0/2]\tTime  0.136 ( 0.136)\tData  0.012 ( 0.012)\tLoss -0.2749 (-0.2749)\n",
      "Epoch 85: Average Training Loss: -0.273605\n",
      "✅ New best model saved with loss -0.273605\n",
      "Epoch: [85][0/2]\tTime  0.045 ( 0.045)\tData  0.008 ( 0.008)\tLoss -0.2690 (-0.2690)\n",
      "Epoch 86: Average Training Loss: -0.272262\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [86][0/2]\tTime  0.155 ( 0.155)\tData  0.018 ( 0.018)\tLoss -0.2771 (-0.2771)\n",
      "Epoch 87: Average Training Loss: -0.277899\n",
      "✅ New best model saved with loss -0.277899\n",
      "Epoch: [87][0/2]\tTime  0.041 ( 0.041)\tData  0.008 ( 0.008)\tLoss -0.2791 (-0.2791)\n",
      "Epoch 88: Average Training Loss: -0.276730\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [88][0/2]\tTime  0.140 ( 0.140)\tData  0.013 ( 0.013)\tLoss -0.2766 (-0.2766)\n",
      "Epoch 89: Average Training Loss: -0.275644\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [89][0/2]\tTime  0.134 ( 0.134)\tData  0.010 ( 0.010)\tLoss -0.2801 (-0.2801)\n",
      "Epoch 90: Average Training Loss: -0.282301\n",
      "✅ New best model saved with loss -0.282301\n",
      "Epoch: [90][0/2]\tTime  0.046 ( 0.046)\tData  0.007 ( 0.007)\tLoss -0.2827 (-0.2827)\n",
      "Epoch 91: Average Training Loss: -0.283850\n",
      "✅ New best model saved with loss -0.283850\n",
      "Epoch: [91][0/2]\tTime  0.042 ( 0.042)\tData  0.007 ( 0.007)\tLoss -0.2746 (-0.2746)\n",
      "Epoch 92: Average Training Loss: -0.279191\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [92][0/2]\tTime  0.140 ( 0.140)\tData  0.011 ( 0.011)\tLoss -0.2853 (-0.2853)\n",
      "Epoch 93: Average Training Loss: -0.287589\n",
      "✅ New best model saved with loss -0.287589\n",
      "Epoch: [93][0/2]\tTime  0.047 ( 0.047)\tData  0.008 ( 0.008)\tLoss -0.2879 (-0.2879)\n",
      "Epoch 94: Average Training Loss: -0.289600\n",
      "✅ New best model saved with loss -0.289600\n",
      "Epoch: [94][0/2]\tTime  0.044 ( 0.044)\tData  0.006 ( 0.006)\tLoss -0.2864 (-0.2864)\n",
      "Epoch 95: Average Training Loss: -0.287731\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [95][0/2]\tTime  0.137 ( 0.137)\tData  0.012 ( 0.012)\tLoss -0.2913 (-0.2913)\n",
      "Epoch 96: Average Training Loss: -0.292872\n",
      "✅ New best model saved with loss -0.292872\n",
      "Epoch: [96][0/2]\tTime  0.034 ( 0.034)\tData  0.003 ( 0.003)\tLoss -0.2875 (-0.2875)\n",
      "Epoch 97: Average Training Loss: -0.291633\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [97][0/2]\tTime  0.138 ( 0.138)\tData  0.012 ( 0.012)\tLoss -0.2898 (-0.2898)\n",
      "Epoch 98: Average Training Loss: -0.296235\n",
      "✅ New best model saved with loss -0.296235\n",
      "Epoch: [98][0/2]\tTime  0.098 ( 0.098)\tData  0.012 ( 0.012)\tLoss -0.2981 (-0.2981)\n",
      "Epoch 99: Average Training Loss: -0.302080\n",
      "✅ New best model saved with loss -0.302080\n",
      "Epoch: [99][0/2]\tTime  0.048 ( 0.048)\tData  0.012 ( 0.012)\tLoss -0.3031 (-0.3031)\n",
      "Epoch 100: Average Training Loss: -0.302431\n",
      "✅ New best model saved with loss -0.302431\n",
      "Epoch: [100][0/2]\tTime  0.098 ( 0.098)\tData  0.010 ( 0.010)\tLoss -0.2983 (-0.2983)\n",
      "Epoch 101: Average Training Loss: -0.300534\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [101][0/2]\tTime  0.137 ( 0.137)\tData  0.011 ( 0.011)\tLoss -0.2982 (-0.2982)\n",
      "Epoch 102: Average Training Loss: -0.305225\n",
      "✅ New best model saved with loss -0.305225\n",
      "Epoch: [102][0/2]\tTime  0.045 ( 0.045)\tData  0.007 ( 0.007)\tLoss -0.2989 (-0.2989)\n",
      "Epoch 103: Average Training Loss: -0.303377\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [103][0/2]\tTime  0.138 ( 0.138)\tData  0.010 ( 0.010)\tLoss -0.3012 (-0.3012)\n",
      "Epoch 104: Average Training Loss: -0.302736\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [104][0/2]\tTime  0.141 ( 0.141)\tData  0.010 ( 0.010)\tLoss -0.3027 (-0.3027)\n",
      "Epoch 105: Average Training Loss: -0.306420\n",
      "✅ New best model saved with loss -0.306420\n",
      "Epoch: [105][0/2]\tTime  0.048 ( 0.048)\tData  0.009 ( 0.009)\tLoss -0.3088 (-0.3088)\n",
      "Epoch 106: Average Training Loss: -0.310938\n",
      "✅ New best model saved with loss -0.310938\n",
      "Epoch: [106][0/2]\tTime  0.044 ( 0.044)\tData  0.009 ( 0.009)\tLoss -0.3154 (-0.3154)\n",
      "Epoch 107: Average Training Loss: -0.312187\n",
      "✅ New best model saved with loss -0.312187\n",
      "Epoch: [107][0/2]\tTime  0.041 ( 0.041)\tData  0.008 ( 0.008)\tLoss -0.3096 (-0.3096)\n",
      "Epoch 108: Average Training Loss: -0.311332\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [108][0/2]\tTime  0.141 ( 0.141)\tData  0.012 ( 0.012)\tLoss -0.3105 (-0.3105)\n",
      "Epoch 109: Average Training Loss: -0.309467\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [109][0/2]\tTime  0.139 ( 0.139)\tData  0.011 ( 0.011)\tLoss -0.3112 (-0.3112)\n",
      "Epoch 110: Average Training Loss: -0.312459\n",
      "✅ New best model saved with loss -0.312459\n",
      "Epoch: [110][0/2]\tTime  0.042 ( 0.042)\tData  0.009 ( 0.009)\tLoss -0.3125 (-0.3125)\n",
      "Epoch 111: Average Training Loss: -0.315817\n",
      "✅ New best model saved with loss -0.315817\n",
      "Epoch: [111][0/2]\tTime  0.046 ( 0.046)\tData  0.010 ( 0.010)\tLoss -0.3089 (-0.3089)\n",
      "Epoch 112: Average Training Loss: -0.312834\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [112][0/2]\tTime  0.156 ( 0.156)\tData  0.015 ( 0.015)\tLoss -0.3172 (-0.3172)\n",
      "Epoch 113: Average Training Loss: -0.317742\n",
      "✅ New best model saved with loss -0.317742\n",
      "Epoch: [113][0/2]\tTime  0.097 ( 0.097)\tData  0.013 ( 0.013)\tLoss -0.3186 (-0.3186)\n",
      "Epoch 114: Average Training Loss: -0.320205\n",
      "✅ New best model saved with loss -0.320205\n",
      "Epoch: [114][0/2]\tTime  0.047 ( 0.047)\tData  0.008 ( 0.008)\tLoss -0.3183 (-0.3183)\n",
      "Epoch 115: Average Training Loss: -0.319795\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [115][0/2]\tTime  0.144 ( 0.144)\tData  0.022 ( 0.022)\tLoss -0.3194 (-0.3194)\n",
      "Epoch 116: Average Training Loss: -0.320392\n",
      "✅ New best model saved with loss -0.320392\n",
      "Epoch: [116][0/2]\tTime  0.044 ( 0.044)\tData  0.008 ( 0.008)\tLoss -0.3229 (-0.3229)\n",
      "Epoch 117: Average Training Loss: -0.323623\n",
      "✅ New best model saved with loss -0.323623\n",
      "Epoch: [117][0/2]\tTime  0.044 ( 0.044)\tData  0.011 ( 0.011)\tLoss -0.3207 (-0.3207)\n",
      "Epoch 118: Average Training Loss: -0.327871\n",
      "✅ New best model saved with loss -0.327871\n",
      "Epoch: [118][0/2]\tTime  0.042 ( 0.042)\tData  0.009 ( 0.009)\tLoss -0.3239 (-0.3239)\n",
      "Epoch 119: Average Training Loss: -0.326019\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [119][0/2]\tTime  0.139 ( 0.139)\tData  0.010 ( 0.010)\tLoss -0.3235 (-0.3235)\n",
      "Epoch 120: Average Training Loss: -0.324444\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [120][0/2]\tTime  0.135 ( 0.135)\tData  0.008 ( 0.008)\tLoss -0.3222 (-0.3222)\n",
      "Epoch 121: Average Training Loss: -0.326346\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [121][0/2]\tTime  0.148 ( 0.148)\tData  0.011 ( 0.011)\tLoss -0.3293 (-0.3293)\n",
      "Epoch 122: Average Training Loss: -0.330975\n",
      "✅ New best model saved with loss -0.330975\n",
      "Epoch: [122][0/2]\tTime  0.050 ( 0.050)\tData  0.014 ( 0.014)\tLoss -0.3326 (-0.3326)\n",
      "Epoch 123: Average Training Loss: -0.334412\n",
      "✅ New best model saved with loss -0.334412\n",
      "Epoch: [123][0/2]\tTime  0.060 ( 0.060)\tData  0.017 ( 0.017)\tLoss -0.3277 (-0.3277)\n",
      "Epoch 124: Average Training Loss: -0.328915\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [124][0/2]\tTime  0.137 ( 0.137)\tData  0.017 ( 0.017)\tLoss -0.3317 (-0.3317)\n",
      "Epoch 125: Average Training Loss: -0.332972\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [125][0/2]\tTime  0.143 ( 0.143)\tData  0.011 ( 0.011)\tLoss -0.3321 (-0.3321)\n",
      "Epoch 126: Average Training Loss: -0.334143\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [126][0/2]\tTime  0.139 ( 0.139)\tData  0.012 ( 0.012)\tLoss -0.3328 (-0.3328)\n",
      "Epoch 127: Average Training Loss: -0.333519\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [127][0/2]\tTime  0.140 ( 0.140)\tData  0.015 ( 0.015)\tLoss -0.3353 (-0.3353)\n",
      "Epoch 128: Average Training Loss: -0.339721\n",
      "✅ New best model saved with loss -0.339721\n",
      "Epoch: [128][0/2]\tTime  0.061 ( 0.061)\tData  0.012 ( 0.012)\tLoss -0.3409 (-0.3409)\n",
      "Epoch 129: Average Training Loss: -0.337766\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [129][0/2]\tTime  0.141 ( 0.141)\tData  0.015 ( 0.015)\tLoss -0.3383 (-0.3383)\n",
      "Epoch 130: Average Training Loss: -0.337763\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [130][0/2]\tTime  0.138 ( 0.138)\tData  0.011 ( 0.011)\tLoss -0.3367 (-0.3367)\n",
      "Epoch 131: Average Training Loss: -0.339909\n",
      "✅ New best model saved with loss -0.339909\n",
      "Epoch: [131][0/2]\tTime  0.048 ( 0.048)\tData  0.010 ( 0.010)\tLoss -0.3367 (-0.3367)\n",
      "Epoch 132: Average Training Loss: -0.335601\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [132][0/2]\tTime  0.148 ( 0.148)\tData  0.015 ( 0.015)\tLoss -0.3411 (-0.3411)\n",
      "Epoch 133: Average Training Loss: -0.342594\n",
      "✅ New best model saved with loss -0.342594\n",
      "Epoch: [133][0/2]\tTime  0.047 ( 0.047)\tData  0.003 ( 0.003)\tLoss -0.3344 (-0.3344)\n",
      "Epoch 134: Average Training Loss: -0.337265\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [134][0/2]\tTime  0.160 ( 0.160)\tData  0.015 ( 0.015)\tLoss -0.3424 (-0.3424)\n",
      "Epoch 135: Average Training Loss: -0.341903\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [135][0/2]\tTime  0.138 ( 0.138)\tData  0.011 ( 0.011)\tLoss -0.3339 (-0.3339)\n",
      "Epoch 136: Average Training Loss: -0.339851\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [136][0/2]\tTime  0.134 ( 0.134)\tData  0.011 ( 0.011)\tLoss -0.3419 (-0.3419)\n",
      "Epoch 137: Average Training Loss: -0.340223\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [137][0/2]\tTime  0.145 ( 0.145)\tData  0.018 ( 0.018)\tLoss -0.3424 (-0.3424)\n",
      "Epoch 138: Average Training Loss: -0.344865\n",
      "✅ New best model saved with loss -0.344865\n",
      "Epoch: [138][0/2]\tTime  0.055 ( 0.055)\tData  0.011 ( 0.011)\tLoss -0.3353 (-0.3353)\n",
      "Epoch 139: Average Training Loss: -0.341423\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [139][0/2]\tTime  0.138 ( 0.138)\tData  0.014 ( 0.014)\tLoss -0.3390 (-0.3390)\n",
      "Epoch 140: Average Training Loss: -0.346312\n",
      "✅ New best model saved with loss -0.346312\n",
      "Epoch: [140][0/2]\tTime  0.097 ( 0.097)\tData  0.007 ( 0.007)\tLoss -0.3387 (-0.3387)\n",
      "Epoch 141: Average Training Loss: -0.343507\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [141][0/2]\tTime  0.137 ( 0.137)\tData  0.013 ( 0.013)\tLoss -0.3426 (-0.3426)\n",
      "Epoch 142: Average Training Loss: -0.347347\n",
      "✅ New best model saved with loss -0.347347\n",
      "Epoch: [142][0/2]\tTime  0.058 ( 0.058)\tData  0.013 ( 0.013)\tLoss -0.3513 (-0.3513)\n",
      "Epoch 143: Average Training Loss: -0.350454\n",
      "✅ New best model saved with loss -0.350454\n",
      "Epoch: [143][0/2]\tTime  0.054 ( 0.054)\tData  0.010 ( 0.010)\tLoss -0.3458 (-0.3458)\n",
      "Epoch 144: Average Training Loss: -0.346967\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [144][0/2]\tTime  0.137 ( 0.137)\tData  0.009 ( 0.009)\tLoss -0.3462 (-0.3462)\n",
      "Epoch 145: Average Training Loss: -0.346542\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [145][0/2]\tTime  0.134 ( 0.134)\tData  0.008 ( 0.008)\tLoss -0.3438 (-0.3438)\n",
      "Epoch 146: Average Training Loss: -0.346687\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [146][0/2]\tTime  0.139 ( 0.139)\tData  0.012 ( 0.012)\tLoss -0.3519 (-0.3519)\n",
      "Epoch 147: Average Training Loss: -0.348222\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [147][0/2]\tTime  0.135 ( 0.135)\tData  0.009 ( 0.009)\tLoss -0.3543 (-0.3543)\n",
      "Epoch 148: Average Training Loss: -0.351032\n",
      "✅ New best model saved with loss -0.351032\n",
      "Epoch: [148][0/2]\tTime  0.045 ( 0.045)\tData  0.006 ( 0.006)\tLoss -0.3433 (-0.3433)\n",
      "Epoch 149: Average Training Loss: -0.349697\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [149][0/2]\tTime  0.141 ( 0.141)\tData  0.011 ( 0.011)\tLoss -0.3454 (-0.3454)\n",
      "Epoch 150: Average Training Loss: -0.349999\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [150][0/2]\tTime  0.131 ( 0.131)\tData  0.025 ( 0.025)\tLoss -0.3493 (-0.3493)\n",
      "Epoch 151: Average Training Loss: -0.352074\n",
      "✅ New best model saved with loss -0.352074\n",
      "Epoch: [151][0/2]\tTime  0.049 ( 0.049)\tData  0.014 ( 0.014)\tLoss -0.3498 (-0.3498)\n",
      "Epoch 152: Average Training Loss: -0.350401\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [152][0/2]\tTime  0.142 ( 0.142)\tData  0.018 ( 0.018)\tLoss -0.3520 (-0.3520)\n",
      "Epoch 153: Average Training Loss: -0.357029\n",
      "✅ New best model saved with loss -0.357029\n",
      "Epoch: [153][0/2]\tTime  0.059 ( 0.059)\tData  0.013 ( 0.013)\tLoss -0.3525 (-0.3525)\n",
      "Epoch 154: Average Training Loss: -0.354947\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [154][0/2]\tTime  0.151 ( 0.151)\tData  0.021 ( 0.021)\tLoss -0.3475 (-0.3475)\n",
      "Epoch 155: Average Training Loss: -0.349070\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [155][0/2]\tTime  0.138 ( 0.138)\tData  0.011 ( 0.011)\tLoss -0.3493 (-0.3493)\n",
      "Epoch 156: Average Training Loss: -0.350793\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [156][0/2]\tTime  0.134 ( 0.134)\tData  0.008 ( 0.008)\tLoss -0.3448 (-0.3448)\n",
      "Epoch 157: Average Training Loss: -0.350358\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [157][0/2]\tTime  0.133 ( 0.133)\tData  0.015 ( 0.015)\tLoss -0.3479 (-0.3479)\n",
      "Epoch 158: Average Training Loss: -0.350678\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [158][0/2]\tTime  0.138 ( 0.138)\tData  0.011 ( 0.011)\tLoss -0.3526 (-0.3526)\n",
      "Epoch 159: Average Training Loss: -0.352989\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [159][0/2]\tTime  0.132 ( 0.132)\tData  0.006 ( 0.006)\tLoss -0.3531 (-0.3531)\n",
      "Epoch 160: Average Training Loss: -0.352942\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [160][0/2]\tTime  0.137 ( 0.137)\tData  0.010 ( 0.010)\tLoss -0.3543 (-0.3543)\n",
      "Epoch 161: Average Training Loss: -0.356648\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [161][0/2]\tTime  0.136 ( 0.136)\tData  0.012 ( 0.012)\tLoss -0.3553 (-0.3553)\n",
      "Epoch 162: Average Training Loss: -0.357510\n",
      "✅ New best model saved with loss -0.357510\n",
      "Epoch: [162][0/2]\tTime  0.097 ( 0.097)\tData  0.007 ( 0.007)\tLoss -0.3531 (-0.3531)\n",
      "Epoch 163: Average Training Loss: -0.357140\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [163][0/2]\tTime  0.139 ( 0.139)\tData  0.007 ( 0.007)\tLoss -0.3548 (-0.3548)\n",
      "Epoch 164: Average Training Loss: -0.354413\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [164][0/2]\tTime  0.136 ( 0.136)\tData  0.007 ( 0.007)\tLoss -0.3579 (-0.3579)\n",
      "Epoch 165: Average Training Loss: -0.356880\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [165][0/2]\tTime  0.137 ( 0.137)\tData  0.011 ( 0.011)\tLoss -0.3554 (-0.3554)\n",
      "Epoch 166: Average Training Loss: -0.358498\n",
      "✅ New best model saved with loss -0.358498\n",
      "Epoch: [166][0/2]\tTime  0.035 ( 0.035)\tData  0.007 ( 0.007)\tLoss -0.3558 (-0.3558)\n",
      "Epoch 167: Average Training Loss: -0.354901\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [167][0/2]\tTime  0.139 ( 0.139)\tData  0.011 ( 0.011)\tLoss -0.3553 (-0.3553)\n",
      "Epoch 168: Average Training Loss: -0.355624\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [168][0/2]\tTime  0.140 ( 0.140)\tData  0.015 ( 0.015)\tLoss -0.3493 (-0.3493)\n",
      "Epoch 169: Average Training Loss: -0.354006\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [169][0/2]\tTime  0.139 ( 0.139)\tData  0.006 ( 0.006)\tLoss -0.3597 (-0.3597)\n",
      "Epoch 170: Average Training Loss: -0.357002\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [170][0/2]\tTime  0.145 ( 0.145)\tData  0.011 ( 0.011)\tLoss -0.3533 (-0.3533)\n",
      "Epoch 171: Average Training Loss: -0.357129\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [171][0/2]\tTime  0.137 ( 0.137)\tData  0.014 ( 0.014)\tLoss -0.3508 (-0.3508)\n",
      "Epoch 172: Average Training Loss: -0.353366\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [172][0/2]\tTime  0.134 ( 0.134)\tData  0.008 ( 0.008)\tLoss -0.3580 (-0.3580)\n",
      "Epoch 173: Average Training Loss: -0.359026\n",
      "✅ New best model saved with loss -0.359026\n",
      "Epoch: [173][0/2]\tTime  0.045 ( 0.045)\tData  0.007 ( 0.007)\tLoss -0.3483 (-0.3483)\n",
      "Epoch 174: Average Training Loss: -0.358040\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [174][0/2]\tTime  0.152 ( 0.152)\tData  0.014 ( 0.014)\tLoss -0.3538 (-0.3538)\n",
      "Epoch 175: Average Training Loss: -0.357769\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [175][0/2]\tTime  0.136 ( 0.136)\tData  0.011 ( 0.011)\tLoss -0.3615 (-0.3615)\n",
      "Epoch 176: Average Training Loss: -0.360552\n",
      "✅ New best model saved with loss -0.360552\n",
      "Epoch: [176][0/2]\tTime  0.048 ( 0.048)\tData  0.007 ( 0.007)\tLoss -0.3556 (-0.3556)\n",
      "Epoch 177: Average Training Loss: -0.354609\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [177][0/2]\tTime  0.146 ( 0.146)\tData  0.014 ( 0.014)\tLoss -0.3524 (-0.3524)\n",
      "Epoch 178: Average Training Loss: -0.353569\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [178][0/2]\tTime  0.135 ( 0.135)\tData  0.011 ( 0.011)\tLoss -0.3564 (-0.3564)\n",
      "Epoch 179: Average Training Loss: -0.361199\n",
      "✅ New best model saved with loss -0.361199\n",
      "Epoch: [179][0/2]\tTime  0.038 ( 0.038)\tData  0.008 ( 0.008)\tLoss -0.3561 (-0.3561)\n",
      "Epoch 180: Average Training Loss: -0.359579\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [180][0/2]\tTime  0.136 ( 0.136)\tData  0.011 ( 0.011)\tLoss -0.3567 (-0.3567)\n",
      "Epoch 181: Average Training Loss: -0.358325\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [181][0/2]\tTime  0.142 ( 0.142)\tData  0.014 ( 0.014)\tLoss -0.3583 (-0.3583)\n",
      "Epoch 182: Average Training Loss: -0.356326\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [182][0/2]\tTime  0.140 ( 0.140)\tData  0.012 ( 0.012)\tLoss -0.3566 (-0.3566)\n",
      "Epoch 183: Average Training Loss: -0.359310\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [183][0/2]\tTime  0.131 ( 0.131)\tData  0.005 ( 0.005)\tLoss -0.3541 (-0.3541)\n",
      "Epoch 184: Average Training Loss: -0.356085\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [184][0/2]\tTime  0.142 ( 0.142)\tData  0.016 ( 0.016)\tLoss -0.3567 (-0.3567)\n",
      "Epoch 185: Average Training Loss: -0.362890\n",
      "✅ New best model saved with loss -0.362890\n",
      "Epoch: [185][0/2]\tTime  0.043 ( 0.043)\tData  0.008 ( 0.008)\tLoss -0.3580 (-0.3580)\n",
      "Epoch 186: Average Training Loss: -0.356858\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [186][0/2]\tTime  0.141 ( 0.141)\tData  0.016 ( 0.016)\tLoss -0.3559 (-0.3559)\n",
      "Epoch 187: Average Training Loss: -0.354805\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [187][0/2]\tTime  0.149 ( 0.149)\tData  0.015 ( 0.015)\tLoss -0.3588 (-0.3588)\n",
      "Epoch 188: Average Training Loss: -0.361132\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [188][0/2]\tTime  0.141 ( 0.141)\tData  0.015 ( 0.015)\tLoss -0.3577 (-0.3577)\n",
      "Epoch 189: Average Training Loss: -0.360640\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [189][0/2]\tTime  0.138 ( 0.138)\tData  0.013 ( 0.013)\tLoss -0.3534 (-0.3534)\n",
      "Epoch 190: Average Training Loss: -0.358837\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [190][0/2]\tTime  0.137 ( 0.137)\tData  0.011 ( 0.011)\tLoss -0.3582 (-0.3582)\n",
      "Epoch 191: Average Training Loss: -0.363680\n",
      "✅ New best model saved with loss -0.363680\n",
      "Epoch: [191][0/2]\tTime  0.041 ( 0.041)\tData  0.007 ( 0.007)\tLoss -0.3508 (-0.3508)\n",
      "Epoch 192: Average Training Loss: -0.354841\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [192][0/2]\tTime  0.142 ( 0.142)\tData  0.018 ( 0.018)\tLoss -0.3576 (-0.3576)\n",
      "Epoch 193: Average Training Loss: -0.357221\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [193][0/2]\tTime  0.138 ( 0.138)\tData  0.012 ( 0.012)\tLoss -0.3588 (-0.3588)\n",
      "Epoch 194: Average Training Loss: -0.358036\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [194][0/2]\tTime  0.131 ( 0.131)\tData  0.011 ( 0.011)\tLoss -0.3621 (-0.3621)\n",
      "Epoch 195: Average Training Loss: -0.360562\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [195][0/2]\tTime  0.137 ( 0.137)\tData  0.014 ( 0.014)\tLoss -0.3531 (-0.3531)\n",
      "Epoch 196: Average Training Loss: -0.357319\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [196][0/2]\tTime  0.139 ( 0.139)\tData  0.012 ( 0.012)\tLoss -0.3524 (-0.3524)\n",
      "Epoch 197: Average Training Loss: -0.356117\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [197][0/2]\tTime  0.134 ( 0.134)\tData  0.009 ( 0.009)\tLoss -0.3640 (-0.3640)\n",
      "Epoch 198: Average Training Loss: -0.364334\n",
      "✅ New best model saved with loss -0.364334\n",
      "Epoch: [198][0/2]\tTime  0.039 ( 0.039)\tData  0.010 ( 0.010)\tLoss -0.3584 (-0.3584)\n",
      "Epoch 199: Average Training Loss: -0.360089\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [199][0/2]\tTime  0.142 ( 0.142)\tData  0.009 ( 0.009)\tLoss -0.3570 (-0.3570)\n",
      "Epoch 200: Average Training Loss: -0.359003\n",
      "❌ No improvement. Patience: 2/50\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "best_loss = float('inf')\n",
    "patience = 50  # Number of epochs to wait for improvement\n",
    "patience_counter = 0\n",
    "\n",
    "start_epoch = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "filename = f\"{timestamp}_model.pth.tar\"\n",
    "filepath = f\"models/pretrain/{filename}\"\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    pretrain_adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # Train and get average loss\n",
    "    avg_loss = pretrain_train(pretrain_train_loader, model, criterion, optimizer, epoch, device)\n",
    "    print(f\"Epoch {epoch + 1}: Average Training Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'vgg16',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_loss': best_loss\n",
    "        }, filepath)\n",
    "\n",
    "        print(f\"✅ New best model saved with loss {best_loss:.6f}\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"❌ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5f40d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain Parameter 164640256\n",
      "models\\pretrain\\20250614_162947_model.pth.tar\n"
     ]
    }
   ],
   "source": [
    "pretrain_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"pretrain Parameter {pretrain_parameters}\")\n",
    "\n",
    "pretrained = rf'models\\pretrain\\{filename}'\n",
    "print(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ba8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "class VGG16_HSI(nn.Module):\n",
    "    def __init__(self, num_classes=2, spectral_band=224):\n",
    "        super(VGG16_HSI, self).__init__()\n",
    "\n",
    "        self.encoder =  vgg16(pretrained=True)\n",
    "\n",
    "        self.encoder.features = nn.Sequential(*list(self.encoder.features.children())[28:])\n",
    "        self.encoder.features[0] = nn.Conv2d(spectral_band, 512, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.encoder.features[1] = nn.ReLU(inplace=True)\n",
    "        self.encoder.features[2] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.encoder.classifier[6] = nn.Linear(in_features=4096, out_features=2048, bias=True)\n",
    "        self.encoder.added_classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=128, bias=True),\n",
    "            nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "        )   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.features(x)  # Pass to VGG-16\n",
    "        x = self.encoder.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.encoder.classifier(x)  # Final classification layer\n",
    "        x = self.encoder.added_classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee42b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "=> creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16_HSI(\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(224, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "    (added_classifier): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=128, bias=True)\n",
      "      (1): Linear(in_features=128, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "\n",
    "print(\"Use GPU: {} for training\".format(gpu))\n",
    "\n",
    "print(\"=> creating model\")\n",
    "\n",
    "model_finetune = VGG16_HSI()\n",
    "print(model_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9fa84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'models\\pretrain\\20250614_162947_model.pth.tar'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus TUF\\AppData\\Local\\Temp\\ipykernel_8944\\1231780717.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict keys: odict_keys(['encoder.features.0.weight', 'encoder.features.0.bias', 'encoder.classifier.0.weight', 'encoder.classifier.0.bias', 'encoder.classifier.3.weight', 'encoder.classifier.3.bias', 'encoder.classifier.6.weight', 'encoder.classifier.6.bias', 'encoder.added_classifier.0.weight', 'encoder.added_classifier.0.bias', 'encoder.added_classifier.1.weight', 'encoder.added_classifier.1.bias'])\n",
      "Checkpoint state_dict keys: odict_keys(['encoder.features.0.weight', 'encoder.features.0.bias', 'encoder.classifier.0.weight', 'encoder.classifier.0.bias', 'encoder.classifier.3.weight', 'encoder.classifier.3.bias', 'encoder.classifier.6.0.weight', 'encoder.classifier.6.1.weight', 'encoder.classifier.6.1.bias', 'encoder.classifier.6.1.running_mean', 'encoder.classifier.6.1.running_var', 'encoder.classifier.6.1.num_batches_tracked', 'encoder.classifier.6.3.weight', 'encoder.classifier.6.4.weight', 'encoder.classifier.6.4.bias', 'encoder.classifier.6.4.running_mean', 'encoder.classifier.6.4.running_var', 'encoder.classifier.6.4.num_batches_tracked', 'encoder.classifier.6.6.weight', 'encoder.classifier.6.6.bias', 'encoder.classifier.6.7.running_mean', 'encoder.classifier.6.7.running_var', 'encoder.classifier.6.7.num_batches_tracked', 'predictor.0.weight', 'predictor.1.weight', 'predictor.1.bias', 'predictor.1.running_mean', 'predictor.1.running_var', 'predictor.1.num_batches_tracked', 'predictor.3.weight', 'predictor.3.bias'])\n",
      "Manually loading each parameter:\n",
      "✓ Loaded: encoder.features.0.weight → encoder.features.0.weight\n",
      "✓ Loaded: encoder.features.0.bias → encoder.features.0.bias\n",
      "✓ Loaded: encoder.classifier.0.weight → encoder.classifier.0.weight\n",
      "✓ Loaded: encoder.classifier.0.bias → encoder.classifier.0.bias\n",
      "✓ Loaded: encoder.classifier.3.weight → encoder.classifier.3.weight\n",
      "✓ Loaded: encoder.classifier.3.bias → encoder.classifier.3.bias\n",
      "❌ Key encoder.classifier.6.0.weight not found in model\n",
      "❌ Key encoder.classifier.6.1.weight not found in model\n",
      "❌ Key encoder.classifier.6.1.bias not found in model\n",
      "❌ Key encoder.classifier.6.1.running_mean not found in model\n",
      "❌ Key encoder.classifier.6.1.running_var not found in model\n",
      "❌ Key encoder.classifier.6.1.num_batches_tracked not found in model\n",
      "❌ Key encoder.classifier.6.3.weight not found in model\n",
      "❌ Key encoder.classifier.6.4.weight not found in model\n",
      "❌ Key encoder.classifier.6.4.bias not found in model\n",
      "❌ Key encoder.classifier.6.4.running_mean not found in model\n",
      "❌ Key encoder.classifier.6.4.running_var not found in model\n",
      "❌ Key encoder.classifier.6.4.num_batches_tracked not found in model\n",
      "✓ Loaded: encoder.classifier.6.6.weight → encoder.classifier.6.weight\n",
      "✓ Loaded: encoder.classifier.6.6.bias → encoder.classifier.6.bias\n",
      "❌ Key encoder.classifier.6.7.running_mean not found in model\n",
      "❌ Key encoder.classifier.6.7.running_var not found in model\n",
      "❌ Key encoder.classifier.6.7.num_batches_tracked not found in model\n",
      "❌ Key predictor.0.weight not found in model\n",
      "❌ Key predictor.1.weight not found in model\n",
      "❌ Key predictor.1.bias not found in model\n",
      "❌ Key predictor.1.running_mean not found in model\n",
      "❌ Key predictor.1.running_var not found in model\n",
      "❌ Key predictor.1.num_batches_tracked not found in model\n",
      "❌ Key predictor.3.weight not found in model\n",
      "❌ Key predictor.3.bias not found in model\n",
      "\n",
      "=== Summary ===\n",
      "Total pretrained keys: 31\n",
      "Loaded keys: 0\n",
      "Missing in model: 0\n",
      "Shape mismatches: 0\n",
      "=> loaded pre-trained model 'models\\pretrain\\20250614_162947_model.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "if pretrained:\n",
    "    if os.path.isfile(pretrained):\n",
    "        print(\"=> loading checkpoint '{}'\".format(pretrained))\n",
    "        checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "\n",
    "        # rename moco pre-trained keys\n",
    "        pretrained_dict = checkpoint['state_dict']\n",
    "        finetune_model_dict = model_finetune.state_dict()\n",
    "\n",
    "        print(\"Model state_dict keys:\", finetune_model_dict.keys())  # Debugging\n",
    "        print(\"Checkpoint state_dict keys:\", pretrained_dict.keys())  \n",
    "    \n",
    "        # Prepare a new state dict to load\n",
    "        new_state_dict = {}\n",
    "\n",
    "        # Track issues\n",
    "        missing_keys = []\n",
    "        shape_mismatches = []\n",
    "\n",
    "        print(\"Manually loading each parameter:\")\n",
    "        # Mapping of renamed keys\n",
    "        key_mapping = {\n",
    "            'encoder.classifier.6.6.weight': 'encoder.classifier.6.weight',\n",
    "            'encoder.classifier.6.6.bias': 'encoder.classifier.6.bias',\n",
    "        }\n",
    "\n",
    "        # Update the keys in the state dict\n",
    "        remapped_dict = {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            new_key = key_mapping[k] if k in key_mapping else k\n",
    "            if new_key in finetune_model_dict and finetune_model_dict[new_key].shape == v.shape:\n",
    "                remapped_dict[new_key] = v\n",
    "                print(f\"✓ Loaded: {k} → {new_key}\")\n",
    "            elif new_key in finetune_model_dict:\n",
    "                print(f\"⚠️ Shape mismatch for {new_key}: model {finetune_model_dict[new_key].shape} vs checkpoint {v.shape}\")\n",
    "            else:\n",
    "                print(f\"❌ Key {new_key} not found in model\")\n",
    "\n",
    "        # Load only matching keys\n",
    "        model_finetune.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n=== Summary ===\")\n",
    "        print(f\"Total pretrained keys: {len(pretrained_dict)}\")\n",
    "        print(f\"Loaded keys: {len(new_state_dict)}\")\n",
    "        print(f\"Missing in model: {len(missing_keys)}\")\n",
    "        print(f\"Shape mismatches: {len(shape_mismatches)}\")\n",
    "\n",
    "  \n",
    "     \n",
    "\n",
    "        print(\"=> loaded pre-trained model '{}'\".format(pretrained))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(pretrained))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28417fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16_HSI(\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(224, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "    (added_classifier): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=128, bias=True)\n",
      "      (1): Linear(in_features=128, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.features.0.weight: requires_grad=False\n",
      "encoder.features.0.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=False\n",
      "encoder.classifier.6.bias: requires_grad=False\n",
      "encoder.added_classifier.0.weight: requires_grad=True\n",
      "encoder.added_classifier.0.bias: requires_grad=True\n",
      "encoder.added_classifier.1.weight: requires_grad=True\n",
      "encoder.added_classifier.1.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for param in model_finetune.encoder.features.parameters():\n",
    "    param.requires_grad = False  # Freeze convolutional layers\n",
    "\n",
    "for param in model_finetune.encoder.classifier.parameters():\n",
    "    param.requires_grad = False  # Freeze all but the last FC layer\n",
    "\n",
    "\n",
    "print(model_finetune)\n",
    "# Check which layers are trainable\n",
    "for name, param in model_finetune.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f67fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "output shape torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test).to(torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "input = test\n",
    "\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2).to(torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "input2 = test2\n",
    "\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model_finetune.eval()\n",
    "output = model_finetune(input)\n",
    "\n",
    "print(f\"output shape {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7321178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_augment shape (40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 0\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "torch.cuda.set_device(gpu)\n",
    "model_finetune = model_finetune.cuda(gpu)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "\n",
    "# optimize only the linear classifier\n",
    "parameters = list(filter(lambda p: p.requires_grad, model_finetune.parameters()))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, init_lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"data_augment shape {data_augment.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d915ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_X_train shape: torch.Size([40, 224, 9, 9])\n",
      "Train shape: torch.Size([20, 224, 9, 9]), Validation shape: torch.Size([20, 224, 9, 9])\n",
      "generate data loader using seed\n",
      "torch.Size([20])\n",
      "Train loader size: 1, Validation loader size: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Example usage\n",
    "class CustomDatasetFinetune(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "        \n",
    "            return img1, label  # Return both augmented versions\n",
    "        \n",
    "        return img, label  # If no transform is provided, return the original image twice\n",
    "    \n",
    "finetune_preloaded_images = data_augment  \n",
    "finetune_X = torch.tensor(finetune_preloaded_images)\n",
    "finetune_X= finetune_X.to(torch.float32)\n",
    "finetune_X = finetune_X.permute(0, 3, 1, 2)\n",
    "print(f\"finetune_X_train shape: {finetune_X.shape}\")\n",
    "\n",
    "finetune_y = torch.tensor(label_augment)\n",
    "#\n",
    "# Define transformations if needed\n",
    "\n",
    "testSize = test_size\n",
    "finetune_X_train, finetune_X_val, finetune_y_train, finetune_y_val = train_test_split(finetune_X, finetune_y, test_size = testSize, random_state=seed, stratify=finetune_y)\n",
    "print(f\"Train shape: {finetune_X_train.shape}, Validation shape: {finetune_X_val.shape}\")\n",
    "\n",
    "finetune_train_dataset = CustomDatasetFinetune(finetune_X_train, finetune_y_train)\n",
    "finetune_val_dataset = CustomDatasetFinetune(finetune_X_val, finetune_y_val)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "if seeded_run:\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    finetune_train_loader = DataLoader(\n",
    "        finetune_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    finetune_val_loader = DataLoader(\n",
    "        finetune_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # set to True if needed\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        generator=g\n",
    "    )\n",
    "    \n",
    "    print(\"generate data loader using seed\")\n",
    "else:\n",
    "    finetune_train_loader = DataLoader(finetune_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=False)\n",
    "    finetune_val_loader = DataLoader(finetune_val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1 = next(iter(finetune_train_loader))\n",
    "\n",
    "print(batch1[1].size())\n",
    "print(f\"Train loader size: {len(finetune_train_loader)}, Validation loader size: {len(finetune_val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = FinetuneAverageMeter('Time', ':6.3f')\n",
    "    data_time = FinetuneAverageMeter('Data', ':6.3f')\n",
    "    losses = FinetuneAverageMeter('Loss', ':.4e')\n",
    "    top1 = FinetuneAverageMeter('Acc@1', ':6.2f')\n",
    "    progress = FinetuneProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    \"\"\"\n",
    "    Switch to eval mode:\n",
    "    Under the protocol of linear classification on frozen features/models,\n",
    "    it is not legitimate to change any part of the pre-trained model.\n",
    "    BatchNorm in train mode may revise running mean/std (even if it receives\n",
    "    no gradient), which are part of the model parameters too.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        gpu = 0\n",
    "        images = images.cuda(gpu, non_blocking=True)\n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, = finetune_accuracy(output, target, topk=(1,))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_freq = 10\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "\n",
    "def finetune_validate(val_loader, model, criterion):\n",
    "    batch_time = FinetuneAverageMeter('Time', ':6.3f')\n",
    "    losses = FinetuneAverageMeter('Loss', ':.4e')\n",
    "    top1 = FinetuneAverageMeter('Acc@1', ':6.2f')\n",
    "  \n",
    "    progress = FinetuneProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "      \n",
    "            gpu = 0\n",
    "            images = images.cuda(gpu, non_blocking=True)\n",
    "            target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, = finetune_accuracy(output, target, topk=(1,))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            # top5.update(acc5[0], images.size(0))\n",
    "            print(f\"in validation finction {acc1}\")\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 10\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def finetune_save_checkpoint(timestamp, epoch, state, is_best, filename='models/checkpoint.pth.tar'):\n",
    "    filename='models/finetune/{}_model.pth.tar'.format(timestamp)\n",
    "    torch.save(state, filename)\n",
    "    # if is_best:\n",
    "    #     shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def finetune_sanity_check(state_dict, pretrained_weights):\n",
    "    \"\"\"\n",
    "    Linear classifier should not change any weights other than the linear layer.\n",
    "    This sanity check asserts nothing wrong happens (e.g., BN stats updated).\n",
    "    \"\"\"\n",
    "    print(\"=> loading '{}' for sanity check\".format(pretrained_weights))\n",
    "    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "    state_dict_pre = checkpoint['state_dict']\n",
    "\n",
    "    for k in list(state_dict.keys()):\n",
    "        # Ignore fc layer\n",
    "        if 'fc.weight' in k or 'fc.bias' in k:\n",
    "            continue\n",
    "\n",
    "        # Adjust key mapping to match checkpoint format\n",
    "        k_pre = k.replace('module.encoder.', '')  # Remove unnecessary prefix\n",
    "\n",
    "        # Skip missing keys\n",
    "        if k_pre not in state_dict_pre:\n",
    "            print(f\"Warning: {k_pre} not found in pretrained model. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Check if tensor shapes match before comparing values\n",
    "        if state_dict[k].shape != state_dict_pre[k_pre].shape:\n",
    "            print(f\"Warning: Shape mismatch for {k}: {state_dict[k].shape} vs {state_dict_pre[k_pre].shape}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Assert that the weights remain unchanged\n",
    "        assert ((state_dict[k].cpu() == state_dict_pre[k_pre]).all()), \\\n",
    "            '{} is changed in linear classifier training.'.format(k)\n",
    "\n",
    "    print(\"=> sanity check passed.\")\n",
    "\n",
    "\n",
    "\n",
    "class FinetuneAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class FinetuneProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def finetune_adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = cur_lr\n",
    "\n",
    "\n",
    "def finetune_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff8982a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.features.0.weight: requires_grad=False\n",
      "encoder.features.0.bias: requires_grad=False\n",
      "encoder.classifier.0.weight: requires_grad=False\n",
      "encoder.classifier.0.bias: requires_grad=False\n",
      "encoder.classifier.3.weight: requires_grad=False\n",
      "encoder.classifier.3.bias: requires_grad=False\n",
      "encoder.classifier.6.weight: requires_grad=False\n",
      "encoder.classifier.6.bias: requires_grad=False\n",
      "encoder.added_classifier.0.weight: requires_grad=True\n",
      "encoder.added_classifier.0.bias: requires_grad=True\n",
      "encoder.added_classifier.1.weight: requires_grad=True\n",
      "encoder.added_classifier.1.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_finetune.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa72c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/1]\tTime  0.275 ( 0.275)\tData  0.004 ( 0.004)\tLoss 4.4768e+00 (4.4768e+00)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.281 ( 0.281)\tLoss 3.9297e+02 (3.9297e+02)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "✅ Epoch 1: New best Acc@1: 50.00. Model saved.\n",
      "Epoch: [1][0/1]\tTime  0.042 ( 0.042)\tData  0.000 ( 0.000)\tLoss 4.2177e+02 (4.2177e+02)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.037 ( 0.037)\tLoss 8.4777e+03 (8.4777e+03)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 2: No improvement. Patience counter: 1/50\n",
      "Epoch: [2][0/1]\tTime  0.038 ( 0.038)\tData  0.007 ( 0.007)\tLoss 7.6513e+03 (7.6513e+03)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.041 ( 0.041)\tLoss 3.3506e+03 (3.3506e+03)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 3: No improvement. Patience counter: 2/50\n",
      "Epoch: [3][0/1]\tTime  0.039 ( 0.039)\tData  0.004 ( 0.004)\tLoss 3.4920e+03 (3.4920e+03)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.014 ( 0.014)\tLoss 9.6135e+05 (9.6135e+05)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 4: No improvement. Patience counter: 3/50\n",
      "Epoch: [4][0/1]\tTime  0.011 ( 0.011)\tData  0.000 ( 0.000)\tLoss 8.9921e+05 (8.9921e+05)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.014 ( 0.014)\tLoss 2.1316e+06 (2.1316e+06)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 5: No improvement. Patience counter: 4/50\n",
      "Epoch: [5][0/1]\tTime  0.010 ( 0.010)\tData  0.002 ( 0.002)\tLoss 1.9584e+06 (1.9584e+06)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 3.4609e+07 (3.4609e+07)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 6: No improvement. Patience counter: 5/50\n",
      "Epoch: [6][0/1]\tTime  0.012 ( 0.012)\tData  0.002 ( 0.002)\tLoss 3.7111e+07 (3.7111e+07)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.012 ( 0.012)\tLoss 1.6234e+08 (1.6234e+08)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 7: No improvement. Patience counter: 6/50\n",
      "Epoch: [7][0/1]\tTime  0.007 ( 0.007)\tData  0.000 ( 0.000)\tLoss 1.6301e+08 (1.6301e+08)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.019 ( 0.019)\tLoss 1.9869e+09 (1.9869e+09)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 8: No improvement. Patience counter: 7/50\n",
      "Epoch: [8][0/1]\tTime  0.012 ( 0.012)\tData  0.003 ( 0.003)\tLoss 2.0569e+09 (2.0569e+09)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 1.1585e+10 (1.1585e+10)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 9: No improvement. Patience counter: 8/50\n",
      "Epoch: [9][0/1]\tTime  0.002 ( 0.002)\tData  0.002 ( 0.002)\tLoss 1.1178e+10 (1.1178e+10)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.022 ( 0.022)\tLoss 1.3025e+11 (1.3025e+11)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 10: No improvement. Patience counter: 9/50\n",
      "Epoch: [10][0/1]\tTime  0.013 ( 0.013)\tData  0.000 ( 0.000)\tLoss 1.3606e+11 (1.3606e+11)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.012 ( 0.012)\tLoss 1.1205e+12 (1.1205e+12)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 11: No improvement. Patience counter: 10/50\n",
      "Epoch: [11][0/1]\tTime  0.013 ( 0.013)\tData  0.000 ( 0.000)\tLoss 1.0924e+12 (1.0924e+12)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.012 ( 0.012)\tLoss 7.9630e+12 (7.9630e+12)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 12: No improvement. Patience counter: 11/50\n",
      "Epoch: [12][0/1]\tTime  0.014 ( 0.014)\tData  0.002 ( 0.002)\tLoss 8.2448e+12 (8.2448e+12)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.011 ( 0.011)\tLoss 9.7875e+13 (9.7875e+13)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 13: No improvement. Patience counter: 12/50\n",
      "Epoch: [13][0/1]\tTime  0.014 ( 0.014)\tData  0.003 ( 0.003)\tLoss 9.5030e+13 (9.5030e+13)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.005 ( 0.005)\tLoss 4.8681e+14 (4.8681e+14)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 14: No improvement. Patience counter: 13/50\n",
      "Epoch: [14][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 5.0166e+14 (5.0166e+14)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 8.6060e+15 (8.6060e+15)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 15: No improvement. Patience counter: 14/50\n",
      "Epoch: [15][0/1]\tTime  0.002 ( 0.002)\tData  0.002 ( 0.002)\tLoss 8.3567e+15 (8.3567e+15)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([30.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.020 ( 0.020)\tLoss 2.9866e+16 (2.9866e+16)\tAcc@1  30.00 ( 30.00)\n",
      " * Acc@1 30.000\n",
      "❌ Epoch 16: No improvement. Patience counter: 15/50\n",
      "Epoch: [16][0/1]\tTime  0.015 ( 0.015)\tData  0.000 ( 0.000)\tLoss 3.0552e+16 (3.0552e+16)\tAcc@1  35.00 ( 35.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.013 ( 0.013)\tLoss 3.2313e+17 (3.2313e+17)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 17: No improvement. Patience counter: 16/50\n",
      "Epoch: [17][0/1]\tTime  0.012 ( 0.012)\tData  0.000 ( 0.000)\tLoss 3.4711e+17 (3.4711e+17)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.006 ( 0.006)\tLoss 1.8549e+18 (1.8549e+18)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 18: No improvement. Patience counter: 17/50\n",
      "Epoch: [18][0/1]\tTime  0.010 ( 0.010)\tData  0.000 ( 0.000)\tLoss 1.4861e+18 (1.4861e+18)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.018 ( 0.018)\tLoss 1.3562e+18 (1.3562e+18)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 19: No improvement. Patience counter: 18/50\n",
      "Epoch: [19][0/1]\tTime  0.014 ( 0.014)\tData  0.000 ( 0.000)\tLoss 1.3788e+18 (1.3788e+18)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.008 ( 0.008)\tLoss 3.4906e+19 (3.4906e+19)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 20: No improvement. Patience counter: 19/50\n",
      "Epoch: [20][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 3.6950e+19 (3.6950e+19)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.010 ( 0.010)\tLoss 8.0626e+18 (8.0626e+18)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 21: No improvement. Patience counter: 20/50\n",
      "Epoch: [21][0/1]\tTime  0.011 ( 0.011)\tData  0.000 ( 0.000)\tLoss 8.2481e+18 (8.2481e+18)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 2.0303e+21 (2.0303e+21)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 22: No improvement. Patience counter: 21/50\n",
      "Epoch: [22][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 2.1311e+21 (2.1311e+21)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 5.4163e+20 (5.4163e+20)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 23: No improvement. Patience counter: 22/50\n",
      "Epoch: [23][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 5.3531e+20 (5.3531e+20)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.018 ( 0.018)\tLoss 1.1275e+23 (1.1275e+23)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 24: No improvement. Patience counter: 23/50\n",
      "Epoch: [24][0/1]\tTime  0.010 ( 0.010)\tData  0.002 ( 0.002)\tLoss 1.1859e+23 (1.1859e+23)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.009 ( 0.009)\tLoss 6.7019e+22 (6.7019e+22)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 25: No improvement. Patience counter: 24/50\n",
      "Epoch: [25][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 6.6652e+22 (6.6652e+22)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 6.0283e+24 (6.0283e+24)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 26: No improvement. Patience counter: 25/50\n",
      "Epoch: [26][0/1]\tTime  0.022 ( 0.022)\tData  0.000 ( 0.000)\tLoss 6.3311e+24 (6.3311e+24)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.010 ( 0.010)\tLoss 5.5061e+24 (5.5061e+24)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 27: No improvement. Patience counter: 26/50\n",
      "Epoch: [27][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 5.4547e+24 (5.4547e+24)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.018 ( 0.018)\tLoss 3.1500e+26 (3.1500e+26)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 28: No improvement. Patience counter: 27/50\n",
      "Epoch: [28][0/1]\tTime  0.012 ( 0.012)\tData  0.002 ( 0.002)\tLoss 3.3064e+26 (3.3064e+26)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 4.2869e+26 (4.2869e+26)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 29: No improvement. Patience counter: 28/50\n",
      "Epoch: [29][0/1]\tTime  0.005 ( 0.005)\tData  0.005 ( 0.005)\tLoss 4.2420e+26 (4.2420e+26)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.023 ( 0.023)\tLoss 1.5955e+28 (1.5955e+28)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 30: No improvement. Patience counter: 29/50\n",
      "Epoch: [30][0/1]\tTime  0.013 ( 0.013)\tData  0.002 ( 0.002)\tLoss 1.6728e+28 (1.6728e+28)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.007 ( 0.007)\tLoss 3.0818e+28 (3.0818e+28)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 31: No improvement. Patience counter: 30/50\n",
      "Epoch: [31][0/1]\tTime  0.016 ( 0.016)\tData  0.007 ( 0.007)\tLoss 3.0423e+28 (3.0423e+28)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.013 ( 0.013)\tLoss 7.8356e+29 (7.8356e+29)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 32: No improvement. Patience counter: 31/50\n",
      "Epoch: [32][0/1]\tTime  0.001 ( 0.001)\tData  0.001 ( 0.001)\tLoss 8.2045e+29 (8.2045e+29)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 2.1059e+30 (2.1059e+30)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 33: No improvement. Patience counter: 32/50\n",
      "Epoch: [33][0/1]\tTime  0.018 ( 0.018)\tData  0.010 ( 0.010)\tLoss 2.0743e+30 (2.0743e+30)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.000 ( 0.000)\tLoss 3.7202e+31 (3.7202e+31)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 34: No improvement. Patience counter: 33/50\n",
      "Epoch: [34][0/1]\tTime  0.000 ( 0.000)\tData  0.000 ( 0.000)\tLoss 3.8886e+31 (3.8886e+31)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 1.3731e+32 (1.3731e+32)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 35: No improvement. Patience counter: 34/50\n",
      "Epoch: [35][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 1.3492e+32 (1.3492e+32)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss 1.7043e+33 (1.7043e+33)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 36: No improvement. Patience counter: 35/50\n",
      "Epoch: [36][0/1]\tTime  0.012 ( 0.012)\tData  0.000 ( 0.000)\tLoss 1.7775e+33 (1.7775e+33)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.014 ( 0.014)\tLoss 8.5853e+33 (8.5853e+33)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 37: No improvement. Patience counter: 36/50\n",
      "Epoch: [37][0/1]\tTime  0.014 ( 0.014)\tData  0.004 ( 0.004)\tLoss 8.4143e+33 (8.4143e+33)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.006 ( 0.006)\tLoss 7.5135e+34 (7.5135e+34)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 38: No improvement. Patience counter: 37/50\n",
      "Epoch: [38][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 7.8140e+34 (7.8140e+34)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.011 ( 0.011)\tLoss 5.1541e+35 (5.1541e+35)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 39: No improvement. Patience counter: 38/50\n",
      "Epoch: [39][0/1]\tTime  0.011 ( 0.011)\tData  0.005 ( 0.005)\tLoss 5.0385e+35 (5.0385e+35)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.010 ( 0.010)\tLoss 3.1785e+36 (3.1785e+36)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 40: No improvement. Patience counter: 39/50\n",
      "Epoch: [40][0/1]\tTime  0.016 ( 0.016)\tData  0.000 ( 0.000)\tLoss 3.2931e+36 (3.2931e+36)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss inf (inf)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 41: No improvement. Patience counter: 40/50\n",
      "Epoch: [41][0/1]\tTime  0.015 ( 0.015)\tData  0.003 ( 0.003)\tLoss inf (inf)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.010 ( 0.010)\tLoss inf (inf)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 42: No improvement. Patience counter: 41/50\n",
      "Epoch: [42][0/1]\tTime  0.013 ( 0.013)\tData  0.002 ( 0.002)\tLoss inf (inf)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.014 ( 0.014)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 43: No improvement. Patience counter: 42/50\n",
      "Epoch: [43][0/1]\tTime  0.012 ( 0.012)\tData  0.000 ( 0.000)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.013 ( 0.013)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 44: No improvement. Patience counter: 43/50\n",
      "Epoch: [44][0/1]\tTime  0.014 ( 0.014)\tData  0.002 ( 0.002)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.015 ( 0.015)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 45: No improvement. Patience counter: 44/50\n",
      "Epoch: [45][0/1]\tTime  0.014 ( 0.014)\tData  0.004 ( 0.004)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.012 ( 0.012)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 46: No improvement. Patience counter: 45/50\n",
      "Epoch: [46][0/1]\tTime  0.018 ( 0.018)\tData  0.000 ( 0.000)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.018 ( 0.018)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 47: No improvement. Patience counter: 46/50\n",
      "Epoch: [47][0/1]\tTime  0.017 ( 0.017)\tData  0.004 ( 0.004)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.020 ( 0.020)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 48: No improvement. Patience counter: 47/50\n",
      "Epoch: [48][0/1]\tTime  0.015 ( 0.015)\tData  0.002 ( 0.002)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.017 ( 0.017)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 49: No improvement. Patience counter: 48/50\n",
      "Epoch: [49][0/1]\tTime  0.016 ( 0.016)\tData  0.002 ( 0.002)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.016 ( 0.016)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 50: No improvement. Patience counter: 49/50\n",
      "Epoch: [50][0/1]\tTime  0.015 ( 0.015)\tData  0.002 ( 0.002)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      "in validation finction tensor([50.], device='cuda:0')\n",
      "Test: [0/1]\tTime  0.017 ( 0.017)\tLoss nan (nan)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.000\n",
      "❌ Epoch 51: No improvement. Patience counter: 50/50\n",
      "⏹️ Early stopping triggered at epoch 51. Best Acc@1: 50.00\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0.0\n",
    "patience = 50  # Adjust as needed\n",
    "patience_counter = 0\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "start_epoch = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_finetune.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    finetune_adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # Train for one epoch\n",
    "    finetune_train(finetune_train_loader, model_finetune, criterion, optimizer, epoch)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    acc1 = finetune_validate(finetune_val_loader, model_finetune, criterion)\n",
    "\n",
    "    # Check if current accuracy is the best\n",
    "    is_best = acc1 > best_acc1\n",
    "\n",
    "    if is_best:\n",
    "        best_acc1 = acc1\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Save best model only\n",
    "        finetune_save_checkpoint(timestamp, epoch, {\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'vgg16',\n",
    "            'state_dict': model_finetune.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best=True)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}: New best Acc@1: {best_acc1:.2f}. Model saved.\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"❌ Epoch {epoch+1}: No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⏹️ Early stopping triggered at epoch {epoch+1}. Best Acc@1: {best_acc1:.2f}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "828ce5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250614_162947\n"
     ]
    }
   ],
   "source": [
    "train_time = time.time()\n",
    "\n",
    "\n",
    "print(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c42bbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9859cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithDataset(n): \n",
    "    hsi_test = dataset[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    test_gt = hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {test_gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(test_gt == 0)\n",
    "    indices1 = np.argwhere(test_gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    num_samples = 5000\n",
    "\n",
    "    random_indices0 = indices0[np.random.choice(len(indices0), num_samples, replace=False)]\n",
    "    random_indices1 = indices1[np.random.choice(len(indices1), num_samples, replace=False)]\n",
    "\n",
    "    test_indices = np.vstack((random_indices0, random_indices1))\n",
    "\n",
    "    print(test_indices.shape)\n",
    "\n",
    "    return test_indices, test_gt, matrix, random_indices0.shape, random_indices1.shape\n",
    "\n",
    "\n",
    "def testWithWholeDataset(n): \n",
    "    hsi_test = dataset[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    gt= hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(gt == 0)\n",
    "    indices1 = np.argwhere(gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    return matrix, gt, indices0.shape, indices1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a0cfff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, batch_input, device):\n",
    "    model.eval()\n",
    "    batch_input = batch_input.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_input)\n",
    "        # Apply softmax to get class probabilities\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        # Get predicted class (0 or 1)\n",
    "        predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "\n",
    "        # Get probability of class 1 (positive class) — required for ROC\n",
    "        positive_class_probs = probabilities[:, 1].cpu().numpy()\n",
    "\n",
    "    \n",
    "\n",
    "    return predicted_classes, positive_class_probs\n",
    "\n",
    "\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, matrix, gt, half_patch, expected_shape):\n",
    "        self.matrix = matrix\n",
    "        self.gt = gt\n",
    "        self.half_patch = half_patch\n",
    "        self.expected_shape = expected_shape\n",
    "        self.size_x, self.size_y = matrix.shape[0], matrix.shape[1]\n",
    "        self.valid_coords = [\n",
    "            (x, y)\n",
    "            for x in range(half_patch, self.size_x - half_patch)\n",
    "            for y in range(half_patch, self.size_y - half_patch)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.valid_coords[idx]\n",
    "        true_label = self.gt[x - self.half_patch, y - self.half_patch]\n",
    "\n",
    "        selected_rows = self.matrix[x- self.half_patch:x + 2 * self.half_patch + 1 - self.half_patch, :]\n",
    "        testing_patch = selected_rows[:, y - self.half_patch:y + 2 * self.half_patch + 1 - self.half_patch]\n",
    "\n",
    "        # Verify patch size\n",
    "        if testing_patch.shape != self.expected_shape:\n",
    "            raise ValueError(f\"Patch at ({x},{y}) has wrong shape {testing_patch.shape}\")\n",
    "\n",
    "        patch_tensor = torch.tensor(testing_patch, dtype=torch.float32)\n",
    "        patch_tensor = patch_tensor.permute(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "        return patch_tensor, true_label, x, y  # Also return (x, y) for positioning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "575b807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\finetune\\20250614_162947_model.pth.tar\n",
      "Creating model 20250614_162947_model.pth.tar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus TUF\\AppData\\Local\\Temp\\ipykernel_8944\\4111734671.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device\n",
      "SimSiam(\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(224, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (4): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "        (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "        (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64  # You can change this depending on your GPU capacity\n",
    "\n",
    "model_path = rf\"models\\finetune\\{timestamp}_model.pth.tar\"\n",
    "model_name = model_path.split('\\\\')[-1]\n",
    "print(model_path)\n",
    "\n",
    "print(f\"Creating model {model_name}...\")\n",
    "saved_model = VGG16_HSI().to(device)\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "saved_model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Model loaded and moved to device\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9ae6c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tes: 0\n",
      "img shape: (1243, 684, 224)\n",
      "img shape after padding (1251, 692, 224)\n",
      "number of pixel 850212\n",
      "ground truth shape: (1243, 684)\n",
      "indices = 0 shape: (820876, 2)\n",
      "indices = 1 shape: (29336, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:03<00:00, 49.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 1\n",
      "img shape: (1786, 699, 224)\n",
      "img shape after padding (1794, 707, 224)\n",
      "number of pixel 1248414\n",
      "ground truth shape: (1786, 699)\n",
      "indices = 0 shape: (1236269, 2)\n",
      "indices = 1 shape: (12145, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 55.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 2\n",
      "img shape: (1386, 690, 224)\n",
      "img shape after padding (1394, 698, 224)\n",
      "number of pixel 956340\n",
      "ground truth shape: (1386, 690)\n",
      "indices = 0 shape: (916980, 2)\n",
      "indices = 1 shape: (39360, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 55.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 3\n",
      "img shape: (1466, 676, 224)\n",
      "img shape after padding (1474, 684, 224)\n",
      "number of pixel 991016\n",
      "ground truth shape: (1466, 676)\n",
      "indices = 0 shape: (959167, 2)\n",
      "indices = 1 shape: (31849, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 55.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 4\n",
      "img shape: (2085, 682, 224)\n",
      "img shape after padding (2093, 690, 224)\n",
      "number of pixel 1421970\n",
      "ground truth shape: (2085, 682)\n",
      "indices = 0 shape: (1363408, 2)\n",
      "indices = 1 shape: (58562, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 54.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 5\n",
      "img shape: (2088, 691, 224)\n",
      "img shape after padding (2096, 699, 224)\n",
      "number of pixel 1442808\n",
      "ground truth shape: (2088, 691)\n",
      "indices = 0 shape: (1389552, 2)\n",
      "indices = 1 shape: (53256, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 55.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 6\n",
      "img shape: (1965, 492, 224)\n",
      "img shape after padding (1973, 500, 224)\n",
      "number of pixel 966780\n",
      "ground truth shape: (1965, 492)\n",
      "indices = 0 shape: (873365, 2)\n",
      "indices = 1 shape: (93415, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 55.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 7\n",
      "img shape: (1532, 567, 224)\n",
      "img shape after padding (1540, 575, 224)\n",
      "number of pixel 868644\n",
      "ground truth shape: (1532, 567)\n",
      "indices = 0 shape: (824964, 2)\n",
      "indices = 1 shape: (43680, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 54.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 8\n",
      "img shape: (1569, 517, 224)\n",
      "img shape after padding (1577, 525, 224)\n",
      "number of pixel 811173\n",
      "ground truth shape: (1569, 517)\n",
      "indices = 0 shape: (742935, 2)\n",
      "indices = 1 shape: (68238, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 56.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n",
      "tes: 9\n",
      "img shape: (1084, 680, 224)\n",
      "img shape after padding (1092, 688, 224)\n",
      "number of pixel 737120\n",
      "ground truth shape: (1084, 680)\n",
      "indices = 0 shape: (691437, 2)\n",
      "indices = 1 shape: (45683, 2)\n",
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 157/157 [00:02<00:00, 55.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5000/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "scores = []\n",
    "\n",
    "groundtruth = []\n",
    "prediction = []\n",
    "y_probs = []\n",
    "\n",
    "\n",
    "if mode == \"test\":\n",
    "    for hsi_test in range(len(dataset)):\n",
    "        print(f\"tes: {hsi_test}\")\n",
    "        test_indices, test_gt, matrix, indices_0_shape, indices_1_shape = testWithDataset(hsi_test)\n",
    "\n",
    "        total = len(test_indices)\n",
    "        correct0 = 0\n",
    "        correct1 = 0\n",
    "\n",
    "        input_patches = []\n",
    "        true_labels = []\n",
    "\n",
    "        # Prepare all patches\n",
    "        for x_pos, y_pos in test_indices:\n",
    "            true_label = test_gt[x_pos][y_pos]\n",
    "\n",
    "            selected_rows = matrix[x_pos:x_pos + 2*half_patch + 1, :]\n",
    "            testing_patch = selected_rows[:, y_pos:y_pos + 2*half_patch + 1]\n",
    "\n",
    "            patch_tensor = torch.tensor(testing_patch, dtype=torch.float32)\n",
    "            patch_tensor = patch_tensor.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "            input_patches.append(patch_tensor)\n",
    "            true_labels.append(true_label)\n",
    "\n",
    "        input_patches = torch.cat(input_patches, dim=0)  # Shape: (N, C, H, W)\n",
    "        true_labels = torch.tensor(true_labels)\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, total, batch_size), desc=\"Predicting\"):\n",
    "            batch = input_patches[i:i+batch_size]\n",
    "            labels = true_labels[i:i+batch_size]\n",
    "\n",
    "            groundtruth.append(labels)\n",
    "\n",
    "            preds, postive_class_probs = predict_batch(saved_model, batch, device)\n",
    "\n",
    "            prediction.append(preds)\n",
    "            y_probs.append(postive_class_probs)\n",
    "\n",
    "            for j in range(len(preds)):\n",
    "                index = i + j\n",
    "                # print(f\"{index+1}: prediction = {preds[j]}, confidence: {confs[j]:.4f}, expected: {labels[j].item()}\")\n",
    "                if preds[j] == labels[j].item():\n",
    "                    if labels[j].item() == 0:\n",
    "                        correct0 += 1\n",
    "                    elif labels[j] == 1:\n",
    "                        correct1 += 1\n",
    "\n",
    "        correct = correct0 + correct1\n",
    "        print(f\"Score: {correct}/{total}\")\n",
    "        \n",
    "        score = {\n",
    "            'dataset': hsi_test,\n",
    "            'class0_size': indices_0_shape[0],\n",
    "            'class1_size': indices_1_shape[0],\n",
    "            'correct_0': correct0,\n",
    "            'correct_1': correct1,\n",
    "            'correct_total': correct,\n",
    "            'total': total\n",
    "        }\n",
    "        scores.append(score)\n",
    "\n",
    "if mode == \"full\":\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    os.makedirs(f\"predictions/{timestamp}\", exist_ok=True)\n",
    "    for hsi_full in range(len(dataset)):\n",
    "        print(f\"tes: {hsi_full}\")\n",
    "        score = []\n",
    "        patch_size = 9\n",
    "        half_patch = patch_size // 2\n",
    "\n",
    "        data_sampler = None\n",
    "        batch_size = 64\n",
    "\n",
    "        correct0 = 0\n",
    "        correct1 = 0\n",
    "        matrix = []\n",
    "        gt = []\n",
    "        expected_patch_shape = []\n",
    "        dataset_patches = []\n",
    "        data_loader = []\n",
    "        patch_tensor = []\n",
    "        true_label = [] \n",
    "        x = []\n",
    "        y = []\n",
    "        pred_matrix = []\n",
    "\n",
    "        matrix, gt, indices_0_shape, indices_1_shape = testWithWholeDataset(hsi_full)\n",
    "        print(indices_0_shape[0])\n",
    "        print(indices_1_shape[0])\n",
    "\n",
    "        expected_patch_shape = (2 * half_patch + 1, 2 * half_patch + 1, matrix.shape[2])\n",
    "        dataset_patches = PatchDataset(matrix, gt, half_patch, expected_patch_shape)\n",
    "\n",
    "        if seeded_run:\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(seed)\n",
    "\n",
    "            data_loader = DataLoader(\n",
    "                dataset_patches,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,  # set to True if needed\n",
    "                num_workers=0,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "                generator=g\n",
    "            )\n",
    "            print(\"generate data loader using seed\")\n",
    "        else:\n",
    "            data_loader = DataLoader(dataset_patches, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "        patch_tensor, true_label, x, y = next(iter(data_loader))\n",
    "\n",
    "        print(patch_tensor.size())\n",
    "        print(true_label.size())\n",
    "        print(f\"data loader size: {len(data_loader)}\")\n",
    "\n",
    "        pred_matrix = np.full(gt.shape, -1, dtype=np.int32)\n",
    "        correct = 0\n",
    "\n",
    "        for input_batch, label_batch, x_batch, y_batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "\n",
    "\n",
    "            preds, confs = predict_batch(saved_model, input_batch, device)\n",
    "\n",
    "            prediction.append(preds)\n",
    "            y_probs.append(confs)\n",
    "            \n",
    "            label_batch = label_batch.numpy()\n",
    "            x_batch = x_batch.numpy()\n",
    "            y_batch = y_batch.numpy()\n",
    "\n",
    "            for pred, label, x, y in zip(preds, label_batch, x_batch, y_batch):\n",
    "                groundtruth.append(label)\n",
    "                pred_matrix[x - half_patch, y - half_patch] = pred\n",
    "                if pred == label:\n",
    "                    if label == 0:\n",
    "                        correct0 += 1\n",
    "                    elif label == 1:\n",
    "                        correct1 += 1\n",
    "                    \n",
    "        correct = correct0+correct1\n",
    "        print(f\"correct0 = {correct0}\")\n",
    "        print(f\"correct1 = {correct1}\")\n",
    "        total = gt.shape[0] * gt.shape[1]\n",
    "        print(f\"Score: {correct}/{total}\")\n",
    "\n",
    "        score = {\n",
    "            'dataset': hsi_full,\n",
    "            'class0_size': indices_0_shape[0],\n",
    "            'class1_size': indices_1_shape[0],\n",
    "            'correct_0': correct0,\n",
    "            'correct_1': correct1,\n",
    "            'correct_total': correct,\n",
    "            'total': total\n",
    "        }\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "        # Save prediction matrix\n",
    "        \n",
    "        np.save(f\"predictions/{timestamp}/prediction_matrix_dataset {hsi_full} MyMethod.npy\", pred_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3802cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 0\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 1\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 2\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 3\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 4\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 5\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 6\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 7\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 8\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "dataset: 9\t 0/5000\t 5000/5000\t 5000/10000\t\n",
      "total: \t\t 0/25000.0 \t 50000/25000.0 \t 50000/100000\n",
      "acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "all_correct = 0\n",
    "all_total = 0\n",
    "all_correct0 = 0\n",
    "all_correct1 = 0\n",
    "class0_total = 0\n",
    "class1_total = 0\n",
    "\n",
    "for score in scores:\n",
    "    dataset = score['dataset']\n",
    "    correct0 = score['correct_0']\n",
    "    correct1 = score['correct_1']\n",
    "    class0_size = score['class0_size']\n",
    "    class1_size = score['class1_size']\n",
    "    correct = score['correct_total']\n",
    "    total = score['total']\n",
    "    print(f\"dataset: {dataset}\\t\", f'{correct0}/{class0_size}\\t', f'{correct1}/{class1_size}\\t', f'{correct}/{total}\\t')\n",
    "\n",
    "    all_correct += correct\n",
    "    all_total += total\n",
    "    all_correct0 += correct0\n",
    "    all_correct1 += correct1\n",
    "    class0_total += class0_size\n",
    "    class1_total += class1_size\n",
    "\n",
    "\n",
    "\n",
    "print(f\"total: \\t\\t {all_correct0}/{class0_total/2} \\t {all_correct1}/{class1_total/2} \\t {all_correct}/{all_total}\")\n",
    "\n",
    "print(f\"acc: {all_correct/all_total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c74a969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_total_score = {\n",
    "    'dataset': 'Total Dataset',\n",
    "    'correct_0': all_correct0,\n",
    "    'correct_1': all_correct1,\n",
    "    'class0_total': class0_total,\n",
    "    'class1_total': class1_total,\n",
    "    'correct_total': all_correct,\n",
    "    'total': all_total\n",
    "}\n",
    "\n",
    "scores.append(all_total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddab0694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "groundtruths = groundtruth\n",
    "groundtruth_in = []\n",
    "\n",
    "if mode == \"test\":\n",
    "    for x in groundtruths:\n",
    "        for y in x:\n",
    "            groundtruth_in.append(y)    \n",
    "\n",
    "if mode == \"full\":\n",
    "    for x in groundtruths:\n",
    "        groundtruth_in.append(x)\n",
    "\n",
    "predictions = prediction\n",
    "prediction_in = []\n",
    "\n",
    "for x in predictions:\n",
    "    for y in x:\n",
    "        prediction_in.append(y)\n",
    "\n",
    "\n",
    "y_prob_in = []\n",
    "\n",
    "for x in y_probs:\n",
    "    for y in x:\n",
    "        y_prob_in.append(y)\n",
    "\n",
    "print(len(groundtruth_in))\n",
    "print(len(prediction_in))\n",
    "print(len(y_prob_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e85a806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/100000\n"
     ]
    }
   ],
   "source": [
    "y_test = groundtruth_in\n",
    "y_pred = prediction_in\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for x, y in zip(y_test, y_pred):\n",
    "    total += 1\n",
    "    if x == y:\n",
    "        correct += 1\n",
    "\n",
    "print(f'{correct}/{total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a4761e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test: [0 1]\n",
      "Sample y_pred values: [np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "y_test_np = np.array([label.item() for label in y_test])\n",
    "# Ensure labels are binary (0 and 1)\n",
    "print(\"Unique values in y_test:\", pd.Series(y_test_np).unique())\n",
    "\n",
    "# Check if y_pred is probability (float) or hard prediction (int)\n",
    "print(\"Sample y_pred values:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29515cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(\n",
    "    {'True': y_test_np, 'Model': y_prob_in})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_df['True'], test_df['Model'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'Model (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Two Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6a288ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2500\n",
      "Recall:    0.5000\n",
      "F1 Score:  0.3333\n",
      "OA:        0.5000\n",
      "AA:        0.5000\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([int(label) for label in y_test_np])  # true labels\n",
    "y_pred = prediction_in                         # predicted class labels (e.g., from predict_batch)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # Use 'binary' if binary task\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Overall Accuracy (OA)\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Average Accuracy (AA) — mean of per-class accuracies\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "aa = per_class_acc.mean()\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"OA:        {oa:.4f}\")\n",
    "print(f\"AA:        {aa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8abb93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {\n",
    "    'AUC': float(roc_auc),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'F1 Score': float(f1),\n",
    "    'OA': float(oa),\n",
    "    'AA': float(aa),\n",
    "}\n",
    "result_json = {\n",
    "    'prediction' : scores,\n",
    "    'performance' : performance,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c0c5d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': [{'dataset': 0, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 1, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 2, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 3, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 4, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 5, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 6, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 7, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 8, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 9, 'class0_size': 5000, 'class1_size': 5000, 'correct_0': 0, 'correct_1': 5000, 'correct_total': 5000, 'total': 10000}, {'dataset': 'Total Dataset', 'correct_0': 0, 'correct_1': 50000, 'class0_total': 50000, 'class1_total': 50000, 'correct_total': 50000, 'total': 100000}], 'performance': {'AUC': 0.5, 'precision': 0.25, 'recall': 0.5, 'F1 Score': 0.3333333333333333, 'OA': 0.5, 'AA': 0.5}}\n",
      "JSON saved to results.json\n"
     ]
    }
   ],
   "source": [
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(result_json)\n",
    "\n",
    "with open(f\"performance/MyMethod {timestamp}_results.json\", \"w\") as f:\n",
    "    json.dump(result_json, f, indent=2)\n",
    "\n",
    "print(\"JSON saved to results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "901b6440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 515.7980 seconds\n",
      "predicting time: 122.7980 seconds\n",
      "Run time: 638.5960 seconds\n",
      "mode used: test\n",
      "20250614_162947\n",
      "seet used: 10\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Train time: {train_time - start_time:.4f} seconds\")\n",
    "print(f\"predicting time: {end_time - train_time:.4f} seconds\")\n",
    "print(f\"Run time: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"mode used: {mode}\")\n",
    "\n",
    "print(timestamp)\n",
    "print(f\"seet used: {seed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
