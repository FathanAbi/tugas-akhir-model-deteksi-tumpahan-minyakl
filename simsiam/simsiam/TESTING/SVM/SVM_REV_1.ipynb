{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab9c9cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearnex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearnex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m patch_sklearn\n\u001b[0;32m     17\u001b[0m patch_sklearn()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearnex'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from HSI_class import HSI\n",
    "import zeroPadding\n",
    "import augmentation as aug\n",
    "import createSample as CS\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "seeded_run = True\n",
    "seed = 10\n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "epochs = 200\n",
    "\n",
    "batch_size =40\n",
    "test_size = 0.5\n",
    "\n",
    "random_indices = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # PyTorch determinism\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if seeded_run:\n",
    "    set_seed(seed)\n",
    "    print(\"seed has been set\")\n",
    "    print(f\"seet used: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f79fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\FathanAbi\\tugas-akhir-model-deteksi-tumpahan-minyakl\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "datasets = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i>9:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        datasets.append(hsi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d40bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithDataset(n): \n",
    "    hsi_test = datasets[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    test_gt = hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {test_gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(test_gt == 0)\n",
    "    indices1 = np.argwhere(test_gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    num_samples = 50\n",
    "\n",
    "    random_indices0 = indices0[np.random.choice(len(indices0), num_samples, replace=False)]\n",
    "    random_indices1 = indices1[np.random.choice(len(indices1), num_samples, replace=False)]\n",
    "\n",
    "    test_indices = np.vstack((random_indices0, random_indices1))\n",
    "\n",
    "    print(test_indices.shape)\n",
    "\n",
    "    return test_indices, test_gt, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, batch_input, device):\n",
    "    test_features_np = torch.flatten(batch_input, start_dim=1)  # Flatten all dims except batch\n",
    "   # convert to NumPy\n",
    "\n",
    "    predicted_classes = model.predict(test_features_np)\n",
    "    probs = model.predict_proba(test_features_np)\n",
    "    positive_probs = probs[:, 1]  # probability of class 1\n",
    "\n",
    "\n",
    "    return predicted_classes, positive_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsi_ = datasets[0]\n",
    "patch_size = 9\n",
    "\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random_indices}\")\n",
    "random_indices = 1\n",
    "if random_indices:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(188), np.int64(124)), (np.int64(523), np.int64(150)), (np.int64(1003), np.int64(474)), (np.int64(616), np.int64(508)), (np.int64(905), np.int64(552))]\n",
    "    indices_1 = [(np.int64(106), np.int64(606)), (np.int64(297), np.int64(468)), (np.int64(926), np.int64(35)), (np.int64(536), np.int64(519)), (np.int64(508), np.int64(442))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "\n",
    "\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =1\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])\n",
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ccef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e55134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvTo1D(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvTo1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=224, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))  # (batch_size, 128, 1, 1)\n",
    "#         self.flatten = nn.Flatten()               # (batch_size, 128)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.flatten(x)\n",
    "#         return x\n",
    "\n",
    "# feature_extractor = ConvTo1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59567562",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_augment.shape)\n",
    "print(label_augment.shape)\n",
    "\n",
    "x_data = data_augment \n",
    "y_labels = label_augment\n",
    "\n",
    "x_data = torch.tensor(x_data)\n",
    "x_data = x_data.to(torch.float32)\n",
    "x_data = x_data.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {x_data.shape}\")\n",
    "\n",
    "features_np = torch.flatten(x_data, start_dim=1)  # Flatten all dims except batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_np\n",
    "y = y_labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b654b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train SVM with probability enabled and deterministic behavior ---\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', C=100, gamma=\"scale\", probability=True, random_state=seed)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate on validation set ---\n",
    "predicted_classes = pipeline.predict(X_val)\n",
    "probs = pipeline.predict_proba(X_val)\n",
    "positive_probs = probs[:, 1]  # probability of class 1\n",
    "\n",
    "# Combine both into a display\n",
    "for i, (pred, prob) in enumerate(zip(predicted_classes, positive_probs)):\n",
    "    print(f\"Sample {i}: Predicted = {pred}, Prob(class 1) = {prob:.4f}\")\n",
    "\n",
    "# y_pred = svm.predict(X_val)\n",
    "acc = accuracy_score(y_val, predicted_classes)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fa9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithWholeDataset(n): \n",
    "    hsi_test = datasets[n]\n",
    "\n",
    "    test_img = hsi_test.img\n",
    "    gt= hsi_test.gt\n",
    "\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    height = test_img.shape[0]\n",
    "    width = test_img.shape[1]\n",
    "\n",
    "    matrix=zeroPadding.zeroPadding_3D(test_img,half_patch) #add 0 in every side of the data\n",
    "    print(f\"img shape: {test_img.shape}\")\n",
    "    print(f\"img shape after padding {matrix.shape}\")\n",
    "    print(f\"number of pixel {width * height}\")\n",
    "\n",
    "    print(f\"ground truth shape: {gt.shape}\")\n",
    "\n",
    "    indices0 = np.argwhere(gt == 0)\n",
    "    indices1 = np.argwhere(gt == 1)\n",
    "\n",
    "    print(f\"indices = 0 shape: {indices0.shape}\")\n",
    "    print(f\"indices = 1 shape: {indices1.shape}\")\n",
    "\n",
    "    return matrix, gt, indices0.shape, indices1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_whole(pipeline, batch_input, device):\n",
    "  test_features_np = torch.flatten(batch_input, start_dim=1)  # Flatten all dims except batch\n",
    "\n",
    "\n",
    "  # --- Evaluate on validation set ---\n",
    "  predicted_classes = pipeline.predict(test_features_np)\n",
    "  probs = pipeline.predict_proba(test_features_np)\n",
    "  positive_probs = probs[:, 1]  # probability of class 1\n",
    "\n",
    "  return predicted_classes, positive_probs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, matrix, gt, half_patch, expected_shape):\n",
    "        self.matrix = matrix\n",
    "        self.gt = gt\n",
    "        self.half_patch = half_patch\n",
    "        self.expected_shape = expected_shape\n",
    "        self.size_x, self.size_y = matrix.shape[0], matrix.shape[1]\n",
    "        self.valid_coords = [\n",
    "            (x, y)\n",
    "            for x in range(half_patch, self.size_x - half_patch)\n",
    "            for y in range(half_patch, self.size_y - half_patch)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.valid_coords[idx]\n",
    "        true_label = self.gt[x - self.half_patch, y - self.half_patch]\n",
    "\n",
    "        selected_rows = self.matrix[x- self.half_patch:x + 2 * self.half_patch + 1 - self.half_patch, :]\n",
    "        testing_patch = selected_rows[:, y - self.half_patch:y + 2 * self.half_patch + 1 - self.half_patch]\n",
    "\n",
    "        # Verify patch size\n",
    "        if testing_patch.shape != self.expected_shape:\n",
    "            raise ValueError(f\"Patch at ({x},{y}) has wrong shape {testing_patch.shape}\")\n",
    "\n",
    "        patch_tensor = torch.tensor(testing_patch, dtype=torch.float32)\n",
    "        patch_tensor = patch_tensor.permute(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "        return patch_tensor, true_label, x, y  # Also return (x, y) for positioning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29552a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(prediction, y_probs, groundtruth):\n",
    "    groundtruths = groundtruth\n",
    "    groundtruth_in = []\n",
    "\n",
    "    for x in groundtruths:\n",
    "        groundtruth_in.append(x)\n",
    "\n",
    "    predictions = prediction\n",
    "    prediction_in = []\n",
    "\n",
    "    for x in predictions:\n",
    "        for y in x:\n",
    "            prediction_in.append(y)\n",
    "\n",
    "\n",
    "    y_prob_in = []\n",
    "\n",
    "    for x in y_probs:\n",
    "        for y in x:\n",
    "            y_prob_in.append(y)\n",
    "\n",
    "    # print(len(groundtruth_in))\n",
    "    # print(len(prediction_in))\n",
    "    # print(len(y_prob_in))\n",
    "\n",
    "    y_test = groundtruth_in\n",
    "    y_pred = prediction_in\n",
    "\n",
    "    for x, y in zip(y_test, y_pred):\n",
    "        total += 1\n",
    "        if x == y:\n",
    "            correct += 1\n",
    "    \n",
    "    print(f'{correct}/{total}')\n",
    "\n",
    "    y_test_np = np.array([label.item() for label in y_test])\n",
    "    # Ensure labels are binary (0 and 1)\n",
    "    # print(\"Unique values in y_test:\", pd.Series(y_test_np).unique())\n",
    "\n",
    "    # # Check if y_pred is probability (float) or hard prediction (int)\n",
    "    # print(\"Sample y_pred values:\", y_pred[:5])\n",
    "\n",
    "    test_df = pd.DataFrame(\n",
    "        {'True': y_test_np, 'Model': y_prob_in})\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_df['True'], test_df['Model'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'Model (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Two Models')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    y_true = np.array([int(label) for label in y_test_np])  # true labels\n",
    "    y_pred = prediction_in                         # predicted class labels (e.g., from predict_batch)\n",
    "\n",
    "    # Precision, Recall, F1\n",
    "    precision = precision_score(y_true, y_pred, average='macro')  # Use 'binary' if binary task\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Overall Accuracy (OA)\n",
    "    oa = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Average Accuracy (AA) — mean of per-class accuracies\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    aa = per_class_acc.mean()\n",
    "\n",
    "    # Print all metrics\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"OA:        {oa:.4f}\")\n",
    "    print(f\"AA:        {aa:.4f}\")\n",
    "\n",
    "    performance = {\n",
    "        'AUC': float(roc_auc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'F1 Score': float(f1),\n",
    "        'OA': float(oa),\n",
    "        'AA': float(aa),\n",
    "    }\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scores = []\n",
    "groundtruth = []\n",
    "prediction = []\n",
    "y_probs = []\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "os.makedirs(f\"predictions/{timestamp}\", exist_ok=True)\n",
    "for dataset in range(len(datasets)):\n",
    "    hsi_prediction = []\n",
    "    hsi_yprobs = []\n",
    "    hsi_groundtruth = []\n",
    "\n",
    "    score = []\n",
    "    patch_size = 9\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    data_sampler = None\n",
    "    batch_size = 64\n",
    "\n",
    "    correct0 = 0\n",
    "    correct1 = 0\n",
    "    matrix = []\n",
    "    gt = []\n",
    "    expected_patch_shape = []\n",
    "    dataset_patches = []\n",
    "    data_loader = []\n",
    "    patch_tensor = []\n",
    "    true_label = [] \n",
    "    x = []\n",
    "    y = []\n",
    "    pred_matrix = []\n",
    "\n",
    "    matrix, gt, indices_0_shape, indices_1_shape = testWithWholeDataset(dataset)\n",
    "    print(indices_0_shape[0])\n",
    "    print(indices_1_shape[0])\n",
    "\n",
    "    expected_patch_shape = (2 * half_patch + 1, 2 * half_patch + 1, matrix.shape[2])\n",
    "    dataset_patches = PatchDataset(matrix, gt, half_patch, expected_patch_shape)\n",
    "\n",
    "    if seeded_run:\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset_patches,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,  # set to True if needed\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "            generator=g\n",
    "        )\n",
    "        print(\"generate data loader using seed\")\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset_patches, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    patch_tensor, true_label, x, y = next(iter(data_loader))\n",
    "\n",
    "    print(patch_tensor.size())\n",
    "    print(true_label.size())\n",
    "    print(f\"data loader size: {len(data_loader)}\")\n",
    "\n",
    "    pred_matrix = np.full(gt.shape, -1, dtype=np.int32)\n",
    "    correct = 0\n",
    "\n",
    "    for input_batch, label_batch, x_batch, y_batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "\n",
    "\n",
    "        preds, confs = predict_batch_whole(pipeline, input_batch, device)\n",
    "\n",
    "        hsi_prediction.append(preds)\n",
    "        prediction.append(preds)\n",
    "        hsi_yprobs.append(confs)\n",
    "        y_probs.append(confs)\n",
    "        \n",
    "        label_batch = label_batch.numpy()\n",
    "        x_batch = x_batch.numpy()\n",
    "        y_batch = y_batch.numpy()\n",
    "\n",
    "        for pred, label, x, y in zip(preds, label_batch, x_batch, y_batch):\n",
    "            hsi_groundtruth.append(label)\n",
    "            groundtruth.append(label)\n",
    "            pred_matrix[x - half_patch, y - half_patch] = pred\n",
    "            if pred == label:\n",
    "                if label == 0:\n",
    "                    correct0 += 1\n",
    "                elif label == 1:\n",
    "                    correct1 += 1\n",
    "\n",
    "    performance_metrics = getScore(hsi_prediction, hsi_yprobs, hsi_groundtruth)      \n",
    "          \n",
    "    correct = correct0+correct1\n",
    "    print(f\"correct0 = {correct0}\")\n",
    "    print(f\"correct1 = {correct1}\")\n",
    "    total = gt.shape[0] * gt.shape[1]\n",
    "    print(f\"Score: {correct}/{total}\")\n",
    "\n",
    "    score = {\n",
    "        'dataset': dataset,\n",
    "        'class0_size': indices_0_shape[0],\n",
    "        'class1_size': indices_1_shape[0],\n",
    "        'correct_0': correct0,\n",
    "        'correct_1': correct1,\n",
    "        'correct_total': correct,\n",
    "        'total': total,\n",
    "        'AUC': float(performance_metrics['AUC']),\n",
    "        'precision': float(performance_metrics['precision']),\n",
    "        'recall': float(performance_metrics['recall']),\n",
    "        'F1 Score': float(performance_metrics['F1 Score']),\n",
    "        'OA': float(performance_metrics['OA']),\n",
    "        'AA': float(performance_metrics['AA']),\n",
    "    }\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "    # Save prediction matrix\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    np.save(f\"predictions/{timestamp}/results {dataset} SVM.npy\", pred_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correct = 0\n",
    "all_total = 0\n",
    "all_correct0 = 0\n",
    "all_correct1 = 0\n",
    "class0_total = 0\n",
    "class1_total = 0\n",
    "\n",
    "score = {\n",
    "        'dataset': dataset,\n",
    "        'class0_size': indices_0_shape[0],\n",
    "        'class1_size': indices_1_shape[0],\n",
    "        'correct_0': correct0,\n",
    "        'correct_1': correct1,\n",
    "        'correct_total': correct,\n",
    "        'total': total,\n",
    "        'AUC': float(performance_metrics['AUC']),\n",
    "        'precision': float(performance_metrics['precision']),\n",
    "        'recall': float(performance_metrics['recall']),\n",
    "        'F1 Score': float(performance_metrics['F1 Score']),\n",
    "        'OA': float(performance_metrics['OA']),\n",
    "        'AA': float(performance_metrics['AA']),\n",
    "    }\n",
    "\n",
    "for score in scores:\n",
    "    dataset = score['dataset']\n",
    "    correct0 = score['correct_0']\n",
    "    correct1 = score['correct_1']\n",
    "    class0_size = score['class0_size']\n",
    "    class1_size = score['class1_size']\n",
    "    correct = score['correct_total']\n",
    "    total = score['total']\n",
    "    auc = score['AUC']\n",
    "    precission = score['precission']\n",
    "    recall = score['recall']\n",
    "    f1 = score['F1 Score']\n",
    "    oa = score['OA']\n",
    "    aa = score['AA']\n",
    "    \n",
    "    print(f\"dataset: {dataset}\\t\", f'{correct0}/{class0_size}\\t', f'{correct1}/{class1_size}\\t', f'{correct}/{total}\\t')\n",
    "\n",
    "    all_correct += correct\n",
    "    all_total += total\n",
    "    all_correct0 += correct0\n",
    "    all_correct1 += correct1\n",
    "    class0_total += class0_size\n",
    "    class1_total += class1_size\n",
    "\n",
    "\n",
    "\n",
    "print(f\"total: \\t\\t {all_correct0}/{class0_total/2} \\t {all_correct1}/{class1_total/2} \\t {all_correct}/{all_total}\")\n",
    "\n",
    "print(f\"acc: {all_correct/all_total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_total_score = {\n",
    "    'dataset': 'Total Dataset',\n",
    "    'correct_0': all_correct0,\n",
    "    'correct_1': all_correct1,\n",
    "    'class0_total': class0_total,\n",
    "    'class1_total': class1_total,\n",
    "    'correct_total': all_correct,\n",
    "    'total': all_total\n",
    "}\n",
    "\n",
    "scores.append(all_total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee05689",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruths = groundtruth\n",
    "groundtruth_in = []\n",
    "\n",
    "for x in groundtruths:\n",
    "    groundtruth_in.append(x)\n",
    "\n",
    "predictions = prediction\n",
    "prediction_in = []\n",
    "\n",
    "for x in predictions:\n",
    "    for y in x:\n",
    "        prediction_in.append(y)\n",
    "\n",
    "\n",
    "y_prob_in = []\n",
    "\n",
    "for x in y_probs:\n",
    "    for y in x:\n",
    "        y_prob_in.append(y)\n",
    "\n",
    "print(len(groundtruth_in))\n",
    "print(len(prediction_in))\n",
    "print(len(y_prob_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e42082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = groundtruth_in\n",
    "y_pred = prediction_in\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for x, y in zip(y_test, y_pred):\n",
    "    total += 1\n",
    "    if x == y:\n",
    "        correct += 1\n",
    "\n",
    "print(f'{correct}/{total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = np.array([label.item() for label in y_test])\n",
    "# Ensure labels are binary (0 and 1)\n",
    "print(\"Unique values in y_test:\", pd.Series(y_test_np).unique())\n",
    "\n",
    "# Check if y_pred is probability (float) or hard prediction (int)\n",
    "print(\"Sample y_pred values:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e48bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    {'True': y_test_np, 'Model': y_prob_in})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_df['True'], test_df['Model'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'Model (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Two Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_true = np.array([int(label) for label in y_test_np])  # true labels\n",
    "y_pred = prediction_in                         # predicted class labels (e.g., from predict_batch)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # Use 'binary' if binary task\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Overall Accuracy (OA)\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Average Accuracy (AA) — mean of per-class accuracies\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "aa = per_class_acc.mean()\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"OA:        {oa:.4f}\")\n",
    "print(f\"AA:        {aa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63022330",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {\n",
    "    'AUC': float(roc_auc),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'F1 Score': float(f1),\n",
    "    'OA': float(oa),\n",
    "    'AA': float(aa),\n",
    "}\n",
    "result_json = {\n",
    "    'prediction' : scores,\n",
    "    'performance' : performance,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711abcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(result_json)\n",
    "\n",
    "with open(f\"performance/SVM {timestamp}_results.json\", \"w\") as f:\n",
    "    json.dump(result_json, f, indent=2)\n",
    "\n",
    "print(\"JSON saved to results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Run time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(timestamp)\n",
    "print(f\"seet used: {seed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
