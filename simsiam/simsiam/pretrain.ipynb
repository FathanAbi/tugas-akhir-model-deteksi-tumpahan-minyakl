{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "import simsiam.builder\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 20\n",
    "num_per_category_augment_2 = 20\n",
    "epochs = 200\n",
    "\n",
    "random = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception_v3', 'list_models', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n",
      "=> creating model 'vgg16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)\n",
    "# create model\n",
    "arch = 'vgg16' \n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = simsiam.builder.SimSiam(\n",
    "    models.__dict__[arch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n",
      "random: 1\n",
      "generating random sample\n",
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "creating 5 Randomly chosen 0 indices:\n",
      "creating 5 Randomly chosen 1 indices:\n",
      "indices 0 used: [(np.int64(680), np.int64(308)), (np.int64(587), np.int64(239)), (np.int64(275), np.int64(579)), (np.int64(106), np.int64(440)), (np.int64(226), np.int64(659))]\n",
      "indices 1 used: [(np.int64(935), np.int64(574)), (np.int64(176), np.int64(113)), (np.int64(544), np.int64(420)), (np.int64(929), np.int64(33)), (np.int64(144), np.int64(202))]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 0:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1\n",
    "\n",
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random}\")\n",
    "\n",
    "if random:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(526), np.int64(187)), (np.int64(537), np.int64(71)), (np.int64(496), np.int64(222)), (np.int64(1200), np.int64(102)), (np.int64(1178), np.int64(413))]\n",
    "    indices_1 = [(np.int64(174), np.int64(66)), (np.int64(382), np.int64(580)), (np.int64(1202), np.int64(171)), (np.int64(469), np.int64(254)), (np.int64(267), np.int64(228))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "# print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "# print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "# print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "# print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-238 -370  383  442  490  622  708  714  692  671  636  583  538  501\n",
      "  464  442  420  394  369  334  310  284  259  233  212  199  184  165\n",
      "  166  149  147  132  129  125  114  103   88   85   76   47   63   75\n",
      "   81   49   71   76   71   67   64   40   49   54   68   68   67   60\n",
      "   58   35   17    7    3  -47 -179  -77    4   17   36   45   48   53\n",
      "   55   47   53   48   47   35   28   23    5  -27 -258 -277 -130 -139\n",
      "  -43   -5    7    9   14   10   17   22   29   31   31   21   33   37\n",
      "   36   23   36   21   22   19  -30  -34    0    0    0    0    0    0\n",
      "    0    0 -227  -92  -56  -27  -72  -35    1   15   25   29   36   58\n",
      "   41   42   42   38   43   40   38   37   33   40   34   32   31   34\n",
      "   36   43   32   37   39   36   17   17   11   -1   -8 -132    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -96  -42  -11   -9   -4   -9  -47   -2    8   17   13   12   21   15\n",
      "   14   17   17   22   21   21   17   20   18   18   16   17   16   27\n",
      "   23   28   15   27   23   31   33   12   17   14    6    8   23    6\n",
      "   -5   12  -32   10   -1    5    6   10  -30    5  -30  -48  -85    3]\n",
      "[-238. -370.  383.  442.  490.  622.  708.  714.  692.  671.  636.  583.\n",
      "  538.  501.  464.  442.  420.  394.  369.  334.  310.  284.  259.  233.\n",
      "  212.  199.  184.  165.  166.  149.  147.  132.  129.  125.  114.  103.\n",
      "   88.   85.   76.   47.   63.   75.   81.   49.   71.   76.   71.   67.\n",
      "   64.   40.   49.   54.   68.   68.   67.   60.   58.   35.   17.    7.\n",
      "    3.  -47. -179.  -77.    4.   17.   36.   45.   48.   53.   55.   47.\n",
      "   53.   48.   47.   35.   28.   23.    5.  -27. -258. -277. -130. -139.\n",
      "  -43.   -5.    7.    9.   14.   10.   17.   22.   29.   31.   31.   21.\n",
      "   33.   37.   36.   23.   36.   21.   22.   19.  -30.  -34.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -227.  -92.  -56.  -27.  -72.  -35.\n",
      "    1.   15.   25.   29.   36.   58.   41.   42.   42.   38.   43.   40.\n",
      "   38.   37.   33.   40.   34.   32.   31.   34.   36.   43.   32.   37.\n",
      "   39.   36.   17.   17.   11.   -1.   -8. -132.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -96.  -42.  -11.   -9.   -4.   -9.  -47.   -2.    8.   17.   13.   12.\n",
      "   21.   15.   14.   17.   17.   22.   21.   21.   17.   20.   18.   18.\n",
      "   16.   17.   16.   27.   23.   28.   15.   27.   23.   31.   33.   12.\n",
      "   17.   14.    6.    8.   23.    6.   -5.   12.  -32.   10.   -1.    5.\n",
      "    6.   10.  -30.    5.  -30.  -48.  -85.    3.]\n",
      "[-249 -448  319  411  457  596  679  704  662  625  602  560  524  499\n",
      "  476  458  433  414  397  378  363  346  326  315  292  279  274  265\n",
      "  263  262  258  242  246  233  227  212  185  181  172  136  140  143\n",
      "  147  124  131  138  133  129  124  110  112  109  120  113  116  103\n",
      "  107   83   60   50   48   16  -98   -3   63   72   80   95  104  103\n",
      "  112  105  110  101  105   93   81   70   52   22 -174 -196  -77  -81\n",
      "   -1   31   49   52   66   68   78   78   89   85   83   90   87   77\n",
      "   85   87   71   78   69   20   18   10    0    0    0    0    0    0\n",
      "    0    0 -317 -396  -85  -47  -86  -32   35   49   55   67   78   79\n",
      "   84   92   86   91   91   94   98   97   97   90   92   84   86   93\n",
      "   88  101  100   96   87   71   59   54   44   38   20 -197    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -66  -28   16   17   25   31   53   66   55   61   64   74   79   81\n",
      "   83   89   90   86   84   85   90   92   94   85   90   84   90   87\n",
      "   90   77   87   82  100   93   93   97   89   89   70   78   60   52\n",
      "   58   30   27   40    8   -9   14   -4  -17   -2   -1  -32  -20  -23]\n",
      "[-249. -448.  319.  411.  457.  596.  679.  704.  662.  625.  602.  560.\n",
      "  524.  499.  476.  458.  433.  414.  397.  378.  363.  346.  326.  315.\n",
      "  292.  279.  274.  265.  263.  262.  258.  242.  246.  233.  227.  212.\n",
      "  185.  181.  172.  136.  140.  143.  147.  124.  131.  138.  133.  129.\n",
      "  124.  110.  112.  109.  120.  113.  116.  103.  107.   83.   60.   50.\n",
      "   48.   16.  -98.   -3.   63.   72.   80.   95.  104.  103.  112.  105.\n",
      "  110.  101.  105.   93.   81.   70.   52.   22. -174. -196.  -77.  -81.\n",
      "   -1.   31.   49.   52.   66.   68.   78.   78.   89.   85.   83.   90.\n",
      "   87.   77.   85.   87.   71.   78.   69.   20.   18.   10.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -317. -396.  -85.  -47.  -86.  -32.\n",
      "   35.   49.   55.   67.   78.   79.   84.   92.   86.   91.   91.   94.\n",
      "   98.   97.   97.   90.   92.   84.   86.   93.   88.  101.  100.   96.\n",
      "   87.   71.   59.   54.   44.   38.   20. -197.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -66.  -28.   16.   17.   25.   31.   53.   66.   55.   61.   64.   74.\n",
      "   79.   81.   83.   89.   90.   86.   84.   85.   90.   92.   94.   85.\n",
      "   90.   84.   90.   87.   90.   77.   87.   82.  100.   93.   93.   97.\n",
      "   89.   89.   70.   78.   60.   52.   58.   30.   27.   40.    8.   -9.\n",
      "   14.   -4.  -17.   -2.   -1.  -32.  -20.  -23.]\n"
     ]
    }
   ],
   "source": [
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(526), np.int64(187)), (np.int64(537), np.int64(71)), (np.int64(496), np.int64(222)), (np.int64(1200), np.int64(102)), (np.int64(1178), np.int64(413))]\n",
      "[(np.int64(174), np.int64(66)), (np.int64(382), np.int64(580)), (np.int64(1202), np.int64(171)), (np.int64(469), np.int64(254)), (np.int64(267), np.int64(228))]\n",
      "[(np.int64(526), np.int64(187)), (np.int64(537), np.int64(71)), (np.int64(496), np.int64(222)), (np.int64(1200), np.int64(102)), (np.int64(1178), np.int64(413)), (np.int64(174), np.int64(66)), (np.int64(382), np.int64(580)), (np.int64(1202), np.int64(171)), (np.int64(469), np.int64(254)), (np.int64(267), np.int64(228))]\n",
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "print(indices_0)\n",
    "print(indices_1)\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "print(indices)\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  40\n",
      "hasil augmentasi 1 shape: (40, 9, 9, 224)\n",
      "label augmentai 1 shape: (40,)\n",
      "hasil augmentasi 2 shape: (40, 9, 9, 224)\n",
      "label augmentasi 2 shape: (40,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts1):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts2):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "print(label_augment1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi gabungan untuk training: (80, 9, 9, 224)\n",
      "label augmentasi gabungan: (80,)\n",
      "Element 0 occurs 40 times.\n",
      "Element 1 occurs 40 times.\n"
     ]
    }
   ],
   "source": [
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[ 0.0056, -0.0212,  0.0257,  ...,  0.0472, -0.0012, -0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0055, -0.0213,  0.0255,  ...,  0.0473, -0.0011, -0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0084, 0.0000, 0.0148,  ..., 0.0000, 0.0000, 0.0961]])\n",
      "tensor([[0.0071, 0.0000, 0.0151,  ..., 0.0000, 0.0000, 0.0964]])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 40\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "gpu = 0\n",
    "\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(80, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([80, 224, 9, 9])\n",
      "bacth size: torch.Size([40, 224, 9, 9])\n",
      "length batch: 40\n",
      "torch.Size([9, 9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "            img2 = self.transform(img)  # Second augmentation\n",
    "        \n",
    "            return img1, img2  # Return both augmented versions\n",
    "        \n",
    "        return img, img  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "preloaded_images = data_augment  # Example tensor with 100 images\n",
    "X_train = torch.tensor(preloaded_images)\n",
    "X_train = X_train.to(torch.float32)\n",
    "X_train = X_train.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example normalization\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1, batch2 = next(iter(train_loader))\n",
    "\n",
    "print(f\"bacth size: {batch1.size()}\")\n",
    "print(f\"length batch: {len(batch1)}\")  # Should print 2 (Two transformed views per image)\n",
    "print(f\"{batch1[0][0].shape}\")  # Should print torch.Size([9, 9, 224]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images1, images2) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input1 = images1.to(device, non_blocking=True)\n",
    "        input2 = images2.to(device, non_blocking=True)\n",
    "\n",
    "        p1, p2, z1, z2 = model(x1=input1, x2=input2) \n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), input1.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "    # Return average training loss for early stopping\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/2]\tTime  4.421 ( 4.421)\tData  0.039 ( 0.039)\tLoss 0.0008 (0.0008)\n",
      "Epoch 1: Average Training Loss: 0.000688\n",
      "✅ New best model saved with loss 0.000688\n",
      "Epoch: [1][0/2]\tTime  0.184 ( 0.184)\tData  0.032 ( 0.032)\tLoss -0.0002 (-0.0002)\n",
      "Epoch 2: Average Training Loss: 0.000343\n",
      "✅ New best model saved with loss 0.000343\n",
      "Epoch: [2][0/2]\tTime  0.216 ( 0.216)\tData  0.062 ( 0.062)\tLoss -0.0015 (-0.0015)\n",
      "Epoch 3: Average Training Loss: -0.000466\n",
      "✅ New best model saved with loss -0.000466\n",
      "Epoch: [3][0/2]\tTime  0.195 ( 0.195)\tData  0.058 ( 0.058)\tLoss 0.0018 (0.0018)\n",
      "Epoch 4: Average Training Loss: 0.001910\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [4][0/2]\tTime  0.412 ( 0.412)\tData  0.032 ( 0.032)\tLoss -0.0013 (-0.0013)\n",
      "Epoch 5: Average Training Loss: -0.001236\n",
      "✅ New best model saved with loss -0.001236\n",
      "Epoch: [5][0/2]\tTime  0.200 ( 0.200)\tData  0.069 ( 0.069)\tLoss -0.0006 (-0.0006)\n",
      "Epoch 6: Average Training Loss: -0.001313\n",
      "✅ New best model saved with loss -0.001313\n",
      "Epoch: [6][0/2]\tTime  0.158 ( 0.158)\tData  0.018 ( 0.018)\tLoss -0.0016 (-0.0016)\n",
      "Epoch 7: Average Training Loss: -0.000096\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [7][0/2]\tTime  0.366 ( 0.366)\tData  0.023 ( 0.023)\tLoss -0.0031 (-0.0031)\n",
      "Epoch 8: Average Training Loss: -0.002888\n",
      "✅ New best model saved with loss -0.002888\n",
      "Epoch: [8][0/2]\tTime  0.159 ( 0.159)\tData  0.016 ( 0.016)\tLoss -0.0029 (-0.0029)\n",
      "Epoch 9: Average Training Loss: -0.003704\n",
      "✅ New best model saved with loss -0.003704\n",
      "Epoch: [9][0/2]\tTime  0.189 ( 0.189)\tData  0.043 ( 0.043)\tLoss 0.0006 (0.0006)\n",
      "Epoch 10: Average Training Loss: -0.002333\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [10][0/2]\tTime  0.369 ( 0.369)\tData  0.014 ( 0.014)\tLoss -0.0003 (-0.0003)\n",
      "Epoch 11: Average Training Loss: 0.000584\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [11][0/2]\tTime  0.362 ( 0.362)\tData  0.018 ( 0.018)\tLoss 0.0005 (0.0005)\n",
      "Epoch 12: Average Training Loss: -0.000219\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [12][0/2]\tTime  0.364 ( 0.364)\tData  0.018 ( 0.018)\tLoss -0.0039 (-0.0039)\n",
      "Epoch 13: Average Training Loss: -0.000557\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [13][0/2]\tTime  0.364 ( 0.364)\tData  0.008 ( 0.008)\tLoss 0.0018 (0.0018)\n",
      "Epoch 14: Average Training Loss: 0.000982\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [14][0/2]\tTime  0.340 ( 0.340)\tData  0.037 ( 0.037)\tLoss -0.0025 (-0.0025)\n",
      "Epoch 15: Average Training Loss: 0.000228\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [15][0/2]\tTime  0.381 ( 0.381)\tData  0.027 ( 0.027)\tLoss 0.0001 (0.0001)\n",
      "Epoch 16: Average Training Loss: 0.000906\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [16][0/2]\tTime  0.364 ( 0.364)\tData  0.021 ( 0.021)\tLoss 0.0011 (0.0011)\n",
      "Epoch 17: Average Training Loss: 0.001851\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [17][0/2]\tTime  0.359 ( 0.359)\tData  0.008 ( 0.008)\tLoss -0.0017 (-0.0017)\n",
      "Epoch 18: Average Training Loss: -0.000379\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [18][0/2]\tTime  0.368 ( 0.368)\tData  0.016 ( 0.016)\tLoss -0.0026 (-0.0026)\n",
      "Epoch 19: Average Training Loss: -0.002725\n",
      "❌ No improvement. Patience: 10/50\n",
      "Epoch: [19][0/2]\tTime  0.361 ( 0.361)\tData  0.011 ( 0.011)\tLoss -0.0012 (-0.0012)\n",
      "Epoch 20: Average Training Loss: -0.003158\n",
      "❌ No improvement. Patience: 11/50\n",
      "Epoch: [20][0/2]\tTime  0.351 ( 0.351)\tData  0.021 ( 0.021)\tLoss -0.0010 (-0.0010)\n",
      "Epoch 21: Average Training Loss: 0.000101\n",
      "❌ No improvement. Patience: 12/50\n",
      "Epoch: [21][0/2]\tTime  0.356 ( 0.356)\tData  0.015 ( 0.015)\tLoss 0.0033 (0.0033)\n",
      "Epoch 22: Average Training Loss: 0.001953\n",
      "❌ No improvement. Patience: 13/50\n",
      "Epoch: [22][0/2]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0039 (-0.0039)\n",
      "Epoch 23: Average Training Loss: -0.003375\n",
      "❌ No improvement. Patience: 14/50\n",
      "Epoch: [23][0/2]\tTime  0.356 ( 0.356)\tData  0.014 ( 0.014)\tLoss 0.0008 (0.0008)\n",
      "Epoch 24: Average Training Loss: -0.000501\n",
      "❌ No improvement. Patience: 15/50\n",
      "Epoch: [24][0/2]\tTime  0.351 ( 0.351)\tData  0.016 ( 0.016)\tLoss -0.0030 (-0.0030)\n",
      "Epoch 25: Average Training Loss: -0.002600\n",
      "❌ No improvement. Patience: 16/50\n",
      "Epoch: [25][0/2]\tTime  0.358 ( 0.358)\tData  0.017 ( 0.017)\tLoss 0.0005 (0.0005)\n",
      "Epoch 26: Average Training Loss: -0.000535\n",
      "❌ No improvement. Patience: 17/50\n",
      "Epoch: [26][0/2]\tTime  0.468 ( 0.468)\tData  0.011 ( 0.011)\tLoss -0.0012 (-0.0012)\n",
      "Epoch 27: Average Training Loss: -0.002236\n",
      "❌ No improvement. Patience: 18/50\n",
      "Epoch: [27][0/2]\tTime  0.361 ( 0.361)\tData  0.015 ( 0.015)\tLoss -0.0039 (-0.0039)\n",
      "Epoch 28: Average Training Loss: -0.002810\n",
      "❌ No improvement. Patience: 19/50\n",
      "Epoch: [28][0/2]\tTime  0.365 ( 0.365)\tData  0.014 ( 0.014)\tLoss 0.0000 (0.0000)\n",
      "Epoch 29: Average Training Loss: -0.001587\n",
      "❌ No improvement. Patience: 20/50\n",
      "Epoch: [29][0/2]\tTime  0.351 ( 0.351)\tData  0.014 ( 0.014)\tLoss 0.0005 (0.0005)\n",
      "Epoch 30: Average Training Loss: -0.000940\n",
      "❌ No improvement. Patience: 21/50\n",
      "Epoch: [30][0/2]\tTime  0.358 ( 0.358)\tData  0.019 ( 0.019)\tLoss -0.0040 (-0.0040)\n",
      "Epoch 31: Average Training Loss: -0.003032\n",
      "❌ No improvement. Patience: 22/50\n",
      "Epoch: [31][0/2]\tTime  0.471 ( 0.471)\tData  0.019 ( 0.019)\tLoss -0.0022 (-0.0022)\n",
      "Epoch 32: Average Training Loss: -0.003013\n",
      "❌ No improvement. Patience: 23/50\n",
      "Epoch: [32][0/2]\tTime  0.358 ( 0.358)\tData  0.014 ( 0.014)\tLoss -0.0076 (-0.0076)\n",
      "Epoch 33: Average Training Loss: -0.005096\n",
      "✅ New best model saved with loss -0.005096\n",
      "Epoch: [33][0/2]\tTime  0.195 ( 0.195)\tData  0.025 ( 0.025)\tLoss -0.0053 (-0.0053)\n",
      "Epoch 34: Average Training Loss: -0.006564\n",
      "✅ New best model saved with loss -0.006564\n",
      "Epoch: [34][0/2]\tTime  0.160 ( 0.160)\tData  0.013 ( 0.013)\tLoss -0.0060 (-0.0060)\n",
      "Epoch 35: Average Training Loss: -0.004347\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [35][0/2]\tTime  0.360 ( 0.360)\tData  0.020 ( 0.020)\tLoss -0.0104 (-0.0104)\n",
      "Epoch 36: Average Training Loss: -0.008928\n",
      "✅ New best model saved with loss -0.008928\n",
      "Epoch: [36][0/2]\tTime  0.174 ( 0.174)\tData  0.014 ( 0.014)\tLoss -0.0075 (-0.0075)\n",
      "Epoch 37: Average Training Loss: -0.007094\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [37][0/2]\tTime  0.360 ( 0.360)\tData  0.015 ( 0.015)\tLoss -0.0055 (-0.0055)\n",
      "Epoch 38: Average Training Loss: -0.005013\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [38][0/2]\tTime  0.359 ( 0.359)\tData  0.015 ( 0.015)\tLoss -0.0072 (-0.0072)\n",
      "Epoch 39: Average Training Loss: -0.006841\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [39][0/2]\tTime  0.360 ( 0.360)\tData  0.015 ( 0.015)\tLoss -0.0063 (-0.0063)\n",
      "Epoch 40: Average Training Loss: -0.008951\n",
      "✅ New best model saved with loss -0.008951\n",
      "Epoch: [40][0/2]\tTime  0.148 ( 0.148)\tData  0.021 ( 0.021)\tLoss -0.0068 (-0.0068)\n",
      "Epoch 41: Average Training Loss: -0.006195\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [41][0/2]\tTime  0.359 ( 0.359)\tData  0.018 ( 0.018)\tLoss -0.0050 (-0.0050)\n",
      "Epoch 42: Average Training Loss: -0.006553\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [42][0/2]\tTime  0.488 ( 0.488)\tData  0.015 ( 0.015)\tLoss -0.0040 (-0.0040)\n",
      "Epoch 43: Average Training Loss: -0.006854\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [43][0/2]\tTime  0.358 ( 0.358)\tData  0.014 ( 0.014)\tLoss -0.0053 (-0.0053)\n",
      "Epoch 44: Average Training Loss: -0.006058\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [44][0/2]\tTime  0.362 ( 0.362)\tData  0.018 ( 0.018)\tLoss -0.0061 (-0.0061)\n",
      "Epoch 45: Average Training Loss: -0.008601\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [45][0/2]\tTime  0.362 ( 0.362)\tData  0.016 ( 0.016)\tLoss -0.0089 (-0.0089)\n",
      "Epoch 46: Average Training Loss: -0.008865\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [46][0/2]\tTime  0.353 ( 0.353)\tData  0.016 ( 0.016)\tLoss -0.0111 (-0.0111)\n",
      "Epoch 47: Average Training Loss: -0.010518\n",
      "✅ New best model saved with loss -0.010518\n",
      "Epoch: [47][0/2]\tTime  0.178 ( 0.178)\tData  0.014 ( 0.014)\tLoss -0.0081 (-0.0081)\n",
      "Epoch 48: Average Training Loss: -0.007483\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [48][0/2]\tTime  0.365 ( 0.365)\tData  0.018 ( 0.018)\tLoss -0.0086 (-0.0086)\n",
      "Epoch 49: Average Training Loss: -0.009951\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [49][0/2]\tTime  0.356 ( 0.356)\tData  0.012 ( 0.012)\tLoss -0.0114 (-0.0114)\n",
      "Epoch 50: Average Training Loss: -0.011919\n",
      "✅ New best model saved with loss -0.011919\n",
      "Epoch: [50][0/2]\tTime  0.189 ( 0.189)\tData  0.014 ( 0.014)\tLoss -0.0151 (-0.0151)\n",
      "Epoch 51: Average Training Loss: -0.012796\n",
      "✅ New best model saved with loss -0.012796\n",
      "Epoch: [51][0/2]\tTime  0.192 ( 0.192)\tData  0.016 ( 0.016)\tLoss -0.0151 (-0.0151)\n",
      "Epoch 52: Average Training Loss: -0.013395\n",
      "✅ New best model saved with loss -0.013395\n",
      "Epoch: [52][0/2]\tTime  0.189 ( 0.189)\tData  0.014 ( 0.014)\tLoss -0.0143 (-0.0143)\n",
      "Epoch 53: Average Training Loss: -0.015114\n",
      "✅ New best model saved with loss -0.015114\n",
      "Epoch: [53][0/2]\tTime  0.204 ( 0.204)\tData  0.021 ( 0.021)\tLoss -0.0124 (-0.0124)\n",
      "Epoch 54: Average Training Loss: -0.013832\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [54][0/2]\tTime  0.367 ( 0.367)\tData  0.015 ( 0.015)\tLoss -0.0132 (-0.0132)\n",
      "Epoch 55: Average Training Loss: -0.012313\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [55][0/2]\tTime  0.358 ( 0.358)\tData  0.053 ( 0.053)\tLoss -0.0154 (-0.0154)\n",
      "Epoch 56: Average Training Loss: -0.015623\n",
      "✅ New best model saved with loss -0.015623\n",
      "Epoch: [56][0/2]\tTime  0.191 ( 0.191)\tData  0.015 ( 0.015)\tLoss -0.0174 (-0.0174)\n",
      "Epoch 57: Average Training Loss: -0.017565\n",
      "✅ New best model saved with loss -0.017565\n",
      "Epoch: [57][0/2]\tTime  0.245 ( 0.245)\tData  0.032 ( 0.032)\tLoss -0.0153 (-0.0153)\n",
      "Epoch 58: Average Training Loss: -0.014040\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [58][0/2]\tTime  0.452 ( 0.452)\tData  0.014 ( 0.014)\tLoss -0.0178 (-0.0178)\n",
      "Epoch 59: Average Training Loss: -0.017453\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [59][0/2]\tTime  0.342 ( 0.342)\tData  0.013 ( 0.013)\tLoss -0.0151 (-0.0151)\n",
      "Epoch 60: Average Training Loss: -0.015933\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [60][0/2]\tTime  0.350 ( 0.350)\tData  0.015 ( 0.015)\tLoss -0.0206 (-0.0206)\n",
      "Epoch 61: Average Training Loss: -0.018690\n",
      "✅ New best model saved with loss -0.018690\n",
      "Epoch: [61][0/2]\tTime  0.380 ( 0.380)\tData  0.050 ( 0.050)\tLoss -0.0204 (-0.0204)\n",
      "Epoch 62: Average Training Loss: -0.018990\n",
      "✅ New best model saved with loss -0.018990\n",
      "Epoch: [62][0/2]\tTime  0.345 ( 0.345)\tData  0.016 ( 0.016)\tLoss -0.0179 (-0.0179)\n",
      "Epoch 63: Average Training Loss: -0.021293\n",
      "✅ New best model saved with loss -0.021293\n",
      "Epoch: [63][0/2]\tTime  0.265 ( 0.265)\tData  0.045 ( 0.045)\tLoss -0.0205 (-0.0205)\n",
      "Epoch 64: Average Training Loss: -0.019293\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [64][0/2]\tTime  0.353 ( 0.353)\tData  0.010 ( 0.010)\tLoss -0.0219 (-0.0219)\n",
      "Epoch 65: Average Training Loss: -0.021271\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [65][0/2]\tTime  0.353 ( 0.353)\tData  0.013 ( 0.013)\tLoss -0.0207 (-0.0207)\n",
      "Epoch 66: Average Training Loss: -0.020743\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [66][0/2]\tTime  0.339 ( 0.339)\tData  0.023 ( 0.023)\tLoss -0.0261 (-0.0261)\n",
      "Epoch 67: Average Training Loss: -0.025425\n",
      "✅ New best model saved with loss -0.025425\n",
      "Epoch: [67][0/2]\tTime  0.145 ( 0.145)\tData  0.014 ( 0.014)\tLoss -0.0227 (-0.0227)\n",
      "Epoch 68: Average Training Loss: -0.021102\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [68][0/2]\tTime  0.350 ( 0.350)\tData  0.020 ( 0.020)\tLoss -0.0217 (-0.0217)\n",
      "Epoch 69: Average Training Loss: -0.024421\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [69][0/2]\tTime  0.358 ( 0.358)\tData  0.016 ( 0.016)\tLoss -0.0261 (-0.0261)\n",
      "Epoch 70: Average Training Loss: -0.024328\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [70][0/2]\tTime  0.344 ( 0.344)\tData  0.013 ( 0.013)\tLoss -0.0216 (-0.0216)\n",
      "Epoch 71: Average Training Loss: -0.023985\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [71][0/2]\tTime  0.337 ( 0.337)\tData  0.007 ( 0.007)\tLoss -0.0250 (-0.0250)\n",
      "Epoch 72: Average Training Loss: -0.024166\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [72][0/2]\tTime  0.348 ( 0.348)\tData  0.004 ( 0.004)\tLoss -0.0258 (-0.0258)\n",
      "Epoch 73: Average Training Loss: -0.026117\n",
      "✅ New best model saved with loss -0.026117\n",
      "Epoch: [73][0/2]\tTime  0.171 ( 0.171)\tData  0.010 ( 0.010)\tLoss -0.0294 (-0.0294)\n",
      "Epoch 74: Average Training Loss: -0.028401\n",
      "✅ New best model saved with loss -0.028401\n",
      "Epoch: [74][0/2]\tTime  0.350 ( 0.350)\tData  0.116 ( 0.116)\tLoss -0.0336 (-0.0336)\n",
      "Epoch 75: Average Training Loss: -0.033209\n",
      "✅ New best model saved with loss -0.033209\n",
      "Epoch: [75][0/2]\tTime  0.258 ( 0.258)\tData  0.021 ( 0.021)\tLoss -0.0274 (-0.0274)\n",
      "Epoch 76: Average Training Loss: -0.028634\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [76][0/2]\tTime  0.497 ( 0.497)\tData  0.059 ( 0.059)\tLoss -0.0312 (-0.0312)\n",
      "Epoch 77: Average Training Loss: -0.030680\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [77][0/2]\tTime  0.388 ( 0.388)\tData  0.040 ( 0.040)\tLoss -0.0307 (-0.0307)\n",
      "Epoch 78: Average Training Loss: -0.029237\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [78][0/2]\tTime  0.354 ( 0.354)\tData  0.021 ( 0.021)\tLoss -0.0286 (-0.0286)\n",
      "Epoch 79: Average Training Loss: -0.029365\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [79][0/2]\tTime  0.389 ( 0.389)\tData  0.016 ( 0.016)\tLoss -0.0277 (-0.0277)\n",
      "Epoch 80: Average Training Loss: -0.030092\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [80][0/2]\tTime  0.379 ( 0.379)\tData  0.014 ( 0.014)\tLoss -0.0305 (-0.0305)\n",
      "Epoch 81: Average Training Loss: -0.032410\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [81][0/2]\tTime  0.326 ( 0.326)\tData  0.025 ( 0.025)\tLoss -0.0347 (-0.0347)\n",
      "Epoch 82: Average Training Loss: -0.035798\n",
      "✅ New best model saved with loss -0.035798\n",
      "Epoch: [82][0/2]\tTime  0.397 ( 0.397)\tData  0.061 ( 0.061)\tLoss -0.0323 (-0.0323)\n",
      "Epoch 83: Average Training Loss: -0.032812\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [83][0/2]\tTime  0.349 ( 0.349)\tData  0.013 ( 0.013)\tLoss -0.0363 (-0.0363)\n",
      "Epoch 84: Average Training Loss: -0.037137\n",
      "✅ New best model saved with loss -0.037137\n",
      "Epoch: [84][0/2]\tTime  0.196 ( 0.196)\tData  0.057 ( 0.057)\tLoss -0.0400 (-0.0400)\n",
      "Epoch 85: Average Training Loss: -0.038851\n",
      "✅ New best model saved with loss -0.038851\n",
      "Epoch: [85][0/2]\tTime  0.193 ( 0.193)\tData  0.015 ( 0.015)\tLoss -0.0380 (-0.0380)\n",
      "Epoch 86: Average Training Loss: -0.037712\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [86][0/2]\tTime  0.343 ( 0.343)\tData  0.010 ( 0.010)\tLoss -0.0410 (-0.0410)\n",
      "Epoch 87: Average Training Loss: -0.038620\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [87][0/2]\tTime  0.345 ( 0.345)\tData  0.026 ( 0.026)\tLoss -0.0405 (-0.0405)\n",
      "Epoch 88: Average Training Loss: -0.040314\n",
      "✅ New best model saved with loss -0.040314\n",
      "Epoch: [88][0/2]\tTime  0.241 ( 0.241)\tData  0.024 ( 0.024)\tLoss -0.0394 (-0.0394)\n",
      "Epoch 89: Average Training Loss: -0.040004\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [89][0/2]\tTime  0.349 ( 0.349)\tData  0.024 ( 0.024)\tLoss -0.0407 (-0.0407)\n",
      "Epoch 90: Average Training Loss: -0.040945\n",
      "✅ New best model saved with loss -0.040945\n",
      "Epoch: [90][0/2]\tTime  0.362 ( 0.362)\tData  0.036 ( 0.036)\tLoss -0.0406 (-0.0406)\n",
      "Epoch 91: Average Training Loss: -0.040477\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [91][0/2]\tTime  0.352 ( 0.352)\tData  0.013 ( 0.013)\tLoss -0.0405 (-0.0405)\n",
      "Epoch 92: Average Training Loss: -0.041042\n",
      "✅ New best model saved with loss -0.041042\n",
      "Epoch: [92][0/2]\tTime  0.348 ( 0.348)\tData  0.014 ( 0.014)\tLoss -0.0418 (-0.0418)\n",
      "Epoch 93: Average Training Loss: -0.041912\n",
      "✅ New best model saved with loss -0.041912\n",
      "Epoch: [93][0/2]\tTime  0.278 ( 0.278)\tData  0.060 ( 0.060)\tLoss -0.0397 (-0.0397)\n",
      "Epoch 94: Average Training Loss: -0.043792\n",
      "✅ New best model saved with loss -0.043792\n",
      "Epoch: [94][0/2]\tTime  0.209 ( 0.209)\tData  0.017 ( 0.017)\tLoss -0.0453 (-0.0453)\n",
      "Epoch 95: Average Training Loss: -0.044564\n",
      "✅ New best model saved with loss -0.044564\n",
      "Epoch: [95][0/2]\tTime  0.167 ( 0.167)\tData  0.012 ( 0.012)\tLoss -0.0461 (-0.0461)\n",
      "Epoch 96: Average Training Loss: -0.046291\n",
      "✅ New best model saved with loss -0.046291\n",
      "Epoch: [96][0/2]\tTime  0.230 ( 0.230)\tData  0.060 ( 0.060)\tLoss -0.0453 (-0.0453)\n",
      "Epoch 97: Average Training Loss: -0.045616\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [97][0/2]\tTime  0.355 ( 0.355)\tData  0.025 ( 0.025)\tLoss -0.0418 (-0.0418)\n",
      "Epoch 98: Average Training Loss: -0.041879\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [98][0/2]\tTime  0.367 ( 0.367)\tData  0.020 ( 0.020)\tLoss -0.0543 (-0.0543)\n",
      "Epoch 99: Average Training Loss: -0.050765\n",
      "✅ New best model saved with loss -0.050765\n",
      "Epoch: [99][0/2]\tTime  0.169 ( 0.169)\tData  0.044 ( 0.044)\tLoss -0.0510 (-0.0510)\n",
      "Epoch 100: Average Training Loss: -0.049106\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [100][0/2]\tTime  0.367 ( 0.367)\tData  0.014 ( 0.014)\tLoss -0.0504 (-0.0504)\n",
      "Epoch 101: Average Training Loss: -0.048383\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [101][0/2]\tTime  0.356 ( 0.356)\tData  0.008 ( 0.008)\tLoss -0.0514 (-0.0514)\n",
      "Epoch 102: Average Training Loss: -0.051804\n",
      "✅ New best model saved with loss -0.051804\n",
      "Epoch: [102][0/2]\tTime  0.192 ( 0.192)\tData  0.018 ( 0.018)\tLoss -0.0484 (-0.0484)\n",
      "Epoch 103: Average Training Loss: -0.049698\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [103][0/2]\tTime  0.359 ( 0.359)\tData  0.015 ( 0.015)\tLoss -0.0585 (-0.0585)\n",
      "Epoch 104: Average Training Loss: -0.057080\n",
      "✅ New best model saved with loss -0.057080\n",
      "Epoch: [104][0/2]\tTime  0.161 ( 0.161)\tData  0.018 ( 0.018)\tLoss -0.0511 (-0.0511)\n",
      "Epoch 105: Average Training Loss: -0.049231\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [105][0/2]\tTime  0.359 ( 0.359)\tData  0.022 ( 0.022)\tLoss -0.0560 (-0.0560)\n",
      "Epoch 106: Average Training Loss: -0.055128\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [106][0/2]\tTime  0.354 ( 0.354)\tData  0.017 ( 0.017)\tLoss -0.0541 (-0.0541)\n",
      "Epoch 107: Average Training Loss: -0.052059\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [107][0/2]\tTime  0.357 ( 0.357)\tData  0.013 ( 0.013)\tLoss -0.0487 (-0.0487)\n",
      "Epoch 108: Average Training Loss: -0.050764\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [108][0/2]\tTime  0.385 ( 0.385)\tData  0.015 ( 0.015)\tLoss -0.0513 (-0.0513)\n",
      "Epoch 109: Average Training Loss: -0.052846\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [109][0/2]\tTime  0.359 ( 0.359)\tData  0.016 ( 0.016)\tLoss -0.0600 (-0.0600)\n",
      "Epoch 110: Average Training Loss: -0.059239\n",
      "✅ New best model saved with loss -0.059239\n",
      "Epoch: [110][0/2]\tTime  0.178 ( 0.178)\tData  0.050 ( 0.050)\tLoss -0.0561 (-0.0561)\n",
      "Epoch 111: Average Training Loss: -0.057908\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [111][0/2]\tTime  0.414 ( 0.414)\tData  0.015 ( 0.015)\tLoss -0.0554 (-0.0554)\n",
      "Epoch 112: Average Training Loss: -0.057473\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [112][0/2]\tTime  0.374 ( 0.374)\tData  0.020 ( 0.020)\tLoss -0.0625 (-0.0625)\n",
      "Epoch 113: Average Training Loss: -0.059995\n",
      "✅ New best model saved with loss -0.059995\n",
      "Epoch: [113][0/2]\tTime  0.194 ( 0.194)\tData  0.058 ( 0.058)\tLoss -0.0574 (-0.0574)\n",
      "Epoch 114: Average Training Loss: -0.056907\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [114][0/2]\tTime  0.360 ( 0.360)\tData  0.012 ( 0.012)\tLoss -0.0610 (-0.0610)\n",
      "Epoch 115: Average Training Loss: -0.057920\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [115][0/2]\tTime  0.360 ( 0.360)\tData  0.016 ( 0.016)\tLoss -0.0576 (-0.0576)\n",
      "Epoch 116: Average Training Loss: -0.057570\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [116][0/2]\tTime  0.377 ( 0.377)\tData  0.018 ( 0.018)\tLoss -0.0602 (-0.0602)\n",
      "Epoch 117: Average Training Loss: -0.060062\n",
      "✅ New best model saved with loss -0.060062\n",
      "Epoch: [117][0/2]\tTime  0.236 ( 0.236)\tData  0.074 ( 0.074)\tLoss -0.0615 (-0.0615)\n",
      "Epoch 118: Average Training Loss: -0.061012\n",
      "✅ New best model saved with loss -0.061012\n",
      "Epoch: [118][0/2]\tTime  0.306 ( 0.306)\tData  0.044 ( 0.044)\tLoss -0.0606 (-0.0606)\n",
      "Epoch 119: Average Training Loss: -0.058526\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [119][0/2]\tTime  0.358 ( 0.358)\tData  0.017 ( 0.017)\tLoss -0.0601 (-0.0601)\n",
      "Epoch 120: Average Training Loss: -0.062823\n",
      "✅ New best model saved with loss -0.062823\n",
      "Epoch: [120][0/2]\tTime  0.178 ( 0.178)\tData  0.017 ( 0.017)\tLoss -0.0657 (-0.0657)\n",
      "Epoch 121: Average Training Loss: -0.063628\n",
      "✅ New best model saved with loss -0.063628\n",
      "Epoch: [121][0/2]\tTime  0.191 ( 0.191)\tData  0.021 ( 0.021)\tLoss -0.0600 (-0.0600)\n",
      "Epoch 122: Average Training Loss: -0.060010\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [122][0/2]\tTime  0.363 ( 0.363)\tData  0.014 ( 0.014)\tLoss -0.0651 (-0.0651)\n",
      "Epoch 123: Average Training Loss: -0.065271\n",
      "✅ New best model saved with loss -0.065271\n",
      "Epoch: [123][0/2]\tTime  0.241 ( 0.241)\tData  0.021 ( 0.021)\tLoss -0.0616 (-0.0616)\n",
      "Epoch 124: Average Training Loss: -0.062958\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [124][0/2]\tTime  0.355 ( 0.355)\tData  0.018 ( 0.018)\tLoss -0.0581 (-0.0581)\n",
      "Epoch 125: Average Training Loss: -0.062372\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [125][0/2]\tTime  0.468 ( 0.468)\tData  0.014 ( 0.014)\tLoss -0.0682 (-0.0682)\n",
      "Epoch 126: Average Training Loss: -0.064731\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [126][0/2]\tTime  0.361 ( 0.361)\tData  0.018 ( 0.018)\tLoss -0.0615 (-0.0615)\n",
      "Epoch 127: Average Training Loss: -0.064662\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [127][0/2]\tTime  0.359 ( 0.359)\tData  0.015 ( 0.015)\tLoss -0.0652 (-0.0652)\n",
      "Epoch 128: Average Training Loss: -0.065435\n",
      "✅ New best model saved with loss -0.065435\n",
      "Epoch: [128][0/2]\tTime  0.159 ( 0.159)\tData  0.015 ( 0.015)\tLoss -0.0636 (-0.0636)\n",
      "Epoch 129: Average Training Loss: -0.060764\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [129][0/2]\tTime  0.372 ( 0.372)\tData  0.021 ( 0.021)\tLoss -0.0672 (-0.0672)\n",
      "Epoch 130: Average Training Loss: -0.066941\n",
      "✅ New best model saved with loss -0.066941\n",
      "Epoch: [130][0/2]\tTime  0.197 ( 0.197)\tData  0.018 ( 0.018)\tLoss -0.0706 (-0.0706)\n",
      "Epoch 131: Average Training Loss: -0.069277\n",
      "✅ New best model saved with loss -0.069277\n",
      "Epoch: [131][0/2]\tTime  0.182 ( 0.182)\tData  0.020 ( 0.020)\tLoss -0.0688 (-0.0688)\n",
      "Epoch 132: Average Training Loss: -0.067957\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [132][0/2]\tTime  0.474 ( 0.474)\tData  0.012 ( 0.012)\tLoss -0.0708 (-0.0708)\n",
      "Epoch 133: Average Training Loss: -0.073091\n",
      "✅ New best model saved with loss -0.073091\n",
      "Epoch: [133][0/2]\tTime  0.171 ( 0.171)\tData  0.017 ( 0.017)\tLoss -0.0672 (-0.0672)\n",
      "Epoch 134: Average Training Loss: -0.069497\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [134][0/2]\tTime  0.358 ( 0.358)\tData  0.013 ( 0.013)\tLoss -0.0701 (-0.0701)\n",
      "Epoch 135: Average Training Loss: -0.069735\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [135][0/2]\tTime  0.357 ( 0.357)\tData  0.006 ( 0.006)\tLoss -0.0736 (-0.0736)\n",
      "Epoch 136: Average Training Loss: -0.071104\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [136][0/2]\tTime  0.355 ( 0.355)\tData  0.020 ( 0.020)\tLoss -0.0699 (-0.0699)\n",
      "Epoch 137: Average Training Loss: -0.073292\n",
      "✅ New best model saved with loss -0.073292\n",
      "Epoch: [137][0/2]\tTime  0.164 ( 0.164)\tData  0.016 ( 0.016)\tLoss -0.0709 (-0.0709)\n",
      "Epoch 138: Average Training Loss: -0.071485\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [138][0/2]\tTime  0.368 ( 0.368)\tData  0.015 ( 0.015)\tLoss -0.0745 (-0.0745)\n",
      "Epoch 139: Average Training Loss: -0.072479\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [139][0/2]\tTime  0.356 ( 0.356)\tData  0.014 ( 0.014)\tLoss -0.0696 (-0.0696)\n",
      "Epoch 140: Average Training Loss: -0.067664\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [140][0/2]\tTime  0.351 ( 0.351)\tData  0.021 ( 0.021)\tLoss -0.0695 (-0.0695)\n",
      "Epoch 141: Average Training Loss: -0.072529\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [141][0/2]\tTime  0.353 ( 0.353)\tData  0.016 ( 0.016)\tLoss -0.0752 (-0.0752)\n",
      "Epoch 142: Average Training Loss: -0.074332\n",
      "✅ New best model saved with loss -0.074332\n",
      "Epoch: [142][0/2]\tTime  0.255 ( 0.255)\tData  0.041 ( 0.041)\tLoss -0.0738 (-0.0738)\n",
      "Epoch 143: Average Training Loss: -0.075067\n",
      "✅ New best model saved with loss -0.075067\n",
      "Epoch: [143][0/2]\tTime  0.263 ( 0.263)\tData  0.047 ( 0.047)\tLoss -0.0708 (-0.0708)\n",
      "Epoch 144: Average Training Loss: -0.071488\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [144][0/2]\tTime  0.360 ( 0.360)\tData  0.016 ( 0.016)\tLoss -0.0712 (-0.0712)\n",
      "Epoch 145: Average Training Loss: -0.068941\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [145][0/2]\tTime  0.360 ( 0.360)\tData  0.019 ( 0.019)\tLoss -0.0661 (-0.0661)\n",
      "Epoch 146: Average Training Loss: -0.066157\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [146][0/2]\tTime  0.357 ( 0.357)\tData  0.014 ( 0.014)\tLoss -0.0736 (-0.0736)\n",
      "Epoch 147: Average Training Loss: -0.074253\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [147][0/2]\tTime  0.363 ( 0.363)\tData  0.018 ( 0.018)\tLoss -0.0735 (-0.0735)\n",
      "Epoch 148: Average Training Loss: -0.076183\n",
      "✅ New best model saved with loss -0.076183\n",
      "Epoch: [148][0/2]\tTime  0.198 ( 0.198)\tData  0.017 ( 0.017)\tLoss -0.0801 (-0.0801)\n",
      "Epoch 149: Average Training Loss: -0.076468\n",
      "✅ New best model saved with loss -0.076468\n",
      "Epoch: [149][0/2]\tTime  0.176 ( 0.176)\tData  0.014 ( 0.014)\tLoss -0.0750 (-0.0750)\n",
      "Epoch 150: Average Training Loss: -0.075241\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [150][0/2]\tTime  0.356 ( 0.356)\tData  0.017 ( 0.017)\tLoss -0.0788 (-0.0788)\n",
      "Epoch 151: Average Training Loss: -0.075133\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [151][0/2]\tTime  0.368 ( 0.368)\tData  0.016 ( 0.016)\tLoss -0.0735 (-0.0735)\n",
      "Epoch 152: Average Training Loss: -0.073705\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [152][0/2]\tTime  0.371 ( 0.371)\tData  0.015 ( 0.015)\tLoss -0.0776 (-0.0776)\n",
      "Epoch 153: Average Training Loss: -0.076398\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [153][0/2]\tTime  0.360 ( 0.360)\tData  0.016 ( 0.016)\tLoss -0.0780 (-0.0780)\n",
      "Epoch 154: Average Training Loss: -0.077010\n",
      "✅ New best model saved with loss -0.077010\n",
      "Epoch: [154][0/2]\tTime  0.170 ( 0.170)\tData  0.014 ( 0.014)\tLoss -0.0774 (-0.0774)\n",
      "Epoch 155: Average Training Loss: -0.074513\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [155][0/2]\tTime  0.359 ( 0.359)\tData  0.017 ( 0.017)\tLoss -0.0732 (-0.0732)\n",
      "Epoch 156: Average Training Loss: -0.074068\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [156][0/2]\tTime  0.341 ( 0.341)\tData  0.024 ( 0.024)\tLoss -0.0746 (-0.0746)\n",
      "Epoch 157: Average Training Loss: -0.076319\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [157][0/2]\tTime  0.359 ( 0.359)\tData  0.015 ( 0.015)\tLoss -0.0777 (-0.0777)\n",
      "Epoch 158: Average Training Loss: -0.078248\n",
      "✅ New best model saved with loss -0.078248\n",
      "Epoch: [158][0/2]\tTime  0.288 ( 0.288)\tData  0.020 ( 0.020)\tLoss -0.0800 (-0.0800)\n",
      "Epoch 159: Average Training Loss: -0.077446\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [159][0/2]\tTime  0.359 ( 0.359)\tData  0.017 ( 0.017)\tLoss -0.0758 (-0.0758)\n",
      "Epoch 160: Average Training Loss: -0.076615\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [160][0/2]\tTime  0.349 ( 0.349)\tData  0.014 ( 0.014)\tLoss -0.0843 (-0.0843)\n",
      "Epoch 161: Average Training Loss: -0.082472\n",
      "✅ New best model saved with loss -0.082472\n",
      "Epoch: [161][0/2]\tTime  0.200 ( 0.200)\tData  0.014 ( 0.014)\tLoss -0.0776 (-0.0776)\n",
      "Epoch 162: Average Training Loss: -0.077883\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [162][0/2]\tTime  0.359 ( 0.359)\tData  0.013 ( 0.013)\tLoss -0.0773 (-0.0773)\n",
      "Epoch 163: Average Training Loss: -0.076751\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [163][0/2]\tTime  0.361 ( 0.361)\tData  0.011 ( 0.011)\tLoss -0.0795 (-0.0795)\n",
      "Epoch 164: Average Training Loss: -0.079974\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [164][0/2]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0694 (-0.0694)\n",
      "Epoch 165: Average Training Loss: -0.071340\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [165][0/2]\tTime  0.353 ( 0.353)\tData  0.017 ( 0.017)\tLoss -0.0782 (-0.0782)\n",
      "Epoch 166: Average Training Loss: -0.080126\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [166][0/2]\tTime  0.355 ( 0.355)\tData  0.012 ( 0.012)\tLoss -0.0809 (-0.0809)\n",
      "Epoch 167: Average Training Loss: -0.080746\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [167][0/2]\tTime  0.356 ( 0.356)\tData  0.014 ( 0.014)\tLoss -0.0769 (-0.0769)\n",
      "Epoch 168: Average Training Loss: -0.076765\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [168][0/2]\tTime  0.358 ( 0.358)\tData  0.013 ( 0.013)\tLoss -0.0763 (-0.0763)\n",
      "Epoch 169: Average Training Loss: -0.077586\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [169][0/2]\tTime  0.355 ( 0.355)\tData  0.017 ( 0.017)\tLoss -0.0786 (-0.0786)\n",
      "Epoch 170: Average Training Loss: -0.078253\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [170][0/2]\tTime  0.361 ( 0.361)\tData  0.022 ( 0.022)\tLoss -0.0837 (-0.0837)\n",
      "Epoch 171: Average Training Loss: -0.082057\n",
      "❌ No improvement. Patience: 10/50\n",
      "Epoch: [171][0/2]\tTime  0.359 ( 0.359)\tData  0.017 ( 0.017)\tLoss -0.0763 (-0.0763)\n",
      "Epoch 172: Average Training Loss: -0.075122\n",
      "❌ No improvement. Patience: 11/50\n",
      "Epoch: [172][0/2]\tTime  0.362 ( 0.362)\tData  0.018 ( 0.018)\tLoss -0.0771 (-0.0771)\n",
      "Epoch 173: Average Training Loss: -0.081188\n",
      "❌ No improvement. Patience: 12/50\n",
      "Epoch: [173][0/2]\tTime  0.357 ( 0.357)\tData  0.015 ( 0.015)\tLoss -0.0784 (-0.0784)\n",
      "Epoch 174: Average Training Loss: -0.075863\n",
      "❌ No improvement. Patience: 13/50\n",
      "Epoch: [174][0/2]\tTime  0.357 ( 0.357)\tData  0.013 ( 0.013)\tLoss -0.0852 (-0.0852)\n",
      "Epoch 175: Average Training Loss: -0.080778\n",
      "❌ No improvement. Patience: 14/50\n",
      "Epoch: [175][0/2]\tTime  0.355 ( 0.355)\tData  0.010 ( 0.010)\tLoss -0.0829 (-0.0829)\n",
      "Epoch 176: Average Training Loss: -0.078433\n",
      "❌ No improvement. Patience: 15/50\n",
      "Epoch: [176][0/2]\tTime  0.363 ( 0.363)\tData  0.015 ( 0.015)\tLoss -0.0778 (-0.0778)\n",
      "Epoch 177: Average Training Loss: -0.080068\n",
      "❌ No improvement. Patience: 16/50\n",
      "Epoch: [177][0/2]\tTime  0.363 ( 0.363)\tData  0.016 ( 0.016)\tLoss -0.0759 (-0.0759)\n",
      "Epoch 178: Average Training Loss: -0.077373\n",
      "❌ No improvement. Patience: 17/50\n",
      "Epoch: [178][0/2]\tTime  0.359 ( 0.359)\tData  0.017 ( 0.017)\tLoss -0.0725 (-0.0725)\n",
      "Epoch 179: Average Training Loss: -0.079192\n",
      "❌ No improvement. Patience: 18/50\n",
      "Epoch: [179][0/2]\tTime  0.360 ( 0.360)\tData  0.019 ( 0.019)\tLoss -0.0822 (-0.0822)\n",
      "Epoch 180: Average Training Loss: -0.081391\n",
      "❌ No improvement. Patience: 19/50\n",
      "Epoch: [180][0/2]\tTime  0.357 ( 0.357)\tData  0.017 ( 0.017)\tLoss -0.0817 (-0.0817)\n",
      "Epoch 181: Average Training Loss: -0.081140\n",
      "❌ No improvement. Patience: 20/50\n",
      "Epoch: [181][0/2]\tTime  0.359 ( 0.359)\tData  0.015 ( 0.015)\tLoss -0.0799 (-0.0799)\n",
      "Epoch 182: Average Training Loss: -0.080278\n",
      "❌ No improvement. Patience: 21/50\n",
      "Epoch: [182][0/2]\tTime  0.360 ( 0.360)\tData  0.012 ( 0.012)\tLoss -0.0731 (-0.0731)\n",
      "Epoch 183: Average Training Loss: -0.074742\n",
      "❌ No improvement. Patience: 22/50\n",
      "Epoch: [183][0/2]\tTime  0.356 ( 0.356)\tData  0.014 ( 0.014)\tLoss -0.0828 (-0.0828)\n",
      "Epoch 184: Average Training Loss: -0.082964\n",
      "✅ New best model saved with loss -0.082964\n",
      "Epoch: [184][0/2]\tTime  0.242 ( 0.242)\tData  0.013 ( 0.013)\tLoss -0.0839 (-0.0839)\n",
      "Epoch 185: Average Training Loss: -0.082618\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [185][0/2]\tTime  0.376 ( 0.376)\tData  0.017 ( 0.017)\tLoss -0.0786 (-0.0786)\n",
      "Epoch 186: Average Training Loss: -0.080943\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [186][0/2]\tTime  0.360 ( 0.360)\tData  0.017 ( 0.017)\tLoss -0.0840 (-0.0840)\n",
      "Epoch 187: Average Training Loss: -0.081892\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [187][0/2]\tTime  0.469 ( 0.469)\tData  0.016 ( 0.016)\tLoss -0.0851 (-0.0851)\n",
      "Epoch 188: Average Training Loss: -0.082994\n",
      "✅ New best model saved with loss -0.082994\n",
      "Epoch: [188][0/2]\tTime  0.279 ( 0.279)\tData  0.048 ( 0.048)\tLoss -0.0813 (-0.0813)\n",
      "Epoch 189: Average Training Loss: -0.079486\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [189][0/2]\tTime  0.356 ( 0.356)\tData  0.017 ( 0.017)\tLoss -0.0837 (-0.0837)\n",
      "Epoch 190: Average Training Loss: -0.083099\n",
      "✅ New best model saved with loss -0.083099\n",
      "Epoch: [190][0/2]\tTime  0.204 ( 0.204)\tData  0.015 ( 0.015)\tLoss -0.0830 (-0.0830)\n",
      "Epoch 191: Average Training Loss: -0.081753\n",
      "❌ No improvement. Patience: 1/50\n",
      "Epoch: [191][0/2]\tTime  0.353 ( 0.353)\tData  0.015 ( 0.015)\tLoss -0.0790 (-0.0790)\n",
      "Epoch 192: Average Training Loss: -0.077770\n",
      "❌ No improvement. Patience: 2/50\n",
      "Epoch: [192][0/2]\tTime  0.355 ( 0.355)\tData  0.020 ( 0.020)\tLoss -0.0844 (-0.0844)\n",
      "Epoch 193: Average Training Loss: -0.081804\n",
      "❌ No improvement. Patience: 3/50\n",
      "Epoch: [193][0/2]\tTime  0.364 ( 0.364)\tData  0.016 ( 0.016)\tLoss -0.0803 (-0.0803)\n",
      "Epoch 194: Average Training Loss: -0.081348\n",
      "❌ No improvement. Patience: 4/50\n",
      "Epoch: [194][0/2]\tTime  0.367 ( 0.367)\tData  0.015 ( 0.015)\tLoss -0.0809 (-0.0809)\n",
      "Epoch 195: Average Training Loss: -0.080968\n",
      "❌ No improvement. Patience: 5/50\n",
      "Epoch: [195][0/2]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0820 (-0.0820)\n",
      "Epoch 196: Average Training Loss: -0.081305\n",
      "❌ No improvement. Patience: 6/50\n",
      "Epoch: [196][0/2]\tTime  0.357 ( 0.357)\tData  0.017 ( 0.017)\tLoss -0.0828 (-0.0828)\n",
      "Epoch 197: Average Training Loss: -0.082228\n",
      "❌ No improvement. Patience: 7/50\n",
      "Epoch: [197][0/2]\tTime  0.355 ( 0.355)\tData  0.017 ( 0.017)\tLoss -0.0778 (-0.0778)\n",
      "Epoch 198: Average Training Loss: -0.081200\n",
      "❌ No improvement. Patience: 8/50\n",
      "Epoch: [198][0/2]\tTime  0.360 ( 0.360)\tData  0.014 ( 0.014)\tLoss -0.0778 (-0.0778)\n",
      "Epoch 199: Average Training Loss: -0.081546\n",
      "❌ No improvement. Patience: 9/50\n",
      "Epoch: [199][0/2]\tTime  0.351 ( 0.351)\tData  0.026 ( 0.026)\tLoss -0.0772 (-0.0772)\n",
      "Epoch 200: Average Training Loss: -0.080284\n",
      "❌ No improvement. Patience: 10/50\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Early stopping parameters\n",
    "best_loss = float('inf')\n",
    "patience = 50  # Number of epochs to wait for improvement\n",
    "patience_counter = 0\n",
    "\n",
    "start_epoch = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # Train and get average loss\n",
    "    avg_loss = train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "    print(f\"Epoch {epoch + 1}: Average Training Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'vgg16',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_loss': best_loss\n",
    "        }, f'models/pretrain/{timestamp}_best_model.pth.tar')\n",
    "\n",
    "        print(f\"✅ New best model saved with loss {best_loss:.6f}\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"❌ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
