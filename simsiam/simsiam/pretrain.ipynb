{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "import simsiam.builder\n",
    "\n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 0:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception_v3', 'list_models', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n",
      "=> creating model 'vgg16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)\n",
    "# create model\n",
    "arch = 'vgg16' \n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = simsiam.builder.SimSiam(\n",
    "    models.__dict__[arch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "indices 0 used: [(np.int64(22), np.int64(143)), (np.int64(988), np.int64(206)), (np.int64(612), np.int64(606)), (np.int64(1016), np.int64(622)), (np.int64(195), np.int64(381))]\n",
      "indices 1 used: [(np.int64(514), np.int64(586)), (np.int64(1069), np.int64(81)), (np.int64(154), np.int64(9)), (np.int64(20), np.int64(446)), (np.int64(900), np.int64(18))]\n",
      "[-264 -574  301  358  419  580  655  671  648  620  596  542  521  477\n",
      "  455  421  396  372  352  323  299  274  251  220  197  182  168  159\n",
      "  150  141  137  129  122  124  117  102   89   85   75   44   58   76\n",
      "   83   50   66   74   72   68   64   44   47   59   71   68   66   60\n",
      "   61   34   18    6   11  -58 -194  -77   12   19   39   49   52   54\n",
      "   61   51   53   52   55   42   33   27   13  -23 -238 -248 -127 -122\n",
      "  -43    2   13    9   23   14   23   28   37   37   34   33   36   55\n",
      "   53   36   33   39   28   80  -29  -10    0    0    0    0    0    0\n",
      "    0    0 -207 -237  -68  -35  -48  -32   11   19   30   39   38   68\n",
      "   48   50   47   52   45   51   46   51   39   47   44   47   46   43\n",
      "   48   47   42   38   41   36   25   21   17    6   -8  -70    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -40  -28    0    0    4  -17    2   -1   21   23   30   31   21   24\n",
      "   29   25   31   26   28   31   35   32   18   31   25   31   37   31\n",
      "   28   25   28   32   32   29   37   24   33   33   33   20   15   19\n",
      "    8   -4   -3    4  -21    7    0  -20  -32   -2  -45  -32    5  -23]\n",
      "[-264. -574.  301.  358.  419.  580.  655.  671.  648.  620.  596.  542.\n",
      "  521.  477.  455.  421.  396.  372.  352.  323.  299.  274.  251.  220.\n",
      "  197.  182.  168.  159.  150.  141.  137.  129.  122.  124.  117.  102.\n",
      "   89.   85.   75.   44.   58.   76.   83.   50.   66.   74.   72.   68.\n",
      "   64.   44.   47.   59.   71.   68.   66.   60.   61.   34.   18.    6.\n",
      "   11.  -58. -194.  -77.   12.   19.   39.   49.   52.   54.   61.   51.\n",
      "   53.   52.   55.   42.   33.   27.   13.  -23. -238. -248. -127. -122.\n",
      "  -43.    2.   13.    9.   23.   14.   23.   28.   37.   37.   34.   33.\n",
      "   36.   55.   53.   36.   33.   39.   28.   80.  -29.  -10.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -207. -237.  -68.  -35.  -48.  -32.\n",
      "   11.   19.   30.   39.   38.   68.   48.   50.   47.   52.   45.   51.\n",
      "   46.   51.   39.   47.   44.   47.   46.   43.   48.   47.   42.   38.\n",
      "   41.   36.   25.   21.   17.    6.   -8.  -70.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -40.  -28.    0.    0.    4.  -17.    2.   -1.   21.   23.   30.   31.\n",
      "   21.   24.   29.   25.   31.   26.   28.   31.   35.   32.   18.   31.\n",
      "   25.   31.   37.   31.   28.   25.   28.   32.   32.   29.   37.   24.\n",
      "   33.   33.   33.   20.   15.   19.    8.   -4.   -3.    4.  -21.    7.\n",
      "    0.  -20.  -32.   -2.  -45.  -32.    5.  -23.]\n",
      "[-216 -330  343  405  411  526  598  591  567  512  474  421  383  352\n",
      "  322  303  298  277  259  238  230  219  213  217  217  217  215  216\n",
      "  220  223  236  243  223  233  244  249  268  291  312  353  352  368\n",
      "  382  442  468  477  497  511  558  603  632  654  670  695  709  721\n",
      "  728  705  650  622  613  593  538  695  753  792  871  972 1054 1100\n",
      " 1144 1164 1167 1170 1149 1103  988  863  769  672  182   98  380  329\n",
      "  537  607  696  782  892  973 1019 1072 1106 1153 1204 1233 1279 1265\n",
      " 1235 1240 1134 1066 1039  923  876  744    0    0    0    0    0    0\n",
      "    0    0 -146  -22   48  104  144  250  336  465  562  659  763  825\n",
      "  909  957 1008 1035 1055 1076 1091 1096 1090 1075 1039  991  935  856\n",
      "  778  703  617  553  489  479  426  368  350  337  281  178    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -11  -52   15   21   29   41   53   82  102  134  158  164  202  247\n",
      "  268  297  322  338  339  365  363  360  348  390  380  403  418  399\n",
      "  378  373  332  294  227  177  150  128   89   91   99  110   77   85\n",
      "   77   71   47   67   38   21   26   17   -2   15   12  -28  -18  -21]\n",
      "[-216. -330.  343.  405.  411.  526.  598.  591.  567.  512.  474.  421.\n",
      "  383.  352.  322.  303.  298.  277.  259.  238.  230.  219.  213.  217.\n",
      "  217.  217.  215.  216.  220.  223.  236.  243.  223.  233.  244.  249.\n",
      "  268.  291.  312.  353.  352.  368.  382.  442.  468.  477.  497.  511.\n",
      "  558.  603.  632.  654.  670.  695.  709.  721.  728.  705.  650.  622.\n",
      "  613.  593.  538.  695.  753.  792.  871.  972. 1054. 1100. 1144. 1164.\n",
      " 1167. 1170. 1149. 1103.  988.  863.  769.  672.  182.   98.  380.  329.\n",
      "  537.  607.  696.  782.  892.  973. 1019. 1072. 1106. 1153. 1204. 1233.\n",
      " 1279. 1265. 1235. 1240. 1134. 1066. 1039.  923.  876.  744.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -146.  -22.   48.  104.  144.  250.\n",
      "  336.  465.  562.  659.  763.  825.  909.  957. 1008. 1035. 1055. 1076.\n",
      " 1091. 1096. 1090. 1075. 1039.  991.  935.  856.  778.  703.  617.  553.\n",
      "  489.  479.  426.  368.  350.  337.  281.  178.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -11.  -52.   15.   21.   29.   41.   53.   82.  102.  134.  158.  164.\n",
      "  202.  247.  268.  297.  322.  338.  339.  365.  363.  360.  348.  390.\n",
      "  380.  403.  418.  399.  378.  373.  332.  294.  227.  177.  150.  128.\n",
      "   89.   91.   99.  110.   77.   85.   77.   71.   47.   67.   38.   21.\n",
      "   26.   17.   -2.   15.   12.  -28.  -18.  -21.]\n"
     ]
    }
   ],
   "source": [
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "indices_0 = [(np.int64(22), np.int64(143)), (np.int64(988), np.int64(206)), (np.int64(612), np.int64(606)), (np.int64(1016), np.int64(622)), (np.int64(195), np.int64(381))]    \n",
    "indices_1 = [(np.int64(514), np.int64(586)), (np.int64(1069), np.int64(81)), (np.int64(154), np.int64(9)), (np.int64(20), np.int64(446)), (np.int64(900), np.int64(18))]\n",
    "selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "indices = indices_0 +  indices_1\n",
    "\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  20\n",
      "hasil augmentasi 1 shape: (20, 9, 9, 224)\n",
      "label augmentai 1 shape: (20,)\n",
      "hasil augmentasi 2 shape: (20, 9, 9, 224)\n",
      "label augmentasi 2 shape: (20,)\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Element 0 occurs 10 times.\n",
      "Element 1 occurs 10 times.\n",
      "Element 0 occurs 10 times.\n",
      "Element 1 occurs 10 times.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts1):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts2):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "print(label_augment1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi gabungan untuk training: (40, 9, 9, 224)\n",
      "label augmentasi gabungan: (40,)\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n"
     ]
    }
   ],
   "source": [
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[-0.0134,  0.0281, -0.0032,  ...,  0.0012,  0.0513, -0.0047]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0134,  0.0281, -0.0032,  ...,  0.0012,  0.0513, -0.0048]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0486, 0.0347, 0.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0485, 0.0350, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 40\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "gpu = 0\n",
    "\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([40, 224, 9, 9])\n",
      "bacth size: torch.Size([40, 224, 9, 9])\n",
      "length batch: 40\n",
      "torch.Size([9, 9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "            img2 = self.transform(img)  # Second augmentation\n",
    "        \n",
    "            return img1, img2  # Return both augmented versions\n",
    "        \n",
    "        return img, img  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "preloaded_images = data_augment  # Example tensor with 100 images\n",
    "X_train = torch.tensor(preloaded_images)\n",
    "X_train = X_train.to(torch.float32)\n",
    "X_train = X_train.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example normalization\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1, batch2 = next(iter(train_loader))\n",
    "\n",
    "print(f\"bacth size: {batch1.size()}\")\n",
    "print(f\"length batch: {len(batch1)}\")  # Should print 2 (Two transformed views per image)\n",
    "print(f\"{batch1[0][0].shape}\")  # Should print torch.Size([9, 9, 224]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images1, images2) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input1 = images1\n",
    "        input2 = images2\n",
    "\n",
    "      \n",
    "        input1 = input1.to(device, non_blocking=True)\n",
    "        input2 = input2.to(device, non_blocking=True)\n",
    "           \n",
    "\n",
    "        p1, p2, z1, z2 = model(x1=input1, x2=input2) \n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), input1.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/1]\tTime 12.278 (12.278)\tData  0.076 ( 0.076)\tLoss -0.0005 (-0.0005)\n",
      "Epoch: [1][0/1]\tTime  2.107 ( 2.107)\tData  0.023 ( 0.023)\tLoss -0.0005 (-0.0005)\n",
      "Epoch: [2][0/1]\tTime  0.662 ( 0.662)\tData  0.019 ( 0.019)\tLoss 0.0046 (0.0046)\n",
      "Epoch: [3][0/1]\tTime  0.628 ( 0.628)\tData  0.039 ( 0.039)\tLoss 0.0008 (0.0008)\n",
      "Epoch: [4][0/1]\tTime  0.979 ( 0.979)\tData  0.049 ( 0.049)\tLoss -0.0022 (-0.0022)\n",
      "Epoch: [5][0/1]\tTime  0.283 ( 0.283)\tData  0.024 ( 0.024)\tLoss 0.0004 (0.0004)\n",
      "Epoch: [6][0/1]\tTime  0.315 ( 0.315)\tData  0.059 ( 0.059)\tLoss -0.0011 (-0.0011)\n",
      "Epoch: [7][0/1]\tTime  0.731 ( 0.731)\tData  0.020 ( 0.020)\tLoss 0.0014 (0.0014)\n",
      "Epoch: [8][0/1]\tTime  0.331 ( 0.331)\tData  0.049 ( 0.049)\tLoss -0.0028 (-0.0028)\n",
      "Epoch: [9][0/1]\tTime  0.256 ( 0.256)\tData  0.018 ( 0.018)\tLoss 0.0005 (0.0005)\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "epochs = epochs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # train for one epoch\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': 'vgg16',\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best=False, filename='models/pretrain/{}_checkpoint_{:04d}.pth.tar'.format(timestamp, epoch+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
