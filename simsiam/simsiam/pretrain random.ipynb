{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from HSI_class import HSI\n",
    "import createSample as CS\n",
    "import augmentation as aug\n",
    "\n",
    "import simsiam.loader\n",
    "import simsiam.builder\n",
    "\n",
    "sample_per_class = 5\n",
    "num_per_category_augment_1 = 10\n",
    "num_per_category_augment_2 = 10\n",
    "epochs = 200\n",
    "\n",
    "random = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM01.mat\n",
      "Processing file: C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\\GM02.mat\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Asus TUF\\Documents\\code\\TA\\Hyperspectral oil spill detection datasets\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if i > 1:\n",
    "        break\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        hsi = HSI(file_path)\n",
    "        dataset.append(hsi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception_v3', 'list_models', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n",
      "=> creating model 'vgg16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus TUF\\Documents\\code\\TA\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)\n",
    "# create model\n",
    "arch = 'vgg16' \n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = simsiam.builder.SimSiam(\n",
    "    models.__dict__[arch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random: 1\n",
      "generating random sample\n",
      "hsi shape\n",
      "(1243, 684, 224)\n",
      "creating 5 Randomly chosen 0 indices:\n",
      "creating 5 Randomly chosen 1 indices:\n",
      "indices 0 used: [(np.int64(526), np.int64(187)), (np.int64(537), np.int64(71)), (np.int64(496), np.int64(222)), (np.int64(1200), np.int64(102)), (np.int64(1178), np.int64(413))]\n",
      "indices 1 used: [(np.int64(174), np.int64(66)), (np.int64(382), np.int64(580)), (np.int64(1202), np.int64(171)), (np.int64(469), np.int64(254)), (np.int64(267), np.int64(228))]\n",
      "[-195 -423  311  356  398  568  618  629  610  576  532  484  448  419\n",
      "  388  356  341  312  286  256  229  208  191  163  141  130  122  110\n",
      "  101   98   95   83   80   75   64   52   33   28   20  -21    2   17\n",
      "   24  -13    5   14    9    5    0  -24  -15   -8   12    3    8    1\n",
      "    0  -20  -26  -31  -31 -138 -118 -116  -25  -12   -6    0    1    2\n",
      "    3    1    3    0    0   -3   -7   -7  -17  -47 -122 -107 -134 -144\n",
      "  -53  -18  -11  -18  -18  -23  -15  -15   -4   -2   -4    0   -5    3\n",
      "   17   10    1    0   -5  -24  -28  -70  -85    0    0    0    0    0\n",
      "    0  -72  -37  -60 -119  -60  -50  -82  -21   -6    0    4    7    5\n",
      "   10   10   11   11   21   14   15   16   11   12   12   10   17   13\n",
      "    8   13    9    8   10    1    8    3   -8  -17  -28  -59  -32    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   -9  -21   -9   -6   -3  -18  -20   -3    3    3    8   11    8   11\n",
      "   14   13   13   11   11   14   12    3   22   14   11   17   20   15\n",
      "    8   10   16   18   11    2   13    0    9    7    1    2  -15    0\n",
      "   -5    2  -24    7   -3  -21   -6    0  -11  -13  -16  -19  -14  -11]\n",
      "[-195. -423.  311.  356.  398.  568.  618.  629.  610.  576.  532.  484.\n",
      "  448.  419.  388.  356.  341.  312.  286.  256.  229.  208.  191.  163.\n",
      "  141.  130.  122.  110.  101.   98.   95.   83.   80.   75.   64.   52.\n",
      "   33.   28.   20.  -21.    2.   17.   24.  -13.    5.   14.    9.    5.\n",
      "    0.  -24.  -15.   -8.   12.    3.    8.    1.    0.  -20.  -26.  -31.\n",
      "  -31. -138. -118. -116.  -25.  -12.   -6.    0.    1.    2.    3.    1.\n",
      "    3.    0.    0.   -3.   -7.   -7.  -17.  -47. -122. -107. -134. -144.\n",
      "  -53.  -18.  -11.  -18.  -18.  -23.  -15.  -15.   -4.   -2.   -4.    0.\n",
      "   -5.    3.   17.   10.    1.    0.   -5.  -24.  -28.  -70.  -85.    0.\n",
      "    0.    0.    0.    0.    0.  -72.  -37.  -60. -119.  -60.  -50.  -82.\n",
      "  -21.   -6.    0.    4.    7.    5.   10.   10.   11.   11.   21.   14.\n",
      "   15.   16.   11.   12.   12.   10.   17.   13.    8.   13.    9.    8.\n",
      "   10.    1.    8.    3.   -8.  -17.  -28.  -59.  -32.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   -9.  -21.   -9.   -6.   -3.  -18.  -20.   -3.    3.    3.    8.   11.\n",
      "    8.   11.   14.   13.   13.   11.   11.   14.   12.    3.   22.   14.\n",
      "   11.   17.   20.   15.    8.   10.   16.   18.   11.    2.   13.    0.\n",
      "    9.    7.    1.    2.  -15.    0.   -5.    2.  -24.    7.   -3.  -21.\n",
      "   -6.    0.  -11.  -13.  -16.  -19.  -14.  -11.]\n",
      "[-149 -265  297  345  381  501  596  609  601  574  557  505  464  434\n",
      "  415  387  370  349  329  312  300  286  274  247  241  228  220  214\n",
      "  209  194  200  193  187  187  184  171  156  149  131   93   91   99\n",
      "  102   69   91   99   99   96   94   71   66   68   76   71   71   60\n",
      "   54   32   13    1    4  -80 -186  -89   -5    0   12   21   24   27\n",
      "   33   31   35   31   32   27   20   13    0  -27 -225 -229 -128 -150\n",
      "  -48  -11    0   -6   -2   -7    6    6   12   14   10    7   10   14\n",
      "    8   11   18    0    0   83  -41  -95    0    0    0    0    0    0\n",
      "    0    0 -222 -258  -78  -55  -95  -30  -25   -5    5   10   10   21\n",
      "   18   13    9   17   11   14   15   18   20   18   15   13   17   16\n",
      "   12    8    2    9    8   10    0    1  -14   -9  -50 -174    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " -177 -153  -36   -8  -11  -15  -19  -26    4    7    5    6    9   12\n",
      "   11   10   13   11   11    7    6   12   10   10    7    9   16   20\n",
      "    8   14   16    9   11   18    9    5   10    8    1    9    0   -6\n",
      "   -5   -4  -23   -3  -11   -9  -16    3  -34  -14  -34  -47  -38 -168]\n",
      "[-149. -265.  297.  345.  381.  501.  596.  609.  601.  574.  557.  505.\n",
      "  464.  434.  415.  387.  370.  349.  329.  312.  300.  286.  274.  247.\n",
      "  241.  228.  220.  214.  209.  194.  200.  193.  187.  187.  184.  171.\n",
      "  156.  149.  131.   93.   91.   99.  102.   69.   91.   99.   99.   96.\n",
      "   94.   71.   66.   68.   76.   71.   71.   60.   54.   32.   13.    1.\n",
      "    4.  -80. -186.  -89.   -5.    0.   12.   21.   24.   27.   33.   31.\n",
      "   35.   31.   32.   27.   20.   13.    0.  -27. -225. -229. -128. -150.\n",
      "  -48.  -11.    0.   -6.   -2.   -7.    6.    6.   12.   14.   10.    7.\n",
      "   10.   14.    8.   11.   18.    0.    0.   83.  -41.  -95.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -222. -258.  -78.  -55.  -95.  -30.\n",
      "  -25.   -5.    5.   10.   10.   21.   18.   13.    9.   17.   11.   14.\n",
      "   15.   18.   20.   18.   15.   13.   17.   16.   12.    8.    2.    9.\n",
      "    8.   10.    0.    1.  -14.   -9.  -50. -174.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " -177. -153.  -36.   -8.  -11.  -15.  -19.  -26.    4.    7.    5.    6.\n",
      "    9.   12.   11.   10.   13.   11.   11.    7.    6.   12.   10.   10.\n",
      "    7.    9.   16.   20.    8.   14.   16.    9.   11.   18.    9.    5.\n",
      "   10.    8.    1.    9.    0.   -6.   -5.   -4.  -23.   -3.  -11.   -9.\n",
      "  -16.    3.  -34.  -14.  -34.  -47.  -38. -168.]\n"
     ]
    }
   ],
   "source": [
    "hsi_ = dataset[0]\n",
    "patch_size = 9\n",
    "sample_per_class = sample_per_class\n",
    "\n",
    "indices_0 = []\n",
    "indices_1 = []\n",
    "\n",
    "print(f\"random: {random}\")\n",
    "\n",
    "if random:\n",
    "    print(\"generating random sample\")\n",
    "    selected_patch_0, selected_patch_1, indices_0, indices_1 = CS.createSample(hsi_, patch_size, sample_per_class)\n",
    "else:\n",
    "    print(\"using generated indices\")\n",
    "    indices_0 = [(np.int64(45), np.int64(528)), (np.int64(872), np.int64(337)), (np.int64(404), np.int64(609)), (np.int64(1226), np.int64(484)), (np.int64(449), np.int64(414))]\n",
    "    indices_1 = [(np.int64(597), np.int64(650)), (np.int64(131), np.int64(59)), (np.int64(104), np.int64(48)), (np.int64(256), np.int64(407)), (np.int64(607), np.int64(440))]\n",
    "\n",
    "    selected_patch_0, selected_patch_1 = CS.getSample(hsi_, patch_size, sample_per_class, indices_0, indices_1)\n",
    "\n",
    "\n",
    "i =0\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-243 -361  383  431  450  591  657  656  619  584  543  491  452  413\n",
      "  381  354  334  305  279  260  234  206  184  160  138  125  115  108\n",
      "   97   94   90   83   81   74   70   60   46   38   34  -11    3   24\n",
      "   28   -7   14   18   17    8    9  -18  -10    0   16   11   11    9\n",
      "    7  -11  -20  -23  -20  -79  -93  -94  -22  -12    0    2    3    6\n",
      "    7    3    3    4    5    0   -1   -2  -12  -36  -85  -72 -102  -99\n",
      "  -44  -13   -5  -15   -6  -20   -8  -12    0    2    1   -3    0  -10\n",
      "   -8    1   -6  -14   -9  -23  -41  -86  -49    0    0    0    0    0\n",
      "    0    0    0    0    0  -21  -47  -50  -12   -1   -4   -3   -6    1\n",
      "    1   -3   -2    0    2    3   -1    0    0   -2   -1   -4    3    4\n",
      "    2   -2    2    0   -7    4   -4   -6  -11  -24  -39  -36  -20    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   -1  -13  -15  -10   -4  -16  -31  -22   -3   -3   -3   -6   -7    0\n",
      "   -3    3   -3   -6   -2   -5   -1    0   -2   -6   -1   -4    0    3\n",
      "   -3   -2   -1   -4    1    2  -12  -11  -15  -15   -8   -8  -13  -17\n",
      "  -10  -11  -20  -18  -21  -21  -14   -6  -13   -8  -18  -11    5  -17]\n",
      "[-243. -361.  383.  431.  450.  591.  657.  656.  619.  584.  543.  491.\n",
      "  452.  413.  381.  354.  334.  305.  279.  260.  234.  206.  184.  160.\n",
      "  138.  125.  115.  108.   97.   94.   90.   83.   81.   74.   70.   60.\n",
      "   46.   38.   34.  -11.    3.   24.   28.   -7.   14.   18.   17.    8.\n",
      "    9.  -18.  -10.    0.   16.   11.   11.    9.    7.  -11.  -20.  -23.\n",
      "  -20.  -79.  -93.  -94.  -22.  -12.    0.    2.    3.    6.    7.    3.\n",
      "    3.    4.    5.    0.   -1.   -2.  -12.  -36.  -85.  -72. -102.  -99.\n",
      "  -44.  -13.   -5.  -15.   -6.  -20.   -8.  -12.    0.    2.    1.   -3.\n",
      "    0.  -10.   -8.    1.   -6.  -14.   -9.  -23.  -41.  -86.  -49.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.  -21.  -47.  -50.\n",
      "  -12.   -1.   -4.   -3.   -6.    1.    1.   -3.   -2.    0.    2.    3.\n",
      "   -1.    0.    0.   -2.   -1.   -4.    3.    4.    2.   -2.    2.    0.\n",
      "   -7.    4.   -4.   -6.  -11.  -24.  -39.  -36.  -20.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   -1.  -13.  -15.  -10.   -4.  -16.  -31.  -22.   -3.   -3.   -3.   -6.\n",
      "   -7.    0.   -3.    3.   -3.   -6.   -2.   -5.   -1.    0.   -2.   -6.\n",
      "   -1.   -4.    0.    3.   -3.   -2.   -1.   -4.    1.    2.  -12.  -11.\n",
      "  -15.  -15.   -8.   -8.  -13.  -17.  -10.  -11.  -20.  -18.  -21.  -21.\n",
      "  -14.   -6.  -13.   -8.  -18.  -11.    5.  -17.]\n",
      "[-145 -211  325  378  435  577  676  675  661  627  599  560  525  489\n",
      "  468  444  432  409  386  359  338  328  314  295  279  272  268  260\n",
      "  257  254  255  250  244  243  235  219  205  201  194  174  180  187\n",
      "  196  168  187  188  181  173  178  160  173  175  186  180  180  171\n",
      "  169  136  121  108  103   51  -13   75  136  144  155  169  183  190\n",
      "  200  195  194  188  189  175  153  134  111   82  -85 -133  -27  -40\n",
      "   45   87  108  119  142  146  163  162  168  173  174  189  183  165\n",
      "  159  157  139  138  120  167   99   51    0    0    0    0    0    0\n",
      "    0    0 -105 -165  -17    9  -11   -8   53   69   80   93  101  106\n",
      "  124  126  131  127  137  134  138  140  141  142  142  133  140  135\n",
      "  137  142  139  132  121  102   93   90   71   55   47   -6    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  -39  -43   16   24   32   43   66   60   65   66   73   79   83   90\n",
      "   84   93   90   98  102  106   98  104  102  102   99  102  111  107\n",
      "  103   99  105  106  104  101  107  101   86   93   82   79   74   72\n",
      "   44   47   57   41  -11    6   22   21   -3   17   -2  -47  -38  -80]\n",
      "[-145. -211.  325.  378.  435.  577.  676.  675.  661.  627.  599.  560.\n",
      "  525.  489.  468.  444.  432.  409.  386.  359.  338.  328.  314.  295.\n",
      "  279.  272.  268.  260.  257.  254.  255.  250.  244.  243.  235.  219.\n",
      "  205.  201.  194.  174.  180.  187.  196.  168.  187.  188.  181.  173.\n",
      "  178.  160.  173.  175.  186.  180.  180.  171.  169.  136.  121.  108.\n",
      "  103.   51.  -13.   75.  136.  144.  155.  169.  183.  190.  200.  195.\n",
      "  194.  188.  189.  175.  153.  134.  111.   82.  -85. -133.  -27.  -40.\n",
      "   45.   87.  108.  119.  142.  146.  163.  162.  168.  173.  174.  189.\n",
      "  183.  165.  159.  157.  139.  138.  120.  167.   99.   51.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. -105. -165.  -17.    9.  -11.   -8.\n",
      "   53.   69.   80.   93.  101.  106.  124.  126.  131.  127.  137.  134.\n",
      "  138.  140.  141.  142.  142.  133.  140.  135.  137.  142.  139.  132.\n",
      "  121.  102.   93.   90.   71.   55.   47.   -6.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  -39.  -43.   16.   24.   32.   43.   66.   60.   65.   66.   73.   79.\n",
      "   83.   90.   84.   93.   90.   98.  102.  106.   98.  104.  102.  102.\n",
      "   99.  102.  111.  107.  103.   99.  105.  106.  104.  101.  107.  101.\n",
      "   86.   93.   82.   79.   74.   72.   44.   47.   57.   41.  -11.    6.\n",
      "   22.   21.   -3.   17.   -2.  -47.  -38.  -80.]\n"
     ]
    }
   ],
   "source": [
    "i =4\n",
    "half_patch = patch_size // 2\n",
    "print(hsi_.img[indices_0[i][0]][indices_0[i][1]])\n",
    "print(selected_patch_0[i][half_patch][half_patch])\n",
    "\n",
    "print(hsi_.img[indices_1[i][0]][indices_1[i][1]])\n",
    "print(selected_patch_1[i][half_patch][half_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(526), np.int64(187)), (np.int64(537), np.int64(71)), (np.int64(496), np.int64(222)), (np.int64(1200), np.int64(102)), (np.int64(1178), np.int64(413))]\n",
      "[(np.int64(174), np.int64(66)), (np.int64(382), np.int64(580)), (np.int64(1202), np.int64(171)), (np.int64(469), np.int64(254)), (np.int64(267), np.int64(228))]\n",
      "[(np.int64(526), np.int64(187)), (np.int64(537), np.int64(71)), (np.int64(496), np.int64(222)), (np.int64(1200), np.int64(102)), (np.int64(1178), np.int64(413)), (np.int64(174), np.int64(66)), (np.int64(382), np.int64(580)), (np.int64(1202), np.int64(171)), (np.int64(469), np.int64(254)), (np.int64(267), np.int64(228))]\n",
      "number of element equal 0 5\n",
      "number of element equal 1 5\n",
      "x_train shape: (10, 9, 9, 224)\n",
      "y_train shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "print(indices_0)\n",
    "print(indices_1)\n",
    "indices = indices_0 +  indices_1\n",
    "\n",
    "print(indices)\n",
    "# Concatenating along axis 0\n",
    "x_train = np.concatenate((selected_patch_0, selected_patch_1), )\n",
    "\n",
    "y_train = np.array([])\n",
    "\n",
    "gt = hsi_.gt\n",
    "for indice in indices:\n",
    "    # print(gt[indice[0]][indice[1]])\n",
    "    y_train = np.append(y_train, gt[indice[0]][indice[1]])\n",
    "\n",
    "count = np.count_nonzero(y_train == 0)  # Count elements equal to 0\n",
    "print(f'number of element equal 0 {count}')\n",
    "\n",
    "count = np.count_nonzero(y_train == 1)  # Count elements equal to 1\n",
    "print(f'number of element equal 1 {count}')\n",
    "\n",
    "\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected output: (10, 9, 9, 224)\n",
    "print(f\"y_train shape: {y_train.shape}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  20\n",
      "hasil augmentasi 1 shape: (20, 9, 9, 224)\n",
      "label augmentai 1 shape: (20,)\n",
      "hasil augmentasi 2 shape: (20, 9, 9, 224)\n",
      "label augmentasi 2 shape: (20,)\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Element 0 occurs 10 times.\n",
      "Element 1 occurs 10 times.\n",
      "Element 0 occurs 10 times.\n",
      "Element 1 occurs 10 times.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n_category = 2\n",
    "band_size = 224\n",
    "num_per_category_augment_1 = num_per_category_augment_1\n",
    "num_per_category_augment_2 = num_per_category_augment_2\n",
    "\n",
    "data_augment1, label_augment1 = aug.Augment_data(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_1)\n",
    "\n",
    "data_augment2, label_augment2 = aug.Augment_data2(x_train, y_train, n_category, patch_size, band_size, num_per_category_augment_2)\n",
    "\n",
    "print(f\"hasil augmentasi 1 shape: {data_augment1.shape}\")\n",
    "print(f\"label augmentai 1 shape: {label_augment1.shape}\")\n",
    "\n",
    "print(f\"hasil augmentasi 2 shape: {data_augment2.shape}\")\n",
    "print(f\"label augmentasi 2 shape: {label_augment2.shape}\")\n",
    "\n",
    "print(label_augment1)\n",
    "print(label_augment2)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts1 = np.bincount(label_augment1)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts1):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "counts2 = np.bincount(label_augment2)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts2):\n",
    "    print(f\"Element {i} occurs {count} times.\")\n",
    "\n",
    "print(label_augment1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil augmentasi gabungan untuk training: (40, 9, 9, 224)\n",
      "label augmentasi gabungan: (40,)\n",
      "Element 0 occurs 20 times.\n",
      "Element 1 occurs 20 times.\n"
     ]
    }
   ],
   "source": [
    "data_augment = np.concatenate((data_augment1, data_augment2))\n",
    "label_augment = np.concatenate((label_augment1, label_augment2))\n",
    "\n",
    "print(f\"hasil augmentasi gabungan untuk training: {data_augment.shape}\")\n",
    "print(f\"label augmentasi gabungan: {label_augment.shape}\")\n",
    "\n",
    "# print(label_augment)\n",
    "\n",
    "# Count occurrences of each unique element\n",
    "counts = np.bincount(label_augment)\n",
    "\n",
    "# Print results\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Element {i} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 224, 9, 9])\n",
      "input2 shape: torch.Size([1, 224, 9, 9])\n",
      "tensor([[ 0.0270,  0.0315,  0.0116,  ..., -0.0291, -0.0339, -0.0541]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0271,  0.0315,  0.0118,  ..., -0.0293, -0.0341, -0.0542]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0581, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0592, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "test = data_augment[0]\n",
    "test = torch.tensor(test)\n",
    "test = test.to(torch.float32)\n",
    "test = test.unsqueeze(0)\n",
    "\n",
    "input = test\n",
    "input = input.permute(0, 3, 1, 2)\n",
    "\n",
    "test2 = data_augment[1]\n",
    "test2 = torch.tensor(test2)\n",
    "test2 = test2.to(torch.float32)\n",
    "test2 = test2.unsqueeze(0)\n",
    "\n",
    "input2 = test2\n",
    "input2 = input2.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "print(f\"input2 shape: {input2.shape}\")\n",
    "\n",
    "# Pass the input through the model\n",
    "model.eval()\n",
    "p1, p2, z1, z2  = model(input, input2)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam(\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=200704, bias=True)\n",
      "  (encoder): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): ReLU(inplace=True)\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): ReLU(inplace=True)\n",
      "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (28): ReLU(inplace=True)\n",
      "      (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 40\n",
    "\n",
    "init_lr = lr * batch_size / 256\n",
    "\n",
    "gpu = 0\n",
    "\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(40, 9, 9, 224)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(gpu)\n",
    "print(gpu)\n",
    "optim_params = model.parameters()\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(optim_params, init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomHorizontalFlip(),  # Flip along width\n",
    "    transforms.RandomVerticalFlip(),    # Flip along height\n",
    "    transforms.RandomRotation(20),      # Rotate image slightly\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize hyperspectral data\n",
    "]\n",
    "\n",
    "transform = simsiam.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "\n",
    "print(data_augment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([40, 224, 9, 9])\n",
      "bacth size: torch.Size([40, 224, 9, 9])\n",
      "length batch: 40\n",
      "torch.Size([9, 9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor or list of Tensors): Preloaded images of shape (N, 9, 9, 224)\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images = images  # Assuming it's a list or tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img)  # First augmentation\n",
    "            img2 = self.transform(img)  # Second augmentation\n",
    "        \n",
    "            return img1, img2  # Return both augmented versions\n",
    "        \n",
    "        return img, img  # If no transform is provided, return the original image twice\n",
    "\n",
    "\n",
    "# Example usage\n",
    "preloaded_images = data_augment  # Example tensor with 100 images\n",
    "X_train = torch.tensor(preloaded_images)\n",
    "X_train = X_train.to(torch.float32)\n",
    "X_train = X_train.permute(0, 3, 1, 2)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example normalization\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, transform=transform)\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# 7. Check Output\n",
    "\n",
    "batch1, batch2 = next(iter(train_loader))\n",
    "\n",
    "print(f\"bacth size: {batch1.size()}\")\n",
    "print(f\"length batch: {len(batch1)}\")  # Should print 2 (Two transformed views per image)\n",
    "print(f\"{batch1[0][0].shape}\")  # Should print torch.Size([9, 9, 224]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images1, images2) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input1 = images1\n",
    "        input2 = images2\n",
    "\n",
    "      \n",
    "        input1 = input1.to(device, non_blocking=True)\n",
    "        input2 = input2.to(device, non_blocking=True)\n",
    "           \n",
    "\n",
    "        p1, p2, z1, z2 = model(x1=input1, x2=input2) \n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), input1.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/1]\tTime  4.426 ( 4.426)\tData  0.023 ( 0.023)\tLoss 0.0000 (0.0000)\n",
      "Epoch: [1][0/1]\tTime  0.357 ( 0.357)\tData  0.030 ( 0.030)\tLoss -0.0020 (-0.0020)\n",
      "Epoch: [2][0/1]\tTime  0.198 ( 0.198)\tData  0.022 ( 0.022)\tLoss 0.0027 (0.0027)\n",
      "Epoch: [3][0/1]\tTime  0.349 ( 0.349)\tData  0.022 ( 0.022)\tLoss 0.0002 (0.0002)\n",
      "Epoch: [4][0/1]\tTime  0.380 ( 0.380)\tData  0.026 ( 0.026)\tLoss -0.0008 (-0.0008)\n",
      "Epoch: [5][0/1]\tTime  0.190 ( 0.190)\tData  0.049 ( 0.049)\tLoss -0.0008 (-0.0008)\n",
      "Epoch: [6][0/1]\tTime  0.443 ( 0.443)\tData  0.025 ( 0.025)\tLoss 0.0006 (0.0006)\n",
      "Epoch: [7][0/1]\tTime  0.355 ( 0.355)\tData  0.015 ( 0.015)\tLoss 0.0031 (0.0031)\n",
      "Epoch: [8][0/1]\tTime  0.360 ( 0.360)\tData  0.015 ( 0.015)\tLoss 0.0013 (0.0013)\n",
      "Epoch: [9][0/1]\tTime  0.438 ( 0.438)\tData  0.014 ( 0.014)\tLoss -0.0022 (-0.0022)\n",
      "Epoch: [10][0/1]\tTime  0.197 ( 0.197)\tData  0.042 ( 0.042)\tLoss 0.0026 (0.0026)\n",
      "Epoch: [11][0/1]\tTime  0.375 ( 0.375)\tData  0.020 ( 0.020)\tLoss -0.0016 (-0.0016)\n",
      "Epoch: [12][0/1]\tTime  0.356 ( 0.356)\tData  0.020 ( 0.020)\tLoss 0.0020 (0.0020)\n",
      "Epoch: [13][0/1]\tTime  0.368 ( 0.368)\tData  0.017 ( 0.017)\tLoss -0.0017 (-0.0017)\n",
      "Epoch: [14][0/1]\tTime  0.339 ( 0.339)\tData  0.021 ( 0.021)\tLoss 0.0001 (0.0001)\n",
      "Epoch: [15][0/1]\tTime  0.357 ( 0.357)\tData  0.012 ( 0.012)\tLoss -0.0021 (-0.0021)\n",
      "Epoch: [16][0/1]\tTime  0.352 ( 0.352)\tData  0.015 ( 0.015)\tLoss 0.0026 (0.0026)\n",
      "Epoch: [17][0/1]\tTime  0.444 ( 0.444)\tData  0.016 ( 0.016)\tLoss 0.0004 (0.0004)\n",
      "Epoch: [18][0/1]\tTime  0.347 ( 0.347)\tData  0.017 ( 0.017)\tLoss -0.0006 (-0.0006)\n",
      "Epoch: [19][0/1]\tTime  0.352 ( 0.352)\tData  0.016 ( 0.016)\tLoss -0.0007 (-0.0007)\n",
      "Epoch: [20][0/1]\tTime  0.167 ( 0.167)\tData  0.011 ( 0.011)\tLoss -0.0025 (-0.0025)\n",
      "Epoch: [21][0/1]\tTime  0.368 ( 0.368)\tData  0.050 ( 0.050)\tLoss -0.0010 (-0.0010)\n",
      "Epoch: [22][0/1]\tTime  0.347 ( 0.347)\tData  0.013 ( 0.013)\tLoss -0.0007 (-0.0007)\n",
      "Epoch: [23][0/1]\tTime  0.368 ( 0.368)\tData  0.012 ( 0.012)\tLoss 0.0015 (0.0015)\n",
      "Epoch: [24][0/1]\tTime  0.343 ( 0.343)\tData  0.029 ( 0.029)\tLoss -0.0018 (-0.0018)\n",
      "Epoch: [25][0/1]\tTime  0.350 ( 0.350)\tData  0.007 ( 0.007)\tLoss -0.0019 (-0.0019)\n",
      "Epoch: [26][0/1]\tTime  0.352 ( 0.352)\tData  0.020 ( 0.020)\tLoss -0.0047 (-0.0047)\n",
      "Epoch: [27][0/1]\tTime  0.351 ( 0.351)\tData  0.012 ( 0.012)\tLoss 0.0018 (0.0018)\n",
      "Epoch: [28][0/1]\tTime  0.357 ( 0.357)\tData  0.013 ( 0.013)\tLoss -0.0018 (-0.0018)\n",
      "Epoch: [29][0/1]\tTime  0.347 ( 0.347)\tData  0.012 ( 0.012)\tLoss -0.0025 (-0.0025)\n",
      "Epoch: [30][0/1]\tTime  0.174 ( 0.174)\tData  0.017 ( 0.017)\tLoss -0.0016 (-0.0016)\n",
      "Epoch: [31][0/1]\tTime  0.372 ( 0.372)\tData  0.055 ( 0.055)\tLoss 0.0003 (0.0003)\n",
      "Epoch: [32][0/1]\tTime  0.354 ( 0.354)\tData  0.018 ( 0.018)\tLoss -0.0010 (-0.0010)\n",
      "Epoch: [33][0/1]\tTime  0.359 ( 0.359)\tData  0.014 ( 0.014)\tLoss -0.0002 (-0.0002)\n",
      "Epoch: [34][0/1]\tTime  0.341 ( 0.341)\tData  0.013 ( 0.013)\tLoss 0.0037 (0.0037)\n",
      "Epoch: [35][0/1]\tTime  0.356 ( 0.356)\tData  0.016 ( 0.016)\tLoss -0.0029 (-0.0029)\n",
      "Epoch: [36][0/1]\tTime  0.352 ( 0.352)\tData  0.015 ( 0.015)\tLoss -0.0007 (-0.0007)\n",
      "Epoch: [37][0/1]\tTime  0.354 ( 0.354)\tData  0.014 ( 0.014)\tLoss 0.0017 (0.0017)\n",
      "Epoch: [38][0/1]\tTime  0.352 ( 0.352)\tData  0.014 ( 0.014)\tLoss -0.0035 (-0.0035)\n",
      "Epoch: [39][0/1]\tTime  0.353 ( 0.353)\tData  0.009 ( 0.009)\tLoss -0.0030 (-0.0030)\n",
      "Epoch: [40][0/1]\tTime  0.191 ( 0.191)\tData  0.014 ( 0.014)\tLoss -0.0061 (-0.0061)\n",
      "Epoch: [41][0/1]\tTime  0.463 ( 0.463)\tData  0.059 ( 0.059)\tLoss -0.0026 (-0.0026)\n",
      "Epoch: [42][0/1]\tTime  0.347 ( 0.347)\tData  0.018 ( 0.018)\tLoss -0.0036 (-0.0036)\n",
      "Epoch: [43][0/1]\tTime  0.363 ( 0.363)\tData  0.011 ( 0.011)\tLoss -0.0024 (-0.0024)\n",
      "Epoch: [44][0/1]\tTime  0.343 ( 0.343)\tData  0.011 ( 0.011)\tLoss -0.0009 (-0.0009)\n",
      "Epoch: [45][0/1]\tTime  0.358 ( 0.358)\tData  0.014 ( 0.014)\tLoss -0.0011 (-0.0011)\n",
      "Epoch: [46][0/1]\tTime  0.353 ( 0.353)\tData  0.012 ( 0.012)\tLoss -0.0064 (-0.0064)\n",
      "Epoch: [47][0/1]\tTime  0.350 ( 0.350)\tData  0.015 ( 0.015)\tLoss -0.0054 (-0.0054)\n",
      "Epoch: [48][0/1]\tTime  0.359 ( 0.359)\tData  0.015 ( 0.015)\tLoss -0.0038 (-0.0038)\n",
      "Epoch: [49][0/1]\tTime  0.363 ( 0.363)\tData  0.016 ( 0.016)\tLoss -0.0048 (-0.0048)\n",
      "Epoch: [50][0/1]\tTime  0.300 ( 0.300)\tData  0.053 ( 0.053)\tLoss -0.0006 (-0.0006)\n",
      "Epoch: [51][0/1]\tTime  0.371 ( 0.371)\tData  0.015 ( 0.015)\tLoss -0.0003 (-0.0003)\n",
      "Epoch: [52][0/1]\tTime  0.436 ( 0.436)\tData  0.015 ( 0.015)\tLoss -0.0025 (-0.0025)\n",
      "Epoch: [53][0/1]\tTime  0.357 ( 0.357)\tData  0.018 ( 0.018)\tLoss -0.0053 (-0.0053)\n",
      "Epoch: [54][0/1]\tTime  0.368 ( 0.368)\tData  0.013 ( 0.013)\tLoss -0.0030 (-0.0030)\n",
      "Epoch: [55][0/1]\tTime  0.444 ( 0.444)\tData  0.012 ( 0.012)\tLoss -0.0074 (-0.0074)\n",
      "Epoch: [56][0/1]\tTime  0.371 ( 0.371)\tData  0.016 ( 0.016)\tLoss -0.0091 (-0.0091)\n",
      "Epoch: [57][0/1]\tTime  0.345 ( 0.345)\tData  0.025 ( 0.025)\tLoss -0.0069 (-0.0069)\n",
      "Epoch: [58][0/1]\tTime  0.358 ( 0.358)\tData  0.014 ( 0.014)\tLoss -0.0059 (-0.0059)\n",
      "Epoch: [59][0/1]\tTime  0.355 ( 0.355)\tData  0.015 ( 0.015)\tLoss -0.0069 (-0.0069)\n",
      "Epoch: [60][0/1]\tTime  0.205 ( 0.205)\tData  0.020 ( 0.020)\tLoss -0.0041 (-0.0041)\n",
      "Epoch: [61][0/1]\tTime  0.363 ( 0.363)\tData  0.050 ( 0.050)\tLoss -0.0080 (-0.0080)\n",
      "Epoch: [62][0/1]\tTime  0.358 ( 0.358)\tData  0.018 ( 0.018)\tLoss -0.0106 (-0.0106)\n",
      "Epoch: [63][0/1]\tTime  0.343 ( 0.343)\tData  0.014 ( 0.014)\tLoss -0.0059 (-0.0059)\n",
      "Epoch: [64][0/1]\tTime  0.353 ( 0.353)\tData  0.014 ( 0.014)\tLoss -0.0074 (-0.0074)\n",
      "Epoch: [65][0/1]\tTime  0.353 ( 0.353)\tData  0.015 ( 0.015)\tLoss -0.0087 (-0.0087)\n",
      "Epoch: [66][0/1]\tTime  0.359 ( 0.359)\tData  0.013 ( 0.013)\tLoss -0.0087 (-0.0087)\n",
      "Epoch: [67][0/1]\tTime  0.351 ( 0.351)\tData  0.014 ( 0.014)\tLoss -0.0089 (-0.0089)\n",
      "Epoch: [68][0/1]\tTime  0.354 ( 0.354)\tData  0.010 ( 0.010)\tLoss -0.0100 (-0.0100)\n",
      "Epoch: [69][0/1]\tTime  0.351 ( 0.351)\tData  0.017 ( 0.017)\tLoss -0.0104 (-0.0104)\n",
      "Epoch: [70][0/1]\tTime  0.158 ( 0.158)\tData  0.015 ( 0.015)\tLoss -0.0084 (-0.0084)\n",
      "Epoch: [71][0/1]\tTime  0.442 ( 0.442)\tData  0.060 ( 0.060)\tLoss -0.0094 (-0.0094)\n",
      "Epoch: [72][0/1]\tTime  0.353 ( 0.353)\tData  0.021 ( 0.021)\tLoss -0.0107 (-0.0107)\n",
      "Epoch: [73][0/1]\tTime  0.389 ( 0.389)\tData  0.016 ( 0.016)\tLoss -0.0059 (-0.0059)\n",
      "Epoch: [74][0/1]\tTime  0.357 ( 0.357)\tData  0.029 ( 0.029)\tLoss -0.0128 (-0.0128)\n",
      "Epoch: [75][0/1]\tTime  0.389 ( 0.389)\tData  0.019 ( 0.019)\tLoss -0.0103 (-0.0103)\n",
      "Epoch: [76][0/1]\tTime  0.398 ( 0.398)\tData  0.042 ( 0.042)\tLoss -0.0095 (-0.0095)\n",
      "Epoch: [77][0/1]\tTime  0.449 ( 0.449)\tData  0.045 ( 0.045)\tLoss -0.0121 (-0.0121)\n",
      "Epoch: [78][0/1]\tTime  0.343 ( 0.343)\tData  0.031 ( 0.031)\tLoss -0.0082 (-0.0082)\n",
      "Epoch: [79][0/1]\tTime  0.477 ( 0.477)\tData  0.011 ( 0.011)\tLoss -0.0129 (-0.0129)\n",
      "Epoch: [80][0/1]\tTime  0.225 ( 0.225)\tData  0.016 ( 0.016)\tLoss -0.0144 (-0.0144)\n",
      "Epoch: [81][0/1]\tTime  0.388 ( 0.388)\tData  0.063 ( 0.063)\tLoss -0.0132 (-0.0132)\n",
      "Epoch: [82][0/1]\tTime  0.358 ( 0.358)\tData  0.010 ( 0.010)\tLoss -0.0096 (-0.0096)\n",
      "Epoch: [83][0/1]\tTime  0.382 ( 0.382)\tData  0.012 ( 0.012)\tLoss -0.0120 (-0.0120)\n",
      "Epoch: [84][0/1]\tTime  0.357 ( 0.357)\tData  0.025 ( 0.025)\tLoss -0.0129 (-0.0129)\n",
      "Epoch: [85][0/1]\tTime  0.356 ( 0.356)\tData  0.012 ( 0.012)\tLoss -0.0131 (-0.0131)\n",
      "Epoch: [86][0/1]\tTime  0.355 ( 0.355)\tData  0.016 ( 0.016)\tLoss -0.0122 (-0.0122)\n",
      "Epoch: [87][0/1]\tTime  0.448 ( 0.448)\tData  0.012 ( 0.012)\tLoss -0.0165 (-0.0165)\n",
      "Epoch: [88][0/1]\tTime  0.351 ( 0.351)\tData  0.014 ( 0.014)\tLoss -0.0125 (-0.0125)\n",
      "Epoch: [89][0/1]\tTime  0.354 ( 0.354)\tData  0.014 ( 0.014)\tLoss -0.0167 (-0.0167)\n",
      "Epoch: [90][0/1]\tTime  0.209 ( 0.209)\tData  0.017 ( 0.017)\tLoss -0.0174 (-0.0174)\n",
      "Epoch: [91][0/1]\tTime  0.358 ( 0.358)\tData  0.054 ( 0.054)\tLoss -0.0136 (-0.0136)\n",
      "Epoch: [92][0/1]\tTime  0.354 ( 0.354)\tData  0.015 ( 0.015)\tLoss -0.0132 (-0.0132)\n",
      "Epoch: [93][0/1]\tTime  0.354 ( 0.354)\tData  0.016 ( 0.016)\tLoss -0.0160 (-0.0160)\n",
      "Epoch: [94][0/1]\tTime  0.360 ( 0.360)\tData  0.018 ( 0.018)\tLoss -0.0145 (-0.0145)\n",
      "Epoch: [95][0/1]\tTime  0.359 ( 0.359)\tData  0.014 ( 0.014)\tLoss -0.0144 (-0.0144)\n",
      "Epoch: [96][0/1]\tTime  0.351 ( 0.351)\tData  0.016 ( 0.016)\tLoss -0.0177 (-0.0177)\n",
      "Epoch: [97][0/1]\tTime  0.452 ( 0.452)\tData  0.011 ( 0.011)\tLoss -0.0157 (-0.0157)\n",
      "Epoch: [98][0/1]\tTime  0.356 ( 0.356)\tData  0.013 ( 0.013)\tLoss -0.0190 (-0.0190)\n",
      "Epoch: [99][0/1]\tTime  0.386 ( 0.386)\tData  0.014 ( 0.014)\tLoss -0.0173 (-0.0173)\n",
      "Epoch: [100][0/1]\tTime  0.202 ( 0.202)\tData  0.017 ( 0.017)\tLoss -0.0191 (-0.0191)\n",
      "Epoch: [101][0/1]\tTime  0.364 ( 0.364)\tData  0.053 ( 0.053)\tLoss -0.0180 (-0.0180)\n",
      "Epoch: [102][0/1]\tTime  0.353 ( 0.353)\tData  0.019 ( 0.019)\tLoss -0.0176 (-0.0176)\n",
      "Epoch: [103][0/1]\tTime  0.369 ( 0.369)\tData  0.010 ( 0.010)\tLoss -0.0175 (-0.0175)\n",
      "Epoch: [104][0/1]\tTime  0.353 ( 0.353)\tData  0.020 ( 0.020)\tLoss -0.0249 (-0.0249)\n",
      "Epoch: [105][0/1]\tTime  0.355 ( 0.355)\tData  0.013 ( 0.013)\tLoss -0.0203 (-0.0203)\n",
      "Epoch: [106][0/1]\tTime  0.358 ( 0.358)\tData  0.016 ( 0.016)\tLoss -0.0192 (-0.0192)\n",
      "Epoch: [107][0/1]\tTime  0.358 ( 0.358)\tData  0.011 ( 0.011)\tLoss -0.0208 (-0.0208)\n",
      "Epoch: [108][0/1]\tTime  0.346 ( 0.346)\tData  0.018 ( 0.018)\tLoss -0.0235 (-0.0235)\n",
      "Epoch: [109][0/1]\tTime  0.357 ( 0.357)\tData  0.015 ( 0.015)\tLoss -0.0233 (-0.0233)\n",
      "Epoch: [110][0/1]\tTime  0.352 ( 0.352)\tData  0.015 ( 0.015)\tLoss -0.0245 (-0.0245)\n",
      "Epoch: [111][0/1]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0210 (-0.0210)\n",
      "Epoch: [112][0/1]\tTime  0.357 ( 0.357)\tData  0.014 ( 0.014)\tLoss -0.0209 (-0.0209)\n",
      "Epoch: [113][0/1]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0226 (-0.0226)\n",
      "Epoch: [114][0/1]\tTime  0.358 ( 0.358)\tData  0.020 ( 0.020)\tLoss -0.0215 (-0.0215)\n",
      "Epoch: [115][0/1]\tTime  0.368 ( 0.368)\tData  0.017 ( 0.017)\tLoss -0.0204 (-0.0204)\n",
      "Epoch: [116][0/1]\tTime  0.354 ( 0.354)\tData  0.013 ( 0.013)\tLoss -0.0249 (-0.0249)\n",
      "Epoch: [117][0/1]\tTime  0.361 ( 0.361)\tData  0.014 ( 0.014)\tLoss -0.0235 (-0.0235)\n",
      "Epoch: [118][0/1]\tTime  0.361 ( 0.361)\tData  0.015 ( 0.015)\tLoss -0.0237 (-0.0237)\n",
      "Epoch: [119][0/1]\tTime  0.361 ( 0.361)\tData  0.010 ( 0.010)\tLoss -0.0256 (-0.0256)\n",
      "Epoch: [120][0/1]\tTime  0.359 ( 0.359)\tData  0.014 ( 0.014)\tLoss -0.0228 (-0.0228)\n",
      "Epoch: [121][0/1]\tTime  0.349 ( 0.349)\tData  0.012 ( 0.012)\tLoss -0.0213 (-0.0213)\n",
      "Epoch: [122][0/1]\tTime  0.357 ( 0.357)\tData  0.014 ( 0.014)\tLoss -0.0248 (-0.0248)\n",
      "Epoch: [123][0/1]\tTime  0.355 ( 0.355)\tData  0.015 ( 0.015)\tLoss -0.0271 (-0.0271)\n",
      "Epoch: [124][0/1]\tTime  0.451 ( 0.451)\tData  0.014 ( 0.014)\tLoss -0.0257 (-0.0257)\n",
      "Epoch: [125][0/1]\tTime  0.192 ( 0.192)\tData  0.010 ( 0.010)\tLoss -0.0241 (-0.0241)\n",
      "Epoch: [126][0/1]\tTime  0.446 ( 0.446)\tData  0.055 ( 0.055)\tLoss -0.0260 (-0.0260)\n",
      "Epoch: [127][0/1]\tTime  0.360 ( 0.360)\tData  0.018 ( 0.018)\tLoss -0.0217 (-0.0217)\n",
      "Epoch: [128][0/1]\tTime  0.351 ( 0.351)\tData  0.015 ( 0.015)\tLoss -0.0271 (-0.0271)\n",
      "Epoch: [129][0/1]\tTime  0.385 ( 0.385)\tData  0.015 ( 0.015)\tLoss -0.0261 (-0.0261)\n",
      "Epoch: [130][0/1]\tTime  0.336 ( 0.336)\tData  0.024 ( 0.024)\tLoss -0.0240 (-0.0240)\n",
      "Epoch: [131][0/1]\tTime  0.359 ( 0.359)\tData  0.016 ( 0.016)\tLoss -0.0225 (-0.0225)\n",
      "Epoch: [132][0/1]\tTime  0.362 ( 0.362)\tData  0.016 ( 0.016)\tLoss -0.0276 (-0.0276)\n",
      "Epoch: [133][0/1]\tTime  0.357 ( 0.357)\tData  0.014 ( 0.014)\tLoss -0.0258 (-0.0258)\n",
      "Epoch: [134][0/1]\tTime  0.357 ( 0.357)\tData  0.014 ( 0.014)\tLoss -0.0278 (-0.0278)\n",
      "Epoch: [135][0/1]\tTime  0.351 ( 0.351)\tData  0.014 ( 0.014)\tLoss -0.0292 (-0.0292)\n",
      "Epoch: [136][0/1]\tTime  0.451 ( 0.451)\tData  0.019 ( 0.019)\tLoss -0.0284 (-0.0284)\n",
      "Epoch: [137][0/1]\tTime  0.369 ( 0.369)\tData  0.018 ( 0.018)\tLoss -0.0258 (-0.0258)\n",
      "Epoch: [138][0/1]\tTime  0.360 ( 0.360)\tData  0.016 ( 0.016)\tLoss -0.0255 (-0.0255)\n",
      "Epoch: [139][0/1]\tTime  0.375 ( 0.375)\tData  0.021 ( 0.021)\tLoss -0.0254 (-0.0254)\n",
      "Epoch: [140][0/1]\tTime  0.367 ( 0.367)\tData  0.018 ( 0.018)\tLoss -0.0277 (-0.0277)\n",
      "Epoch: [141][0/1]\tTime  0.363 ( 0.363)\tData  0.016 ( 0.016)\tLoss -0.0296 (-0.0296)\n",
      "Epoch: [142][0/1]\tTime  0.351 ( 0.351)\tData  0.015 ( 0.015)\tLoss -0.0283 (-0.0283)\n",
      "Epoch: [143][0/1]\tTime  0.361 ( 0.361)\tData  0.014 ( 0.014)\tLoss -0.0287 (-0.0287)\n",
      "Epoch: [144][0/1]\tTime  0.352 ( 0.352)\tData  0.015 ( 0.015)\tLoss -0.0271 (-0.0271)\n",
      "Epoch: [145][0/1]\tTime  0.363 ( 0.363)\tData  0.016 ( 0.016)\tLoss -0.0272 (-0.0272)\n",
      "Epoch: [146][0/1]\tTime  0.356 ( 0.356)\tData  0.016 ( 0.016)\tLoss -0.0323 (-0.0323)\n",
      "Epoch: [147][0/1]\tTime  0.361 ( 0.361)\tData  0.016 ( 0.016)\tLoss -0.0306 (-0.0306)\n",
      "Epoch: [148][0/1]\tTime  0.362 ( 0.362)\tData  0.016 ( 0.016)\tLoss -0.0307 (-0.0307)\n",
      "Epoch: [149][0/1]\tTime  0.471 ( 0.471)\tData  0.013 ( 0.013)\tLoss -0.0300 (-0.0300)\n",
      "Epoch: [150][0/1]\tTime  0.176 ( 0.176)\tData  0.020 ( 0.020)\tLoss -0.0269 (-0.0269)\n",
      "Epoch: [151][0/1]\tTime  0.353 ( 0.353)\tData  0.059 ( 0.059)\tLoss -0.0330 (-0.0330)\n",
      "Epoch: [152][0/1]\tTime  0.363 ( 0.363)\tData  0.015 ( 0.015)\tLoss -0.0313 (-0.0313)\n",
      "Epoch: [153][0/1]\tTime  0.362 ( 0.362)\tData  0.015 ( 0.015)\tLoss -0.0279 (-0.0279)\n",
      "Epoch: [154][0/1]\tTime  0.356 ( 0.356)\tData  0.026 ( 0.026)\tLoss -0.0298 (-0.0298)\n",
      "Epoch: [155][0/1]\tTime  0.357 ( 0.357)\tData  0.016 ( 0.016)\tLoss -0.0288 (-0.0288)\n",
      "Epoch: [156][0/1]\tTime  0.359 ( 0.359)\tData  0.017 ( 0.017)\tLoss -0.0303 (-0.0303)\n",
      "Epoch: [157][0/1]\tTime  0.450 ( 0.450)\tData  0.012 ( 0.012)\tLoss -0.0286 (-0.0286)\n",
      "Epoch: [158][0/1]\tTime  0.359 ( 0.359)\tData  0.017 ( 0.017)\tLoss -0.0318 (-0.0318)\n",
      "Epoch: [159][0/1]\tTime  0.360 ( 0.360)\tData  0.016 ( 0.016)\tLoss -0.0311 (-0.0311)\n",
      "Epoch: [160][0/1]\tTime  0.357 ( 0.357)\tData  0.012 ( 0.012)\tLoss -0.0264 (-0.0264)\n",
      "Epoch: [161][0/1]\tTime  0.359 ( 0.359)\tData  0.019 ( 0.019)\tLoss -0.0317 (-0.0317)\n",
      "Epoch: [162][0/1]\tTime  0.359 ( 0.359)\tData  0.009 ( 0.009)\tLoss -0.0306 (-0.0306)\n",
      "Epoch: [163][0/1]\tTime  0.365 ( 0.365)\tData  0.014 ( 0.014)\tLoss -0.0318 (-0.0318)\n",
      "Epoch: [164][0/1]\tTime  0.352 ( 0.352)\tData  0.015 ( 0.015)\tLoss -0.0322 (-0.0322)\n",
      "Epoch: [165][0/1]\tTime  0.448 ( 0.448)\tData  0.012 ( 0.012)\tLoss -0.0308 (-0.0308)\n",
      "Epoch: [166][0/1]\tTime  0.358 ( 0.358)\tData  0.019 ( 0.019)\tLoss -0.0294 (-0.0294)\n",
      "Epoch: [167][0/1]\tTime  0.458 ( 0.458)\tData  0.020 ( 0.020)\tLoss -0.0297 (-0.0297)\n",
      "Epoch: [168][0/1]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0283 (-0.0283)\n",
      "Epoch: [169][0/1]\tTime  0.359 ( 0.359)\tData  0.016 ( 0.016)\tLoss -0.0273 (-0.0273)\n",
      "Epoch: [170][0/1]\tTime  0.368 ( 0.368)\tData  0.009 ( 0.009)\tLoss -0.0312 (-0.0312)\n",
      "Epoch: [171][0/1]\tTime  0.351 ( 0.351)\tData  0.021 ( 0.021)\tLoss -0.0326 (-0.0326)\n",
      "Epoch: [172][0/1]\tTime  0.358 ( 0.358)\tData  0.017 ( 0.017)\tLoss -0.0273 (-0.0273)\n",
      "Epoch: [173][0/1]\tTime  0.360 ( 0.360)\tData  0.015 ( 0.015)\tLoss -0.0306 (-0.0306)\n",
      "Epoch: [174][0/1]\tTime  0.355 ( 0.355)\tData  0.014 ( 0.014)\tLoss -0.0321 (-0.0321)\n",
      "Epoch: [175][0/1]\tTime  0.261 ( 0.261)\tData  0.026 ( 0.026)\tLoss -0.0297 (-0.0297)\n",
      "Epoch: [176][0/1]\tTime  0.364 ( 0.364)\tData  0.022 ( 0.022)\tLoss -0.0308 (-0.0308)\n",
      "Epoch: [177][0/1]\tTime  0.360 ( 0.360)\tData  0.020 ( 0.020)\tLoss -0.0310 (-0.0310)\n",
      "Epoch: [178][0/1]\tTime  0.391 ( 0.391)\tData  0.018 ( 0.018)\tLoss -0.0289 (-0.0289)\n",
      "Epoch: [179][0/1]\tTime  0.344 ( 0.344)\tData  0.034 ( 0.034)\tLoss -0.0314 (-0.0314)\n",
      "Epoch: [180][0/1]\tTime  0.350 ( 0.350)\tData  0.016 ( 0.016)\tLoss -0.0299 (-0.0299)\n",
      "Epoch: [181][0/1]\tTime  0.360 ( 0.360)\tData  0.016 ( 0.016)\tLoss -0.0286 (-0.0286)\n",
      "Epoch: [182][0/1]\tTime  0.351 ( 0.351)\tData  0.016 ( 0.016)\tLoss -0.0269 (-0.0269)\n",
      "Epoch: [183][0/1]\tTime  0.355 ( 0.355)\tData  0.016 ( 0.016)\tLoss -0.0309 (-0.0309)\n",
      "Epoch: [184][0/1]\tTime  0.363 ( 0.363)\tData  0.016 ( 0.016)\tLoss -0.0275 (-0.0275)\n",
      "Epoch: [185][0/1]\tTime  0.356 ( 0.356)\tData  0.018 ( 0.018)\tLoss -0.0315 (-0.0315)\n",
      "Epoch: [186][0/1]\tTime  0.355 ( 0.355)\tData  0.014 ( 0.014)\tLoss -0.0279 (-0.0279)\n",
      "Epoch: [187][0/1]\tTime  0.365 ( 0.365)\tData  0.017 ( 0.017)\tLoss -0.0307 (-0.0307)\n",
      "Epoch: [188][0/1]\tTime  0.356 ( 0.356)\tData  0.017 ( 0.017)\tLoss -0.0281 (-0.0281)\n",
      "Epoch: [189][0/1]\tTime  0.455 ( 0.455)\tData  0.007 ( 0.007)\tLoss -0.0272 (-0.0272)\n",
      "Epoch: [190][0/1]\tTime  0.354 ( 0.354)\tData  0.014 ( 0.014)\tLoss -0.0288 (-0.0288)\n",
      "Epoch: [191][0/1]\tTime  0.354 ( 0.354)\tData  0.010 ( 0.010)\tLoss -0.0342 (-0.0342)\n",
      "Epoch: [192][0/1]\tTime  0.361 ( 0.361)\tData  0.019 ( 0.019)\tLoss -0.0303 (-0.0303)\n",
      "Epoch: [193][0/1]\tTime  0.358 ( 0.358)\tData  0.015 ( 0.015)\tLoss -0.0310 (-0.0310)\n",
      "Epoch: [194][0/1]\tTime  0.360 ( 0.360)\tData  0.014 ( 0.014)\tLoss -0.0278 (-0.0278)\n",
      "Epoch: [195][0/1]\tTime  0.363 ( 0.363)\tData  0.011 ( 0.011)\tLoss -0.0269 (-0.0269)\n",
      "Epoch: [196][0/1]\tTime  0.364 ( 0.364)\tData  0.014 ( 0.014)\tLoss -0.0332 (-0.0332)\n",
      "Epoch: [197][0/1]\tTime  0.360 ( 0.360)\tData  0.020 ( 0.020)\tLoss -0.0309 (-0.0309)\n",
      "Epoch: [198][0/1]\tTime  0.359 ( 0.359)\tData  0.020 ( 0.020)\tLoss -0.0313 (-0.0313)\n",
      "Epoch: [199][0/1]\tTime  0.361 ( 0.361)\tData  0.018 ( 0.018)\tLoss -0.0275 (-0.0275)\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "epochs = epochs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "\n",
    "    # train for one epoch\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "    if epoch + 1 <= 100:\n",
    "        if epoch+1 == 5:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': 'vgg16',\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best=False, filename='models/pretrain/{}_checkpoint_{:04d}.pth.tar'.format(timestamp, epoch+1))\n",
    "        elif (epoch+1) % 10 == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': 'vgg16',\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best=False, filename='models/pretrain/{}_checkpoint_{:04d}.pth.tar'.format(timestamp, epoch+1))\n",
    "    elif (epoch + 1 > 100):\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': 'vgg16',\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best=False, filename='models/pretrain/{}_checkpoint_{:04d}.pth.tar'.format(timestamp, epoch+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
